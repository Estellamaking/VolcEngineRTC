module.exports =
/******/ (function(modules) { // webpackBootstrap
/******/ 	// The module cache
/******/ 	var installedModules = {};
/******/
/******/ 	// The require function
/******/ 	function __webpack_require__(moduleId) {
/******/
/******/ 		// Check if module is in cache
/******/ 		if(installedModules[moduleId]) {
/******/ 			return installedModules[moduleId].exports;
/******/ 		}
/******/ 		// Create a new module (and put it into the cache)
/******/ 		var module = installedModules[moduleId] = {
/******/ 			i: moduleId,
/******/ 			l: false,
/******/ 			exports: {}
/******/ 		};
/******/
/******/ 		// Execute the module function
/******/ 		modules[moduleId].call(module.exports, module, module.exports, __webpack_require__);
/******/
/******/ 		// Flag the module as loaded
/******/ 		module.l = true;
/******/
/******/ 		// Return the exports of the module
/******/ 		return module.exports;
/******/ 	}
/******/
/******/
/******/ 	// expose the modules object (__webpack_modules__)
/******/ 	__webpack_require__.m = modules;
/******/
/******/ 	// expose the module cache
/******/ 	__webpack_require__.c = installedModules;
/******/
/******/ 	// define getter function for harmony exports
/******/ 	__webpack_require__.d = function(exports, name, getter) {
/******/ 		if(!__webpack_require__.o(exports, name)) {
/******/ 			Object.defineProperty(exports, name, { enumerable: true, get: getter });
/******/ 		}
/******/ 	};
/******/
/******/ 	// define __esModule on exports
/******/ 	__webpack_require__.r = function(exports) {
/******/ 		if(typeof Symbol !== 'undefined' && Symbol.toStringTag) {
/******/ 			Object.defineProperty(exports, Symbol.toStringTag, { value: 'Module' });
/******/ 		}
/******/ 		Object.defineProperty(exports, '__esModule', { value: true });
/******/ 	};
/******/
/******/ 	// create a fake namespace object
/******/ 	// mode & 1: value is a module id, require it
/******/ 	// mode & 2: merge all properties of value into the ns
/******/ 	// mode & 4: return value when already ns object
/******/ 	// mode & 8|1: behave like require
/******/ 	__webpack_require__.t = function(value, mode) {
/******/ 		if(mode & 1) value = __webpack_require__(value);
/******/ 		if(mode & 8) return value;
/******/ 		if((mode & 4) && typeof value === 'object' && value && value.__esModule) return value;
/******/ 		var ns = Object.create(null);
/******/ 		__webpack_require__.r(ns);
/******/ 		Object.defineProperty(ns, 'default', { enumerable: true, value: value });
/******/ 		if(mode & 2 && typeof value != 'string') for(var key in value) __webpack_require__.d(ns, key, function(key) { return value[key]; }.bind(null, key));
/******/ 		return ns;
/******/ 	};
/******/
/******/ 	// getDefaultExport function for compatibility with non-harmony modules
/******/ 	__webpack_require__.n = function(module) {
/******/ 		var getter = module && module.__esModule ?
/******/ 			function getDefault() { return module['default']; } :
/******/ 			function getModuleExports() { return module; };
/******/ 		__webpack_require__.d(getter, 'a', getter);
/******/ 		return getter;
/******/ 	};
/******/
/******/ 	// Object.prototype.hasOwnProperty.call
/******/ 	__webpack_require__.o = function(object, property) { return Object.prototype.hasOwnProperty.call(object, property); };
/******/
/******/ 	// __webpack_public_path__
/******/ 	__webpack_require__.p = "";
/******/
/******/
/******/ 	// Load entry module and return exports
/******/ 	return __webpack_require__(__webpack_require__.s = "./src/preload/index.js");
/******/ })
/************************************************************************/
/******/ ({

/***/ "./node_modules/@byted/vertc-electron-sdk/build/Release/vertc-electron-sdk.node":
/*!**************************************************************************************!*\
  !*** ./node_modules/@byted/vertc-electron-sdk/build/Release/vertc-electron-sdk.node ***!
  \**************************************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

eval("/* WEBPACK VAR INJECTION */(function(module) {const path = __webpack_require__(/*! path */ \"path\");const filePath = path.resolve(__dirname, \"@byted/vertc-electron-sdk/build/Release\",\"vertc-electron-sdk.node\");try { global.process.dlopen(module, filePath); } catch(exception) { throw new Error('Cannot open ' + filePath + ': ' + exception); };\n/* WEBPACK VAR INJECTION */}.call(this, __webpack_require__(/*! ./../../../../webpack/buildin/module.js */ \"./node_modules/webpack/buildin/module.js\")(module)))\n\n//# sourceURL=webpack:///./node_modules/@byted/vertc-electron-sdk/build/Release/vertc-electron-sdk.node?");

/***/ }),

/***/ "./node_modules/@byted/vertc-electron-sdk/js/index.js":
/*!************************************************************!*\
  !*** ./node_modules/@byted/vertc-electron-sdk/js/index.js ***!
  \************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nfunction __export(m) {\n  for (var p in m) if (!exports.hasOwnProperty(p)) exports[p] = m[p];\n}\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\n\n__export(__webpack_require__(/*! ./main */ \"./node_modules/@byted/vertc-electron-sdk/js/main/index.js\"));\n\n__export(__webpack_require__(/*! ./types */ \"./node_modules/@byted/vertc-electron-sdk/js/types.js\"));\n\n//# sourceURL=webpack:///./node_modules/@byted/vertc-electron-sdk/js/index.js?");

/***/ }),

/***/ "./node_modules/@byted/vertc-electron-sdk/js/main/index.js":
/*!*****************************************************************!*\
  !*** ./node_modules/@byted/vertc-electron-sdk/js/main/index.js ***!
  \*****************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval(" //\n//  index.ts\n//\n//  Created by huanghao on 2020/7/2.\n\nvar __decorate = this && this.__decorate || function (decorators, target, key, desc) {\n  var c = arguments.length,\n      r = c < 3 ? target : desc === null ? desc = Object.getOwnPropertyDescriptor(target, key) : desc,\n      d;\n  if (typeof Reflect === \"object\" && typeof Reflect.decorate === \"function\") r = Reflect.decorate(decorators, target, key, desc);else for (var i = decorators.length - 1; i >= 0; i--) if (d = decorators[i]) r = (c < 3 ? d(r) : c > 3 ? d(target, key, r) : d(target, key)) || r;\n  return c > 3 && r && Object.defineProperty(target, key, r), r;\n};\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\n\nconst events_1 = __webpack_require__(/*! events */ \"events\");\n\nconst NativeEngine = __webpack_require__(/*! ../../build/Release/vertc-electron-sdk.node */ \"./node_modules/@byted/vertc-electron-sdk/build/Release/vertc-electron-sdk.node\");\n\nconst {\n  logger\n} = __webpack_require__(/*! ../utils/logger */ \"./node_modules/@byted/vertc-electron-sdk/js/utils/logger.js\");\n\nconst yuv_render_1 = __webpack_require__(/*! ../utils/yuv_render */ \"./node_modules/@byted/vertc-electron-sdk/js/utils/yuv_render.js\");\n\nconst types_1 = __webpack_require__(/*! ../types */ \"./node_modules/@byted/vertc-electron-sdk/js/types.js\");\n\nvar fs = __webpack_require__(/*! fs */ \"fs\");\n\nvar path = __webpack_require__(/*! path */ \"path\");\n\nconst mkdirsSync = dirname => {\n  if (fs.existsSync(dirname)) {\n    return true;\n  } else {\n    if (mkdirsSync(path.dirname(dirname))) {\n      fs.mkdirSync(dirname);\n      return true;\n    }\n  }\n}; ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n// veRTCEngine\n\n\nfunction checkInit(target, propertyName, projectDescriptor) {\n  // console.log('target: ', target);\n  // console.log('propertyName: ', propertyName);\n  // console.log('projectDescriptor: ', projectDescriptor);\n  const method = projectDescriptor.value;\n\n  projectDescriptor.value = function (...args) {\n    let result = 0,\n        params = '';\n\n    try {\n      if (!this.instance) {\n        throw \"instance is null\";\n      }\n\n      params = args.map(a => JSON.stringify(a)).join(); // console.log('params: ', params);\n      // console.log('this: ', this);\n\n      result = method.apply(this, args);\n    } catch (err) {\n      console.error(`Call ${propertyName} with (${params}) results: ${result}, error: `, err);\n      result = -1;\n    }\n\n    return result;\n  };\n\n  return projectDescriptor;\n}\n\nfunction isNULL(obj) {\n  if (typeof obj == \"undefined\" || obj === null) {\n    return true;\n  } else {\n    return false;\n  }\n}\n/** {zh}\n * @list 85530\n * @detail 85532\n */\n\n\nclass veRTCEngine extends events_1.EventEmitter {\n  constructor() {\n    super();\n    this.instance = null; // userId --> userInfo\n\n    this.remoteUsers = null;\n    this.localChannelId = \"\";\n    this.localUserId = \"\";\n    this.localUser = null;\n  }\n  /** {zh}\n   * @brief 创建和初始化引擎\n   * @param appId 每个应用的唯一标识符，由 RTC 控制台随机生成的。不同的 AppId 生成的实例在 RTC 中进行音视频通话完全独立，无法互通。\n   * @param documentPath 存放日志路径\n   * @param params 用以覆盖默认参数的本引擎实例参数。JSON 字符串格式。\n   * @return\n   * + 0：成功\n   * + !0：失败<br>\n   * @region 引擎管理\n   */\n\n\n  init(appId, documentPath, params) {\n    let ret = -1;\n\n    do {\n      if (this.instance) {\n        console.warn('Call Init, but the instance is init yet');\n        break;\n      }\n\n      this.instance = new NativeEngine.veRTCEngine();\n      this.instance.Init(appId, documentPath, this.cbEngine.bind(this), params); // define an empty userId as the local user\n\n      this.localUser = {\n        userId: \"\",\n        renderOptions: {\n          renderMode: types_1.RenderMode.FIT,\n          mirror: false\n        }\n      };\n      this.remoteUsers = new Map();\n      ret = 0;\n    } while (false);\n\n    return ret;\n  }\n  /** {zh}\n   * @brief 销毁引擎\n   * @return\n   * + 0：成功\n   * + !0：失败<br>\n   * @region 引擎管理\n   */\n\n\n  uninit() {\n    this.instance.Uninit();\n    this.instance = null;\n    this.remoteUsers = null;\n    this.localChannelId = \"\";\n    this.localUserId = \"\";\n    this.localUser = null;\n    return 0;\n  }\n  /** {zh}\n   * @brief 获取当前 SDK 版本信息\n   * @return {string} 当前 SDK 版本信息\n   * @region 引擎管理\n   */\n\n\n  static getSDKVersion() {\n    return NativeEngine.GetSDKVersion();\n  }\n  /** {zh}\n   * @brief 获取加载引擎进程的 ID 方便调试\n   * @return {string} 进程的 ID\n   */\n\n\n  static getCurrentProcessId() {\n    return NativeEngine.GetCurrentProcessId();\n  }\n  /**\n   * @region 引擎管理\n   * @brief 设置业务标识参数\n   * @param business_id 用户设置的自己的 business_id 值。business_id 相当于一个 sub AppId，可以分担和细化现在 AppId 的逻辑划分的功能， 但不需要鉴权。business_id 只是一个标签，颗粒度需要用户自定义。\n   * @return + 0： 成功。\n   * + <0： 失败\n   * + -6001： 用户已经在房间中。\n   * + -6002： 输入非法，合法字符包括所有小写字母、大写字母和数字，除此外还包括四个独立字符分别是：英文句号，短横线，下划线和 `@`。<br>\n   * @notes\n   * + 可通过 business_id 区分不同的业务场景（角色/策略等）。business_id 由客户自定义，相当于一个“标签”，可以分担和细化现在 AppId 的逻辑划分的功能。\n   * + 需要在调用 joinRoom{@link #JoinRoom} 之前调用，joinRoom{@link #JoinRoom}之后调用该方法无效。\n   */\n\n\n  setBusinessId(business_id) {\n    return this.instance.SetBusinessId(business_id);\n  }\n  /** {zh}\n   * @region 房间管理\n   * @brief 创建/加入房间：房间不存在时即创建房间；房间存在时即加入这个房间。\n   * @param token 动态密钥，用于对登录用户进行鉴权验证。进入房间需要携带 Token。测试时可使用控制台生成临时 Token，正式上线需要使用密钥 SDK 在您的服务端生成并下发 Token。\n   * @param room_id 加入的房间 ID。房间 ID 为长度在 128 字节以内的非空字符串，支持以下字符集范围:  1. 26 个大写字母 A ~ Z 。   2. 26 个小写字母 a ~ z 。  3. 10 个数字 0 ~ 9 。   4. 下划线 \"_\"，at 符 \"@\"，减号 \"-\"。\n   * @param uid 用户 ID用户 ID 在 appId 的维度下是唯一的。\n   * @param info 用户信息\n   * @param roomConfig 房间参数配置，设置房间模式以及是否自动发布或订阅流。\n   * @return\n   * +  0: 成功\n   * + -1: room_id 为空，失败<br>\n   * + -2: user_info 为空，失败<br>\n   * @notes\n   * + 同一房间内的用户间可以相互通话。\n   * + 进房后重复调用无效，用户必须调用 leaveRoom{@link #LeaveRoom} 退出当前房间后，才能加入下一个房间。\n   * + 本地用户调用此方法加入房间成功后，会收到 onJoinRoomResult{@link 85533#OnJoinRoomResult} 回调通知。\n   * + 本地用户调用 setUserVisibility{@link #SetUserVisibility} 将自身设为可见后加入房间，远端用户会收到 OnUserJoined{@link 85533#OnUserJoined}。+ 使用不同 App ID 的 App 是不能互通的。\n   * + 请务必保证生成 Token 使用的 App ID 和创建引擎时使用的 App ID 相同，否则会导致加入房间失败。\n   * + 用户加入房间成功后，在本地网络状况不佳的情况下，SDK 可能会与服务器失去连接，此时 SDK 将会自动重连。重连成功后，本地会收到 OnJoinRoomResult{@link 85533#OnJoinRoomResult} 回调通知。\n   * + 同一个 App ID 的同一个房间内，每个用户的用户 ID 必须是唯一的。如果两个用户的用户 ID 相同，则后进房的用户会将先进房的用户踢出房间，并且先进房的用户会收到 OnError{@link 85533#OnError} 回调通知，错误类型详见 ErrorCode{@link #ErrorCode} 中的 kErrorCodeDuplicateLogin。\n   */\n\n\n  joinRoom(token, room_id, uid, info, roomConfig) {\n    this.localChannelId = room_id;\n    this.localUserId = uid;\n\n    if (this.localUser) {\n      this.localUser.userId = uid;\n    }\n\n    return this.instance.JoinRoom(token, room_id, uid, info, roomConfig);\n  }\n  /** {zh}\n   * @region 房间管理\n   * @brief 设置用户可见性。默认为可见。\n   * @param enable 设置用户是否对房间内其他用户可见：  + `true`: 可以被房间中的其他用户感知，且可以在房间内发布和订阅音视频流；  + `false`: 无法被房间中的其他用户感知，且只能在房间内订阅音视频流。\n   * @return\n   * + 0：成功\n   * + !0：失败<br>\n   * @notes\n   * + 通过对用户可见性进行设置，可以控制用户在房间内的行为：\n   *          + 能否发布音视频流；\n   *          + 用户自身是否在房间中隐身。\n   * + 该方法在加入房间前后均可调用。\n   * + 在房间内调用此方法，房间内其他用户会收到相应的回调通知：\n   *            - 从 false 切换至 true 时，房间内其他用户会收到 onUserJoined{@link 85533#OnUserJoined} 回调通知；\n   *            - 从 true 切换至 false 时，房间内其他用户会收到 onUserLeave{@link 85533#OnUserLeave} 回调通知。\n   * + 若调用该方法将可见性设为 false，此时尝试发布流会收到 onWarning{@link 85533#OnWarning} 警告，具体原因参看 WarningCode{@link #WarningCode} 中的 kWarningCodePublishStreamForbiden 警告码。\n   */\n\n\n  setUserVisibility(enable) {\n    return this.instance.SetUserVisibility(enable);\n  }\n  /** {zh}\n   * @region 房间管理\n   * @brief 离开房间。调用此方法离开房间，结束通话过程，释放所有通话相关的资源。\n   * @return\n   * + 0：成功\n   * + !0：失败<br>\n   * @notes\n   * + 加入房间后，必须调用此方法结束通话，否则无法开始下一次通话。无论当前是否在房间内，都可以调用此方法。重复调用此方法没有负面影响。\n   * + 此方法是异步操作，调用返回时并没有真正退出房间。真正退出房间后，本地会收到 onLeaveRoom{@link 85533#OnLeaveRoom} 回调通知。\n   * + 调用 setUserVisibility{@link #SetUserVisibility} 将自身设为可见的用户离开房间后，房间内其他用户会收到 OnUserLeave{@link 85533#OnUserLeave} 回调通知。\n   * + 如果调用此方法后立即销毁引擎，SDK 将无法触发 onLeaveRoom{@link 85533#OnLeaveRoom} 回调。\n   */\n\n\n  leaveRoom() {\n    return this.instance.LeaveRoom();\n  }\n  /** {zh}\n   * @region 房间管理\n   * @brief 更新 Token\n   * @param token 更新的动态密钥。\n   * @return\n   * + 0：成功\n   * + !0：失败<br>\n   * @notes\n   * + Token 有一定的有效期，当 Token 过期时，用户需调用此方法更新房间的 Token 信息。\n   * + 用户调用 joinRoom{@link #JoinRoom} 方法加入房间时，如果使用了过期的 Token 将导致加入房间失败，并会收到 OnJoinRoomResult{@link #OnJoinRoomResult} 回调通知，错误码为 ErrorCode.kErrorCodeInvalidToken{@link #ErrorCode}。此时需要重新获取 Token，并调用此方法更新 Token。\n   * + 如果用户因 Token 过期导致加入房间失败，则调用此方法更新 Token 后，SDK 会自动重新加入房间，而不需要用户自己调用 joinRoom{@link #JoinRoom} 方法。\n   * + Token 过期时，如果已经加入房间成功，则不会受到影响。Token 过期的错误会在下一次使用过期 Token 加入房间时，或因本地网络状况不佳导致断网重新连入房间时通知给用户。\n   */\n\n\n  updateToken(token) {\n    return this.instance.UpdateToken(token);\n  }\n  /** {zh}\n   * @type api\n   * @region 音频管理\n   * @brief 调节本地播放的所有远端用户混音后的音量\n   * @param volume 音频播放音量，取值范围： [0,400] + 0: 静音   + 100: 原始音量   + 400: 最大可为原始音量的 4 倍(自带溢出保护)\n   * @return\n   * + 0：成功\n   * + !0：失败<br>\n   * @notes 为保证更好的通话质量，建议将 volume 值设为 [0,100]。\n   */\n\n\n  setPlaybackVolume(volume) {\n    return this.instance.SetPlaybackVolume(volume);\n  }\n  /** {zh}\n   * @region 音频管理\n   * @brief 开启内部音频采集。默认为关闭状态。\n   * @return\n   * + 0：成功\n   * + !0：失败<br>\n   * @notes\n   * + 进房前调用该方法，本地用户会收到 onMediaDeviceStateChanged{@link 85533#OnMediaDeviceStateChanged} 的回调。\n   * + 进房后调用该方法，房间中的其他用户会收到 onUserStartAudioCapture{@link 85533#OnUserStartAudioCapture} 的回调。\n   * + 若未取得当前设备的麦克风权限，调用该方法后会触发 onWarning{@link 85533#OnWarning} 回调。\n   * + 调用 stopAudioCapture{@link #StopAudioCapture} 可以关闭音频采集设备，否则，SDK 只会在销毁引擎的时候自动关闭设备。\n   * + 无论是否发布音频数据，你都可以调用该方法开启音频采集，并且调用后方可发布音频。\n   * + 尚未进房并且已使用自定义采集时，关闭自定义采集后并不会自动开启内部采集。你需调用此方法手动开启内部采集。\n   */\n\n\n  startAudioCapture() {\n    return this.instance.StartAudioCapture();\n  }\n  /** {zh}\n   * @region 音频管理\n   * @brief 关闭内部音频采集。默认为关闭状态。\n   * @return\n   * + 0：成功\n   * + !0：失败<br>\n   * @notes\n   * + 进房前调用该方法，本地用户会收到 onMediaDeviceStateChanged{@link 85533#OnMediaDeviceStateChanged} 的回调。\n   * + 进房后调用该方法后，房间中的其他用户会收到 onUserStopAudioCapture{@link #OnUserStopAudioCapture} 的回调。\n   * + 调用 startAudioCapture{@link #StartAudioCapture} 可以开启音频采集设备。\n   * + 设备开启后若一直未调用该方法关闭，则 SDK 会在销毁引擎的时候自动关闭音频采集设备。\n   */\n\n\n  stopAudioCapture() {\n    return this.instance.StopAudioCapture();\n  }\n  /** {zh}\n   * @region 媒体流管理\n   * @brief 控制本地音频流的发送状态：发送/不发送\n   * @param muteState 发送状态，标识是否发送本地音频流\n   * @return\n   * + 0：成功\n   * + !0：失败<br>\n   * @notes\n   * + 使用此方法后，房间中的其他用户会收到回调： onUserMuteAudio{@link 85533#OnUserMuteAudio}\n   * + 本方法仅控制本地音频流的发送状态，并不影响本地音频采集状态。\n   */\n\n\n  muteLocalAudio(muteState) {\n    return this.instance.MuteLocalAudio(muteState);\n  }\n  /** {zh}\n   * @region 多房间\n   * @brief 开启/关闭音量提示。默认关闭。\n   * @param interval 收到音量提示回调的时间间隔：  <br>建议设置为大于等于 200 毫秒；小于 10 毫秒时，行为未定义。<li> ≤ 0：禁用音量提示功能。  </li><li> > 0：启用音量提示功能，并设置收到音量提示回调的时间间隔。单位为毫秒。</li>\n   * @return\n   * + 0：成功\n   * + !0：失败<br> <br>\n   * @notes\n   * + 开启音量提示后，将按设置的时间间隔收到本地采集音量或订阅的远端用户的音量信息回调。\n   * + 关于回调的具体信息，参看 onAudioVolumeIndication{@link 85533#OnAudioVolumeIndication}\n   */\n\n\n  setAudioVolumeIndicationInterval(interval) {\n    return this.instance.SetAudioVolumeIndicationInterval(interval);\n  }\n  /** {zh}\n   * @region 音频管理\n   * @brief 调节来自远端用户的音频播放音量\n   * @param strUserId 音频来源的远端用户 ID\n   * @param volume  播放音量，取值范围： [0,400]  <li> 0: 静音 </li> <li> 100: 原始音量 </li> <li> 400: 最大可为原始音量的 4 倍(自带溢出保护)  </li>\n   * @return\n   * + 0：成功\n   * + !0：失败<br>\n   */\n\n\n  setRemoteAudioPlaybackVolume(strUserId, volume) {\n    return this.instance.SetRemoteAudioPlaybackVolume(strUserId, volume);\n  }\n  /** {zh}\n   * @region 媒体流管理\n   * @brief 恢复接收来自远端的媒体流\n   * @param type 媒体流类型，指定需要暂停接收音频还是视频流\n   * @return\n   * + 0：成功\n   * + !0：失败<br>\n   * @notes\n   * + 该方法仅恢复远端流的接收，并不影响远端流的采集和发送；\n   * + 该方法不改变用户的订阅状态以及订阅流的属性。\n   */\n\n\n  resumeAllSubscribedStream(type) {\n    return this.instance.ResumeAllSubscribedStream(type);\n  }\n  /** {zh}\n  * @region 视频管理\n  * @brief 开启内部视频采集。默认为关闭状态\n  * @return\n   * + 0：成功\n   * + !0：失败<br>\n  * @notes\n  * + 调用该方法后，本地用户会收到 onMediaDeviceStateChanged{@link 85533#OnMediaDeviceStateChanged} 的回调。\n  * + 进房后调用该方法后，房间中的其他用户会收到 onUserStartVideoCapture{@link 85533#OnUserStartVideoCapture} 的回调。\n  * + 内部视频采集指：使用 RTC SDK 内置视频采集模块，进行采集。\n  * + 若未取得当前设备的摄像头权限，调用该方法会不成功，并触发 onMediaDeviceStateChanged{@link #OnMediaDeviceStateChanged} 回调。\n  * + 调用 stopVideoCapture{@link #StopVideoCapture} 可以关闭视频采集设备，否则，SDK 只会在销毁引擎的时候自动关闭设备。\n  * + 无论是否发布视频数据，你都可以调用该方法开启视频采集，并且调用后方可发布视频。\n  * + 如果需要从自定义视频采集切换为内部视频采集，你必须先关闭自定义采集，再调用此方法手动开启内部采集。\n  */\n\n\n  startVideoCapture() {\n    return this.instance.StartVideoCapture();\n  }\n  /** {zh}\n   * @region 视频管理\n   * @brief 关闭内部视频采集。默认为关闭状态。\n   * @return\n   * + 0：成功\n   * + !0：失败<br>\n   * @notes\n   * + 调用该方法后，本地用户会收到 onMediaDeviceStateChanged{@link 85533#OnMediaDeviceStateChanged} 的回调。\n   * + 进房后调用该方法后，房间中的其他用户会收到 onUserStopVideoCapture{@link 85533#OnUserStopVideoCapture} 的回调。\n   * + 调用 startVideoCapture{@link #StartVideoCapture} 可以开启视频采集设备。\n   * + 设备开启后若一直未调用此方法关闭本地视频采集，则 SDK 会在销毁引擎的时候自动关闭。\n   */\n\n\n  stopVideoCapture() {\n    return this.instance.StopVideoCapture();\n  }\n  /** {zh}\n   * @region 视频管理\n   * @brief 启动推送多路视频流，设置推送多路流时的各路视频参数，包括分辨率、帧率、码率、缩放模式、网络不佳时的回退策略等。\n   * @param index 主流 or 屏幕流<li>主流。包括：由摄像头/麦克风通过内部采集机制，采集到的视频/音频;通过自定义采集，采集到的视频/音频。</li><li>屏幕流。屏幕共享时共享的视频流，或来自声卡的本地播放音频流。</li>\n   * @param solutions 视频参数数组首地址。<br> 最多支持 3 路参数。当设置了多路参数时，分辨率必须是依次减小，从大到小排列的。 <br>最大分辨率没有限制。但是如果设置的分辨率无法编码，就会导致编码推流失败。\n   * @return\n   * + 0：成功\n   * + !0：失败<br>\n   * @notes\n   * + 当使用内部采集时，视频采集的分辨率、帧率会根据编码参数进行适配。\n   * + 默认的视频编码参数为：分辨率 640px * 360px，帧率 15fps。\n   * + 变更编码分辨率后马上生效，可能会引发相机重启。\n   * + 屏幕流只取视频参数数组的第一组数据。\n   */\n\n\n  setVideoEncoderConfig(index, solutions) {\n    // set default value\n    for (let i = 0; i < solutions.length; i++) {\n      solutions[i].maxSendKbps = solutions[i].maxSendKbps || types_1.SEND_KBPS_AUTO_CALCULATE;\n      solutions[i].scaleMode = solutions[i].scaleMode || types_1.ScaleMode.Auto;\n      solutions[i].encodePreference = solutions[i].encodePreference || types_1.EncodePreference.EncodePreferenceFramerate;\n    }\n\n    return this.instance.SetVideoEncoderConfig(index, solutions);\n  }\n  /** {zh}\n   * @region 视频管理\n   * @brief 停止/启动发送本地视频流，默认不发送。\n   * @param muteState 发送状态，标识是否发送本地视频流\n   * @return\n   * + 0：成功\n   * + !0：失败<br>\n   * @notes\n   * + 无论你使用内部视频采集，还是自定义视频采集，你都应使用此接口启动发送本地视频流。\n   * + 调用该方法后，房间中的其他用户会收到 onUserMuteVideo{@link #OnUserMuteVideo} 的回调。\n   * + 本方法只是停止/启动本地视频流的发送，不影响视频采集状态。\n   * + 无论是否开启视频采集，你都可以启动发送本地视频流。这样，一旦开始采集视频，即可进行发送。\n   */\n\n\n  muteLocalVideo(muteState) {\n    return this.instance.MuteLocalVideo(muteState);\n  }\n  /** {zh}\n   * @type api\n   * @region 视频管理\n   * @brief 切换视频内部采集时使用的前置/后置摄像头。默认使用前置摄像头。\n   * @param camera_id 移动端摄像头。可通过 enumerateVideoCaptureDevices{@link #enumerateVideoCaptureDevices} 获取。\n   * @return\n   * + 0：成功\n   * + !0：失败<br>\n   * @notes\n   * + 调用此接口后，在本地会触发 onMediaDeviceStateChanged{@link 85533#onMediaDeviceStateChanged} 回调。\n   * + 如果你正在使用相机进行视频采集，切换操作当即生效；如果相机未启动，后续开启内部采集时，会打开设定的摄像头。\n   * + 如果本地有多个摄像头且想选择特定工作摄像头可通过 IVideoDeviceManager{@link #IVideoDeviceManager} 来控制。\n   */\n\n\n  switchCamera(camera_id) {\n    return this.instance.SwitchCamera(camera_id);\n  }\n  /** {zh}\n   * @region 视频管理\n   * @brief 设置采用前置摄像头采集时，是否开启镜像模式。\n   * @param nMode 是否开启镜像模式\n   * @return\n   * + 0：成功\n   * + !0：失败<br>\n   */\n\n\n  setLocalVideoMirrorMode(nMode) {\n    var _a;\n\n    let user = this.localUser;\n\n    if (user) {\n      user.renderOptions.mirror = nMode == 1 ? true : false;\n      (_a = user.videoRender) === null || _a === void 0 ? void 0 : _a.setMirrorType(nMode == 1 ? true : false);\n    } // return this.instance.SetLocalVideoMirrorMode(nMode);\n\n\n    return 1;\n  }\n  /** {zh}\n   * @region 媒体流处理\n   * @brief 暂停接收来自远端的媒体流。\n   * @param type 媒体流类型，指定需要暂停接收音频还是视频流\n   * @return\n   * + 0：成功\n   * + !0：失败<br>\n   * @notes\n   * + 该方法仅暂停远端流的接收，并不影响远端流的采集和发送；\n   * + 该方法不改变用户的订阅状态以及订阅流的属性。\n   * + 若想恢复接收远端流，需调用 resumeAllSubscribedStream{@link #ResumeAllSubscribedStream}。\n   */\n\n\n  pauseAllSubscribedStream(type) {\n    return this.instance.PauseAllSubscribedStream(type);\n  }\n  /** {zh}\n   * @region 屏幕共享\n   * @brief 采集屏幕视频流，用于共享。屏幕视频流包括：屏幕上显示的内容，应用窗口中显示的内容，或虚拟屏幕中显示的内容。其中，虚拟屏幕中显示的内容仅在 Windows 平台上支持。\n   * @param info 待共享的屏幕源，你可以调用 getScreenCaptureSourceList{@link #GetScreenCaptureSourceList} 获得所有可以共享的屏幕源。\n   * @param params 共享参数\n   * @return\n   * + 0：成功\n   * + !0：失败<br>\n   * @notes\n   * + 调用此方法仅开启屏幕流视频采集，不会发布采集到的视频。发布屏幕流视频需要调用 publishScreen{@link #PublishScreen} 。\n   * + 要关闭屏幕视频源采集，调用 stopScreenVideoCapture{@link #StopScreenVideoCapture}。\n   * + 调用成功后，本端会收到 onFirstLocalVideoFrameCaptured{@link 85533#OnFirstLocalVideoFrameCaptured} 回调。\n   * + 调用此接口前，你可以调用 setVideoEncoderConfig{@link #SetVideoEncoderConfig} 设置屏幕视频流的采集帧率和编码分辨率。\n   * + 在收到 onFirstLocalVideoFrameCaptured{@link 85533#OnFirstLocalVideoFrameCaptured} 事件后通过调用 setLocalVideoCanvas{@link #SetLocalVideoCanvas} 或 SetLocalVideoSink{@link #SetLocalVideoSink} 函数设置本地屏幕共享视图。\n   * + 监听 onLocalScreenFrame{@link #IVideoFrameObserver#OnLocalScreenFrame} 本地屏幕视频回调事件\n   */\n\n\n  startScreenVideoCapture(info, params) {\n    return this.instance.StartScreenVideoCapture(info, params);\n  }\n  /** {zh}\n   * @region 屏幕共享\n   * @brief 更新屏幕共享区域。\n   * @param regionRect <li>当共享屏幕时，指定待共享区域相对于虚拟屏幕的位置</li><li>当共享窗口时，指定待共享区域相对于整个窗口的位置</li>\n   * @return\n   * + 0：成功\n   * + !0：失败<br>\n   */\n\n\n  updateScreenCaptureRegion(regionRect) {\n    // default value\n    return this.instance.UpdateScreenCaptureRegion(regionRect);\n  }\n  /** {zh}\n   * @type api\n   * @region 屏幕共享\n   * @brief 通过 RTC SDK 提供的采集模块采集屏幕视频流时，更新对鼠标的处理设置。默认采集鼠标。\n   * @param state 内部采集屏幕视频流时，是否采集鼠标信息\n   * @return\n   * + 0：成功\n   * + !0：失败<br>\n   * @notes 调用此接口前，必须已通过调用 startScreenVideoCapture{@link #StartScreenVideoCapture} 开启了内部屏幕流采集。\n   */\n\n\n  UpdateScreenCaptureMouseCursor(state) {\n    // default value\n    return this.instance.UpdateScreenCaptureMouseCursor(state);\n  }\n  /** {zh}\n   * @region 屏幕共享\n   * @brief 通过 RTC SDK 提供的采集模块采集屏幕视频流时，更新边框高亮设置。默认展示表框。\n   * @param highlight_config 边框高亮设置\n   * @return\n   * + 0：成功\n   * + !0：失败<br>\n   * @notes 调用此接口前，必须已通过调用 startScreenVideoCapture{@link #StartScreenVideoCapture} 开启了内部屏幕流采集。\n   */\n\n\n  updateScreenCaptureHighlightConfig(highlight_config) {\n    // default value\n    return this.instance.UpdateScreenCaptureHighlightConfig(highlight_config);\n  }\n  /** {zh}\n   * @private\n   * @region 屏幕共享\n   * @brief 通过 RTC SDK 提供的采集模块采集屏幕视频流时，设置需要过滤的窗口。\n   * @param screenFilterConfig 窗口过滤设置，TODO参看 ScreenFilterConfig{@link #ScreenFilterConfig}\n   * @return\n   * + 0：成功\n   * + !0：失败<br>\n   * @notes\n   * + 调用此接口前，必须已通过调用 startScreenVideoCapture{@link #StartScreenVideoCapture} 开启了内部屏幕流采集。\n   * + 本函数在屏幕源类别是屏幕而非应用窗体时才起作用。详见：ScreenCaptureSourceType{@link #ScreenCaptureSourceType}\n   */\n\n\n  updateScreenCaptureFilterConfig(screenFilterConfig) {\n    return this.instance.UpdateScreenCaptureFilterConfig(screenFilterConfig);\n  }\n  /** {zh}\n   * @region 屏幕共享\n   * @brief 停止屏幕视频流采集。\n   * @return\n   * + 0：成功\n   * + !0：失败<br>\n   * @notes\n   * + 要开启屏幕视频流采集，调用 startScreenVideoCapture{@link #StartScreenVideoCapture}\n   * + 调用此接口不影响屏幕视频流发布。\n   */\n\n\n  stopScreenVideoCapture() {\n    this.removeLocalScreen();\n    return this.instance.StopScreenVideoCapture();\n  }\n  /** {zh}\n   * @brief 获取屏幕采集对象列表。\n   * @return 屏幕采集对象列表\n   */\n\n\n  getScreenCaptureSourceList() {\n    return this.instance.GetScreenCaptureSourceList();\n  } // share screen\n\n  /** {zh}\n   * @brief 获取屏幕共享对象 thumbnail。\n   * @param type 屏幕采集对象的类型\n   * @param sourceId 屏幕分享时，共享对象的 ID\n   * @param maxWidth 最大宽度\n   * @param maxHeight 最大高度\n   * @return 屏幕共享对象\n   */\n\n\n  getThumbnail(type, sourceId, maxWidth, maxHeight) {\n    let obj = this.instance.GetThumbnail(type, sourceId, maxWidth, maxHeight);\n\n    if (obj.data) {\n      obj.data = \"data:image/png;base64,\" + obj.data;\n    }\n\n    return obj;\n  }\n  /** {zh}\n   * @region 屏幕共享\n   * @brief 发布本地屏幕共享流到房间。\n   * @return\n   * + 0：成功\n   * + !0：失败<br>\n   * @notes\n   * + 你必须先加入房间，才能调用此方法。\n   * + 直播、游戏、云游戏房间模式下，仅可见的用户可以调用此方法。你可以调用 setUserVisibility{@link #setUserVisibility} 方法设置用户在房间中的可见性。\n   * + 此方法只影响屏幕共享视频流的发布状态，并不影响屏幕视频流的采集情况。参看 startScreenVideoCapture{@link #startScreenVideoCapture} 和 StartScreenAudioCapture{@link #StartScreenAudioCapture}\n   * + 发布成功后，远端会收到 onStreamAdd{@link 85533#OnStreamAdd} 回调和 onRemoteScreenFrame{@link 85533#OnRemoteScreenFrame} 回调。\n   * + 调用 unpublishScreen{@link #unpublishScreen} 取消发布。\n   */\n\n\n  publishScreen() {\n    return this.instance.PublishScreen();\n  }\n  /** {zh}\n   * @region 屏幕共享\n   * @brief 停止发布本地屏幕共享流到房间。\n   * @return\n   * + 0：成功\n   * + !0：失败<br>\n   * @notes\n   * + 远端会收到 onStreamRemove{@link 85533#OnStreamRemove} 事件。\n   * + 此方法只影响屏幕共享视频流的发布状态，并不影响屏幕视频流的采集情况。\n   * + 调用 publishScreen{@link #PublishScreen} 启动发布。\n   */\n\n\n  unpublishScreen() {\n    return this.instance.UnpublishScreen();\n  }\n  /** {zh}\n   * @region 音量管理\n   * @brief 调节音频采集音量\n   * @param index 流索引，指定调节主流还是调节屏幕流的音量<li>主流。包括：由摄像头/麦克风通过内部采集机制，采集到的视频/音频;通过自定义采集，采集到的视频/音频。</li><li>屏幕流。屏幕共享时共享的视频流，或来自声卡的本地播放音频流。</li>\n   * @param volume 采集的音量值和原始音量的比值，范围是 [0, 400]，单位为 %，自带溢出保护。\n   * @return\n   * + 0：成功\n   * + !0：失败<br>\n   * @notes\n   * + 无论是采集来自麦克风的音频流，还是屏幕音频流；无论是 RTC SDK 内部采集，还是自定义采集，都可以使用此接口进行音量调节。\n   * + 在开启音频采集前后，你都可以使用此接口设定采集音量。\n   */\n\n\n  setCaptureVolume(index, volume) {\n    return this.instance.SetCaptureVolume(index, volume);\n  }\n  /** {zh}\n   * @region 混音\n   * @brief 开始播放音乐文件及混音\n   * @param id  混音 ID，用于标识混音，请保证混音 ID 唯一性。 <br> 如果已经通过 preloadAudioMixing{@link #PreloadAudioMixing} 将音效加载至内存，确保此处的 ID 与 preloadAudioMixing{@link #PreloadAudioMixing} 设置的 ID 相同。  <br>如果使用相同的 ID 重复调用本方法，前一次混音会停止，后一次混音开始，且 SDK 会使用 onAudioMixingStateChanged{@link 85533#OnAudioMixingStateChanged} 回调通知前一次混音已停止。\n   * @param file_path 指定需要混音的本地文件的绝对路径，支持音频文件格式有: mp3，aac，m4a，3gp，wav。  <br>可以通过传入不同的 ID 和 file_path 多次调用本方法，以实现同时播放多个音乐文件，实现混音叠加。\n   * @param mixingconfig 混音配置，设置混音的播放次数、是否本地播放混音、以及是否将混音发送至远端，详见 AudioMixingConfig{@link #AudioMixingConfig}\n   * @return\n   * + 0：成功\n   * + !0：失败<br>\n   * @notes\n   * + 调用本方法成功播放音乐文件后，SDK 会向本地回调当前的混音状态，见 onAudioMixingStateChanged{@link 85533#OnAudioMixingStateChanged}。\n   * + 开始播放音乐文件及混音后，可以调用 stopAudioMixing{@link #StopAudioMixing} 方法停止播放音乐文件。\n   */\n\n\n  startAudioMixing(id, file_path, mixingconfig) {\n    return this.instance.StartAudioMixing(id, file_path, mixingconfig);\n  }\n  /** {zh}\n   * @region 混音\n   * @brief 停止播放音乐文件及混音\n   * @param id 混音 ID\n   * @return\n   * + 0：成功\n   * + !0：失败<br>\n   * @notes\n   * + 调用 startAudioMixing{@link #StartAudioMixing} 方法开始播放音乐文件及混音后，可以调用本方法停止播放音乐文件及混音。\n   * + 调用本方法停止播放音乐文件后，SDK 会向本地回调通知已停止混音，见 onAudioMixingStateChanged{@link 85533#OnAudioMixingStateChanged}。\n   * + 调用本方法停止播放音乐文件后，该音乐文件会被自动卸载。\n   */\n\n\n  stopAudioMixing(id) {\n    return this.instance.StopAudioMixing(id);\n  }\n  /**\n   * @type api\n   * @region 混音\n   * @brief 暂停播放音乐文件及混音\n   * @param id 混音 ID\n   * @return\n   * + 0：成功\n   * + !0：失败<br>\n   * @notes\n   * + 调用 startAudioMixing{@link #StartAudioMixing} 方法开始播放音乐文件及混音后，可以通过调用本方法暂停播放音乐文件及混音。\n   * + 调用本方法暂停播放音乐文件及混音后，可调用 resumeAudioMixing{@link #ResumeAudioMixing} 方法恢复播放及混音。\n   * + 调用本方法暂停播放音乐文件后，SDK 会向本地回调通知已暂停混音，见 onAudioMixingStateChanged{@link 85533#OnAudioMixingStateChanged}。\n   */\n\n\n  pauseAudioMixing(id) {\n    return this.instance.PauseAudioMixing(id);\n  }\n  /** {zh}\n   * @region 混音\n   * @brief 恢复播放音乐文件及混音\n   * @param id\n   *        混音 ID\n   * @return\n   * + 0：成功\n   * + !0：失败<br>\n   * @notes\n   * + 调用 pauseAudioMixing{@link #PauseAudioMixing} 方法暂停播放音乐文件及混音后，可以通过调用本方法恢复播放及混音。\n   * + 调用本方法恢复播放音乐文件及混音后，SDK 会向本地回调通知音乐文件正在播放中，见 onAudioMixingStateChanged{@link #OnAudioMixingStateChanged}。\n   */\n\n\n  resumeAudioMixing(id) {\n    return this.instance.ResumeAudioMixing(id);\n  }\n  /** {zh}\n   * @region 混音\n   * @brief 调节音乐文件在本地和远端播放的音量大小\n   * @param id 混音 ID\n   * @param volume 音乐文件播放音量范围为 0~400。 <li> 0：静音 </li> <li> 100：原始音量  </li><li> 400: 最大可调音量 (自带溢出保护)</li>\n   * @param type 混音播放类型 <br>设置音乐文件是否本地播放、以及是否发送到远端，由此控制音乐文件本地或远端播放的音量，详见 AudioMixingType{@link #AudioMixingType}。\n   * @return\n   * + 0：成功\n   * + !0：失败<br>\n   * @notes\n   * + 调用本方法设置音量前，请先调用 preloadAudioMixing{@link #PreloadAudioMixing} 或 startAudioMixing{@link #StartAudioMixing}。\n   * + 为保证更好的音质，建议将 volume 值设为 [0,100]。\n   */\n\n\n  setAudioMixingVolume(id, volume, type) {\n    return this.instance.SetAudioMixingVolume(id, volume, type);\n  }\n  /** {zh}\n   * @region 混音\n   * @brief 获取音乐文件时长\n   * @param id 混音 ID\n   * @return\n   * + `>0`: 成功, 音乐文件时长，单位为毫秒。\n   * + `<0`: 失败<br>\n   * @notes 调用本方法获取音乐文件时长前，需要先调用 preloadAudioMixing{@link #PreloadAudioMixing} 或 startAudioMixing{@link #StartAudioMixing}。\n   */\n\n\n  getAudioMixingDuration(id) {\n    return this.instance.GetAudioMixingDuration(id);\n  }\n  /** {zh}\n   * @type api\n   * @region 混音\n   * @brief 获取音乐文件播放进度\n   * @param id 混音 ID\n   * @return\n   * + `>0`: 成功, 音乐文件播放进度，单位为毫秒。\n   * + `<0`: 失败<br>\n   * @notes 调用本方法获取音乐文件播放进度前，需要先调用 startAudioMixing{@link #StartAudioMixing} 开始播放音乐文件。\n   */\n\n\n  getAudioMixingCurrentPosition(id) {\n    return this.instance.GetAudioMixingCurrentPosition(id);\n  }\n  /** {zh}\n   * @type api\n   * @region 混音\n   * @brief 设置音乐文件的播放位置\n   * @param id 混音 ID\n   * @param pos 进度条位置，整数，单位为毫秒。\n   * @return\n   * + 0：成功\n   * + !0：失败<br>\n   * @notes\n   * + 本方法可以设置音乐文件的播放位置，这样你可以根据实际情况从指定的位置播放音乐文件，无需从头到尾完整播放一个音乐文件。\n   * + 调用本方法设置音乐文件的播放位置前，需要先调用 StartAudioMixing{@link #StartAudioMixing} 开始播放音乐文件。\n   */\n\n\n  setAudioMixingPosition(id, pos) {\n    return this.instance.SetAudioMixingPosition(id, pos);\n  }\n  /** {zh}\n   * @type api\n   * @region 混音\n   * @brief 预加载指定音乐文件到内存中\n   * @param id 混音 ID  <br>应用调用者维护，请保证唯一性。  <br>如果使用相同的 ID 调用本方法，后一次会覆盖前一次。 <br>如果先调用 startAudioMixing{@link #StartAudioMixing} ，再使用相同的 ID 调用本方法 ，会先回调 onAudioMixingStateChanged{@link 85533#OnAudioMixingStateChanged} 上一个混音停止，然后加载后一个混音。  <br>使用一个 ID 调用本方法预加载 A.mp3 后，如果需要使用相同的 ID 调用 startAudioMixing{@link #StartAudioMixing} 播放 B.mp3，请先调用 unloadAudioMixing{@link #UnloadAudioMixing} 卸载 A.mp3 ，否则会报错 kAudioMixingErrorLoadConflict。\n   * @param file_path  指定需要混音的本地文件的绝对路径，支持音频文件格式有: mp3，aac，m4a，3gp，wav。  <br>如果音乐文件长度超过 20s，会回调加载失败，见 onAudioMixingStateChanged{@link 85533#OnAudioMixingStateChanged}。\n   * @return\n   * + 0：成功\n   * + !0：失败<br>\n   * @notes\n   * + 需要频繁播放某个音乐文件的时候，调用本方法预加载该文件，在播放的时候可以只加载一次该文件，减少 CPU 占用。  <br>\n   * + 本方法只是预加载指定音乐文件，只有调用 startAudioMixing{@link #StartAudioMixing} 方法才开始播放指定音乐文件。 <br>\n   * + 调用本方法预加载音乐文件后，SDK 会回调通知音乐文件已加载，见 onAudioMixingStateChanged{@link 85533#OnAudioMixingStateChanged}。  <br>\n   * + 调用本方法预加载的指定音乐文件可以通过 unloadAudioMixing{@link #UnloadAudioMixing} 卸载。\n   */\n\n\n  preloadAudioMixing(id, file_path) {\n    return this.instance.PreloadAudioMixing(id, file_path);\n  }\n  /** {zh}\n   * @type api\n   * @region 混音\n   * @brief 卸载指定音乐文件\n   * @param id 混音 ID\n   * @return\n   * + 0：成功\n   * + !0：失败<br>\n   * @notes 不论音乐文件是否播放，调用本方法卸载该文件后，SDK 会回调通知混音已停止，见 onAudioMixingStateChanged{@link 85533#OnAudioMixingStateChanged}。\n   */\n\n\n  unloadAudioMixing(id) {\n    return this.instance.UnloadAudioMixing(id);\n  }\n  /** {zh}\n   * @type api\n   * @region 视频设备管理\n   * @brief 获取当前系统内视频采集设备列表。\n   * @return 包含系统中所有视频采集设备的列表和相关信息\n   */\n\n\n  enumerateVideoCaptureDevices() {\n    return this.instance.EnumerateVideoCaptureDevices();\n  }\n  /** {zh}\n   * @type api\n   * @region 视频设备管理\n   * @brief 设置当前视频采集设备\n   * @param deviceId 视频设备 ID，可以通过 EnumerateVideoCaptureDevices{@link #EnumerateVideoCaptureDevice} 获取\n   * @return\n   * + 0：方法调用成功\n   * + !0：方法调用失败<br>\n   */\n\n\n  setVideoCaptureDevice(deviceId) {\n    return this.instance.SetVideoCaptureDevice(deviceId);\n  }\n  /** {zh}\n   * @type api\n   * @region 视频设备管理\n   * @brief 获取当前 SDK 正在使用的视频采集设备信息\n   * @return 当前 SDK 正在使用的视频采集设备信息\n   */\n\n\n  getVideoCaptureDevice() {\n    return this.instance.GetVideoCaptureDevice();\n  }\n  /** {zh}\n  * @type api\n  * @region 音频设备管理\n  * @brief 获取当前系统内音频播放设备列表。如果后续设备有变更，你会收到 onMediaDeviceStateChanged{@link 85533#OnMediaDeviceStateChanged} 回调通知，然后需要重新调用本接口以获得新的设备列表。\n  * @return 包含系统中所有音频播放设备的列表和相关信息\n  */\n\n\n  enumerateAudioPlaybackDevices() {\n    return this.instance.EnumerateAudioPlaybackDevices();\n  }\n  /** {zh}\n   * @type api\n   * @region 音频设备管理\n   * @brief 获取当前系统内音频采集设备列表。如果后续设备有变更，你需要重新调用本接口以获得新的设备列表。\n   * @return 包含系统中所有音频采集设备列表和相关信息\n   */\n\n\n  enumerateAudioCaptureDevices() {\n    return this.instance.EnumerateAudioCaptureDevices();\n  }\n  /** {zh}\n   * @type api\n   * @region 音频设备管理\n   * @brief 设置音频播放设备。\n   * @param deviceId 音频播放设备 ID，可通过 enumerateAudioPlaybackDevices{@link #EnumerateAudioPlaybackDevices} 获取。\n   * @return  方法调用结果\n   * + 0：方法调用成功\n   * + <0：方法调用失败<br>\n   */\n\n\n  setAudioPlaybackDevice(deviceId) {\n    return this.instance.SetAudioPlaybackDevice(deviceId);\n  }\n  /** {zh}\n   * @type api\n   * @region 音频设备管理\n   * @brief 设置音频采集设备。\n   * @param deviceId 音频采集设备 ID，可通过 enumerateAudioCaptureDevices{@link #EnumerateAudioCaptureDevices} 获取。\n   * @return  方法调用结果\n   * + 0：方法调用成功\n   * + <0：方法调用失败<br>\n   */\n\n\n  setAudioCaptureDevice(deviceId) {\n    return this.instance.SetAudioCaptureDevice(deviceId);\n  }\n  /** {zh}\n   * @type api\n   * @region 音频设备管理\n   * @brief 获取当前音频播放设备 ID。\n   * @param device_id 音频播放设备 ID\n   * @return  当前音频播放设备 ID\n   */\n\n\n  getAudioPlaybackDevice() {\n    return this.instance.GetAudioPlaybackDevice();\n  }\n  /** {zh}\n   * @type api\n   * @region 音频设备管理\n   * @brief 获取当前音频采集设备 ID。\n   * @return 当前音频采集设备 ID\n   */\n\n\n  getAudioCaptureDevice() {\n    return this.instance.GetAudioCaptureDevice();\n  }\n  /** {zh}\n   * @type api\n   * @region 音频设备管理\n   * @brief 启动音频播放设备测试。  <br>该方法测试播放设备是否能正常工作。SDK 播放指定的音频文件，测试者如果能听到声音，说明播放设备能正常工作。\n   * @param test_audio_file_path 音频文件的绝对路径，路径字符串使用 UTF-8 编码格式，支持以下音频格式: mp3，aac，m4a，3gp，wav。\n   * @return  方法调用结果  <br>\n   * + 0：方法调用成功  <br>\n   * + <0：方法调用失败<br> <br>\n   * @notes\n   * + 该方法必须在 JoinRoom{@link #JoinRoom} 前调用，且不可与其它音频设备测试功能同时应用。  <br>\n   * + 你需调用 StopAudioPlaybackDeviceTest{@link #IAudioDeviceManager#StopAudioPlaybackDeviceTest} 停止测试。  <br>\n   */\n\n\n  startAudioPlaybackDeviceTest(test_audio_file_path) {\n    return this.instance.StartAudioPlaybackDeviceTest(test_audio_file_path);\n  }\n  /** {zh}\n    * @type api\n    * @region 音频设备管理\n    * @brief 停止音频播放设备测试。\n    * @return  方法调用结果  <br>\n    * + 0：方法调用成功  <br>\n    * + <0：方法调用失败<br> <br>\n    * @notes  调用 StartAudioPlaybackDeviceTest{@link #IAudioDeviceManager#StartAudioPlaybackDeviceTest} 后，需调用本方法停止测试。\n    */\n\n\n  stopAudioPlaybackDeviceTest() {\n    return this.instance.StopAudioPlaybackDeviceTest();\n  }\n  /** {zh}\n   * @type api\n   * @region 音频设备管理\n   * @brief 启动音频采集设备测试。 该方法测试音频采集设备是否能正常工作。启动测试后，会收到 OnAudioVolumeIndication{@link 85533#OnAudioVolumeIndication} 回调上报的音量信息。\n   * @param indication_interval onAudioVolumeIndication{@link 85533#OnAudioVolumeIndication} 回调的时间间隔，单位为毫秒。建议设置到大于 200 毫秒。最小不得少于 10 毫秒。小于10 毫秒行为未定义。\n   * @return  方法调用结果  <br>\n   * + 0：方法调用成功  <br>\n   * + <0：方法调用失败<br> <br>\n   * @notes\n   * + 该方法必须在 joinRoom{@link #JoinRoom} 前调用，且不可与其它音频设备测试功能同时应用。  <br>\n   * + 你需调用 stopAudioCaptureDeviceTest{@link #IAudioDeviceManager#StopAudioCaptureDeviceTest} 停止测试。  <br>\n   */\n\n\n  startAudioCaptureDeviceTest(indication_interval) {\n    return this.instance.StartAudioCaptureDeviceTest(indication_interval);\n  }\n  /** {zh}\n   * @type api\n   * @region 音频设备管理\n   * @brief 停止音频采集设备测试。\n   * @return  方法调用结果  <br>\n   * + 0：方法调用成功  <br>\n   * + <0：方法调用失败<br> <br>\n   * @notes  调用 startAudioCaptureDeviceTest{@link #IAudioDeviceManager#StartAudioCaptureDeviceTest} 后，需调用本方法停止测试。\n   */\n\n\n  stopAudioCaptureDeviceTest() {\n    return this.instance.StopAudioCaptureDeviceTest();\n  }\n  /** {zh}\n   * @type api\n   * @region 音频设备管理\n   * @brief 设置当前音频播放设备音量\n   * @param volume 音频播放设备音量，取值范围为 [0,255], 超出此范围设置无效。\n   * @return  方法调用结果\n   * + 0：方法调用成功\n   * + <0：方法调用失败<br>\n   */\n\n\n  setAudioPlaybackDeviceVolume(volume) {\n    return this.instance.SetAudioPlaybackDeviceVolume(volume);\n  }\n  /** {zh}\n   * @type api\n   * @region 音频设备管理\n   * @brief 获取当前音频播放设备音量\n   * @return  方法调用结果\n   * + 0：方法调用成功\n   * + <0：方法调用失败<br>\n   */\n\n\n  getAudioPlaybackDeviceVolume() {\n    return this.instance.GetAudioPlaybackDeviceVolume();\n  }\n  /** {zh}\n   * @type api\n   * @region 音频设备管理\n   * @brief 设置当前音频采集设备音量\n   * @param volume 音频采集设备音量，取值范围为 [0,255], 超出此范围设置无效。\n   * @return  方法调用结果\n   * + 0：方法调用成功\n   * + <0：方法调用失败<br>\n   */\n\n\n  setAudioCaptureDeviceVolume(volume) {\n    return this.instance.SetAudioCaptureDeviceVolume(volume);\n  }\n  /** {zh}\n  * @type api\n  * @region 音频设备管理\n  * @brief 获取当前音频采集设备音量\n  * @return  方法调用结果\n  * + 0：方法调用成功\n  * + <0：方法调用失败<br>\n  */\n\n\n  getAudioCaptureDeviceVolume() {\n    return this.instance.GetAudioCaptureDeviceVolume();\n  }\n  /** {zh}\n   * @type api\n   * @region 音频设备管理\n   * @brief 开始音频设备回路测试。 <br> 该方法测试音频采集设备和音频播放设备是否能正常工作。一旦测试开始，音频采集设备会采集本地声音，并通过音频播放设备播放出来，同时用户 App 会收到 onAudioVolumeIndication{@link 85533#OnAudioVolumeIndication} 回调上报的音量信息。\n   * @param indication_interval onAudioVolumeIndication{@link 85533#OnAudioVolumeIndication} 回调的时间间隔，单位为毫秒。建议设置到大于 200 毫秒。最小不得少于 10 毫秒。小于 10 毫秒行为未定义。\n   * @return  方法调用结果\n   * + 0：方法调用成功\n   * + <0：方法调用失败<br>\n   * @notes\n   * + 该方法必须在 JoinRoom{@link #IRtcEngine#JoinRoom} 前调用。且不可与其它音频设备测试功能同时应用。\n   * + 你需调用 StopAudioDeviceLoopbackTest{@link #IAudioDeviceManager#StopAudioDeviceLoopbackTest} 停止测试。\n   * + 该方法仅在本地进行音频设备测试，不涉及网络连接。\n   */\n\n\n  startAudioDeviceLoopbackTest(indication_interval) {\n    return this.instance.StartAudioDeviceLoopbackTest(indication_interval);\n  }\n  /** {zh}\n   * @type api\n   * @region 音频设备管理\n   * @brief 停止音频设备回路测试。\n   * @return  方法调用结果\n   * + 0：方法调用成功\n   * + <0：方法调用失败<br>\n   * @notes 调用 StartAudioDeviceLoopbackTest{@link #IAudioDeviceManager#StartAudioDeviceLoopbackTest} 后，需调用本方法停止测试。\n   */\n\n\n  stopAudioDeviceLoopbackTest() {\n    return this.instance.StopAudioDeviceLoopbackTest();\n  }\n  /** {zh}\n   * @type api\n   * @region 音频设备管理\n   * @brief 尝试初始化音频播放设备，可检测出设备不存在、权限被拒绝/禁用等异常问题。\n   * @param deviceId 设备索引号\n   * @return 设备状态错误码\n   * + 0: 设备检测结果正常\n   * + -1: 接口状态不正确，例如在正常启动采集后再调用该接口进行检测\n   * + -2: 采集设备无麦克风权限，尝试初始化设备失败\n   * + -3: 设备不存在，当前没有设备或设备被移除时返回\n   * + -4: 设备音频格式不支持\n   * + -5: 其它原因错误<br>\n   * @notes 1. 该接口需在进房前调用；\n   *        2. 检测成功不代表设备一定可以启动成功，还可能因设备被其他应用进程独占，或 CPU/内存不足等原因导致启动失败。\n   */\n\n\n  initAudioPlaybackDeviceForTest(deviceId) {\n    return this.instance.InitAudioPlaybackDeviceForTest(deviceId);\n  }\n  /** {zh}\n   * @type api\n   * @region 音频设备管理\n   * @brief 尝试初始化音频播放设备，可检测出设备不存在、权限被拒绝/禁用等异常问题。\n   * @param deviceId 设备索引号\n   * @return 设备状态错误码\n   * + 0: 设备检测结果正常\n   * + -1: 接口状态不正确，例如在正常启动采集后再调用该接口进行检测\n   * + -2: 采集设备无麦克风权限，尝试初始化设备失败\n   * + -3: 设备不存在，当前没有设备或设备被移除时返回\n   * + -4: 设备音频格式不支持\n   * + -5: 其它原因错误<br>\n   * @notes 1. 该接口需在进房前调用；\n   *        2. 检测成功不代表设备一定可以启动成功，还可能因设备被其他应用进程独占，或 CPU/内存不足等原因导致启动失败。\n   */\n\n\n  initAudioCaptureDeviceForTest(deviceId) {\n    return this.instance.InitAudioCaptureDeviceForTest(deviceId);\n  }\n  /** {zh}\n   * @type api\n   * @region 音频设备管理\n   * @brief 设置当前音频播放设备静音状态，默认为非静音。\n   * @param mute  <li>true：静音</li><li>false：非静音  </li>\n   * @return  方法调用结果\n   * + 0：方法调用成功\n   * + <0：方法调用失败<br>\n   */\n\n\n  setAudioPlaybackDeviceMute(mute) {\n    return this.instance.SetAudioPlaybackDeviceMute(mute);\n  }\n  /** {zh}\n   * @type api\n   * @region 音频设备管理\n   * @brief 获取当前音频播放设备是否静音的信息。\n   * @param mute  <li>true：静音</li><li>false：非静音  </li>\n   * @return  方法调用结果\n   * + `true`：方法调用成功\n   * + `false`：方法调用失败<br>\n   */\n\n\n  getAudioPlaybackDeviceMute() {\n    return this.instance.GetAudioPlaybackDeviceMute();\n  }\n  /** {zh}\n     * @type api\n     * @region 音频设备管理\n     * @brief 设置当前音频采集设备静音状态，默认为非静音。\n     * @param mute  <li>true：静音</li><li>false：非静音  </li>\n     * @return  方法调用结果  <br>\n     * + 0：方法调用成功  <br>\n     * + <0：方法调用失败<br> <br>\n     */\n\n\n  setAudioCaptureDeviceMute(mute) {\n    return this.instance.SetAudioCaptureDeviceMute(mute);\n  }\n  /** {zh}\n   * @type api\n    * @region 音频设备管理\n    * @brief 获取当前音频采集设备是否静音的信息。\n    * @return\n    * + true：静音  <br>\n    * + false：非静音  <br>\n    */\n\n\n  getAudioCaptureDeviceMute() {\n    return this.instance.GetAudioCaptureDeviceMute();\n  }\n  /** {zh}\n   * @type api\n   * @region 音视频回退\n   * @brief 设置发布的音视频流回退选项 。<br>你可以调用这个接口来设置网络情况不佳或设备性能不足时只发送小流，以保证通话质量。\n   * @param option 本地发布的音视频流回退选项  <li>0：（默认）上行网络较弱或性能不佳时，不对音视频流作回退处理。  <li></li>>1：上行网络较弱或性能不佳时，只发送视频小流。</li>\n   * @return\n   * + 0: 方法调用成功\n   * + <0: 方法调用失败<br>\n   * @notes\n   * + 这个方法只在设置了发送多个流的情况下有效。\n   * + 你必须在进房前设置，进房后设置或更改设置无效。\n   * + 设置回退选项后，本端发布的音视频流发生回退或从回退中恢复时，订阅该音视频流的客户端会收到 onSimulcastSubscribeFallback{@link #OnSimulcastSubscribeFallback} 回调通知。\n   * + 你可以调用 API 或者在服务端下发策略设置回退。当使用服务端下发配置实现时，下发配置优先级高于在客户端使用 API 设定的配置。\n   */\n\n\n  setPublishFallbackOption(option) {\n    return this.instance.SetPublishFallbackOption(option);\n  }\n  /** {zh}\n   * @type api\n   * @region 音视频回退\n   * @brief 设置订阅的音视频流回退选项。 <br>你可以通过调用该接口来设置网络情况不佳或性能不足时只订阅小流或音频流，以保证通话质量。\n   * @param option 远端订阅流回退处理选项<li>`0`: 关闭订阅音视频流时的性能回退功能 </li><li>`1`: 只接收视频小流</li><li>`2`: 先尝试只接收视频小流；如果网络环境无法显示视频，则再回退到只接收远端订阅的音频流</li>\n   * @return  方法调用结果\n   * + 0：方法调用成功\n   * + <0：方法调用失败<br>\n   *  @notes\n   * + 你必须在进房前设置，进房后设置或更改设置无效。\n   * + 设置回退选项后，本端订阅的音视频流发生回退或从回退中恢复时,会收到 onSimulcastSubscribeFallback{@link 85533#OnSimulcastSubscribeFallback} 回调通知。\n   * + 设置回退选项后，本端订阅的视频流因为回退分辨率发生变化时,会收到 onRemoteVideoSizeChanged{@link 85533#OnRemoteVideoSizeChanged} 回调通知。\n   * + 你可以调用 API 或者在服务端下发策略设置回退。当使用服务端下发配置实现时，下发配置优先级高于在客户端使用 API 设定的配置。\n   */\n\n\n  setSubscribeFallbackOption(option) {\n    return this.instance.SetSubscribeFallbackOption(option);\n  }\n  /** {zh}\n   * @region 音视频回退\n   * @brief 设置用户优先级\n   * @param userId 远端用户的 ID\n   * @param priority 远端用户的需求优先级，详见枚举类型 RemoteUserPriority{@link #RemoteUserPriority}\n   * @return + 0: 方法调用成功\n   * + <0: 方法调用失败<br>\n   * @notes\n   * + 1. 该方法与 setSubscribeFallbackOption{@link #SetSubscribeFallbackOption} 搭配使用。\n   * + 2. 如果开启了订阅流回退选项，弱网或性能不足时会优先保证收到的高优先级用户的流的质量。\n   * + 3. 该方法在进房前后都可以使用，可以修改远端用户的优先级。\n   */\n\n\n  setRemoteUserPriority(userId, priority) {\n    return this.instance.SetRemoteUserPriority(userId, priority);\n  }\n  /** {zh}\n   * @brief 在当前房间内手动开始发布本地音视频流\n   * @return + 0: 方法调用成功\n   * + <0: 方法调用失败<br>\n   * @notes\n   * + 如果你已经在用户进房时通过调用 [joinRoom](#joinroom) 选择了自动发布，则无需再调用本接口。<br>\n   * + 调用 setUserVisibility{@link #setUserVisibility} 方法将自身设置为不可见后无法调用该方法，需将自身切换至可见后方可调用该方法发布音视频流。 <br>\n   * + 同一用户，同一时间，只能在一个房间内发流。如果你需要发布屏幕共享流，调用 publishScreen{@link #publishScreen}。\n   * + 用户调用此方法成功发布音视频流后，房间中的其他用户将会收到 onStreamAdd{@link 85533#onStreamAdd} 回调通知。\n   */\n\n\n  publish() {\n    return this.instance.Publish();\n  }\n  /** {zh}\n   * @brief 在当前房间内停止发布本地音视频流。\n   * @return + 0: 方法调用成功\n   * + <0: 方法调用失败<br>\n   * @notes\n   * + 调用 publish{@link #publish} 手动发布音视频流后或在用户进房时设置为自动发布，都可以调用此接口停止发布。<br>\n   * + 用户调用此方法停止发布音视频流后，房间中的其他用户将会收到 onStreamRemove{@link 85533#onStreamRemove} 回调通知。\n   */\n\n\n  unpublish() {\n    return this.instance.Unpublish();\n  }\n  /** {zh}\n   * @region 多房间\n   * @brief 订阅房间内指定的远端音视频流。  或更新已经订阅的流的属性、媒体类型等配置。\n   * @param user_id 指定订阅的远端发布音视频流的用户 ID。\n   * @param stream_type 流属性，用于指定订阅主流/屏幕流。<li>主流。包括：由摄像头/麦克风通过内部采集机制，采集到的视频/音频;通过自定义采集，采集到的视频/音频。</li><li>屏幕流。屏幕共享时共享的视频流，或来自声卡的本地播放音频流。</li>\n   * @param media_type 媒体类型，用于指定订阅音/视频，参看 SubscribeMediaType{@link #SubscribeMediaType}。Todo\n   * @param subConfig 视频订阅配置\n   * @return + 0: 方法调用成功\n   * + <0: 方法调用失败<br>\n   * @notes\n   * + 无论是自动订阅还是手动订阅模式，你都可以调用此方法按需订阅房间中的音视频流。\n   * + 你可以通过 onStreamAdd{@link 85533#OnStreamAdd} 和 onStreamRemove{@link 85533#OnStreamRemove} 两个回调获取当前房间你的音视频流信息，并调用本方法按需订阅流或修改订阅配置。\n   * + 若订阅失败，你会收到 onRoomError{@link 85533#OnRoomError} 回调通知，具体失败原因参看 ErrorCode{@link #ErrorCode}。\n   * + 若调用 pauseAllSubscribedStream{@link #PauseAllSubscribedStream} 暂停接收远端音视频流，此时仍可使用该方法对暂停接收的流进行设置，你会在调用 resumeAllSubscribedStream{@link #ResumeAllSubscribedStream} 恢复接收流后收到修改设置后的流。\n   */\n\n\n  subscribeUserStream(user_id, stream_type, media_type, subConfig) {\n    // set default value\n    if (isNULL(subConfig.video_index)) {\n      subConfig.video_index = 0;\n    }\n\n    if (isNULL(subConfig.priority)) {\n      subConfig.priority = 0;\n    }\n\n    return this.instance.SubscribeUserStream(user_id, stream_type, media_type, subConfig);\n  }\n  /** {zh}\n   * @brief 停止订阅指定的房间内远端音视频流。无论自动订阅还是手动订阅模式，你都可以调用此方法取消已订阅的音视频流。\n   * @param userId 指定需要取消订阅的远端音视频流的所属的用户的 ID 。\n   * @param isScreen 是否为屏幕共享流。\n   * @return + 0: 方法调用成功\n   * + <0: 方法调用失败<br>\n   * @notes  无论是否订阅了指定的远端音视频流，都可以调用此方法。`userId` 无效或未订阅时，SDK 不会做任何处理，没有负面影响。\n   * */\n\n\n  unsubscribe(userId, isScreen) {\n    return this.instance.Unsubscribe(userId, isScreen);\n  }\n  /** {zh}\n   * @region 流消息\n   * @brief 给房间内指定的用户发送消息，返回这次发送消息的编号。  <br>调用该函数后会收到一次 userMessageSendResult 回调，告知发送结果。\n   * @param userId 指定用户 ID 。\n   * @param message 发送的消息内容。\n   * @return 这次发送消息的编号，从 1 开始递增。<br>\n   * @notes\n   * + 调用该函数后会收到一次 userMessageSendResult 回调；\n   * + 若消息发送成功，则 uid 所指定的用户会收到 userMessageReceived 回调。\n   */\n\n\n  sendUserMessage(userId, message) {\n    return this.instance.SendUserMessage(userId, message);\n  }\n  /** {zh}\n     * @type api\n     * @region 多房间\n     * @brief 给房间内指定的用户发送二进制消息（P2P）\n     * @param uid  消息接收用户的 ID\n     * @param message 二进制消息的内容。消息不超过 46KB。\n     * @return 这次发送消息的编号，从 1 开始递增。<br>\n     * @notes\n     * + 在发送房间内二进制消息前，必须先调用 joinRoom{@link #JoinRoom} 加入房间。  <br>\n     * + 调用该函数后会收到一次 onUserMessageSendResult{@link 85533#OnUserMessageSendResult} 回调，通知消息发送方发送成功或失败；  <br>\n     * + 若二进制消息发送成功，则 uid 所指定的用户会收到 onUserBinaryMessageReceived{@link 85533#OnUserBinaryMessageReceived} 回调。\n     */\n\n\n  sendUserBinaryMessage(uid, message) {\n    return this.instance.SendUserBinaryMessage(uid, message);\n  }\n\n  /** {zh}\n     * @type api\n     * @region 音频数据回调\n     * @brief 设置本地麦克风录制的音频数据回调参数\n     * @param sample_rate 音频采样率（单位HZ），可以设置的值有 8000，16000，32000，44100，48000\n     * @param channels 音频通道数，支持单声道（1）和双声道（2）\n     * @return + 0: 方法调用成功\n     * + <0: 方法调用失败<br>\n     * @notes 使用本方法设置参数后可以在 onRecordAudioFrame{@link 85533#OnRecordAudioFrame} 收到数据\n     */\n  setRecordingAudioFrameParameters(sample_rate, channels) {\n    return this.instance.SetRecordingAudioFrameParameters(sample_rate, channels);\n  }\n  /** {zh}\n   * @type api\n   * @region 音频数据回调\n   * @brief 设置远端所有用户音频数据混音后的音频数据回调参数\n   * @param sample_rate 音频采样率（单位HZ），可以设置的值有 8000，16000，32000，44100，48000\n   * @param channels 音频通道数，支持单声道（1）和双声道（2）\n   * @return + 0: 方法调用成功\n   * + <0: 方法调用失败<br>\n   * @notes 使用本方法设置参数后可以在 onPlaybackAudioFrame{@link 85533#OnPlaybackAudioFrame} 收到数据\n   */\n\n\n  setPlaybackAudioFrameParameters(sample_rate, channels) {\n    return this.instance.SetPlaybackAudioFrameParameters(sample_rate, channels);\n  }\n  /** {zh}\n   * @type api\n   * @region 音频数据回调\n   * @brief 设置本地麦克风录制音频数据和远端所有用户音频数据混音后的音频数据回调参数\n   * @param sample_rate 音频采样率（单位HZ），可以设置的值有 8000，16000，32000，44100，48000\n   * @param channels 音频通道数，支持单声道（1）和双声道（2）\n   * @return + 0: 方法调用成功\n   * + <0: 方法调用失败<br>\n   * @notes 使用本方法设置参数后可以在 onMixedAudioFrame{@link 85533#OnMixedAudioFrame} 收到数据\n   */\n\n\n  setMixedAudioFrameParameters(sample_rate, channels) {\n    return this.instance.SetMixedAudioFrameParameters(sample_rate, channels);\n  }\n  /** {zh}\n    * @type api\n    * @region 本地录制\n    * @brief 该方法将通话过程中的音视频数据录制到本地的文件中。\n    * @param type 流属性，指定录制主流还是屏幕流<li>主流。包括：由摄像头/麦克风通过内部采集机制，采集到的视频/音频;通过自定义采集，采集到的视频/音频。</li><li>屏幕流。屏幕共享时共享的视频流，或来自声卡的本地播放音频流。</li>\n    * @param config 本地录制参数配置\n    * @return\n    * + 0: 正常\n    * + -1: 参数设置异常\n    * + -2: 当前版本 SDK 不支持该特性，请联系技术支持人员<br>\n    * @notes\n    * + 调用该方法后，你会收到 onRecordingStateUpdate{@link 85533#OnRecordingStateUpdate} 回调。  <br>\n    * + 如果录制正常，系统每秒钟会通过 onRecordingProgressUpdate{@link 85533#OnRecordingProgressUpdate} 回调通知录制进度。\n    */\n\n\n  startFileRecording(type, config) {\n    return this.instance.StartFileRecording(type, config);\n  }\n  /** {zh}\n     * @type api\n     * @region 本地录制\n     * @brief 停止本地录制\n     * @param type 流属性，指定停止主流或者屏幕流录制<li>主流。包括：由摄像头/麦克风通过内部采集机制，采集到的视频/音频;通过自定义采集，采集到的视频/音频。</li><li>屏幕流。屏幕共享时共享的视频流，或来自声卡的本地播放音频流。</li>\n     * @return + 0: 方法调用成功\n     * + <0: 方法调用失败<br>\n    * @notes\n     * + 调用 startFileRecording{@link #StartFileRecording} 开启本地录制后，你必须调用该方法停止录制。  <br>\n     * + 调用该方法后，你会收到 onRecordingStateUpdate{@link 85533#OnRecordingStateUpdate} 回调提示录制结果。\n     */\n\n\n  stopFileRecording(type) {\n    return this.instance.StopFileRecording(type);\n  }\n  /** {zh}\n    * @type api\n    * @region 语音识别服务\n    * @brief 开启自动语音识别服务。该方法将识别后的用户语音转化成文字，并通过 onMessage{@link 85533#OnMessage} 事件回调给用户。\n    * @param asr_config 校验信息\n    * @return + 0: 方法调用成功\n    * + <0: 方法调用失败<br>\n  */\n\n\n  startASR(asr_config) {\n    return this.instance.StartASR(asr_config);\n  }\n  /** {zh}\n   * @type api\n   * @region 语音识别服务\n   * @brief 关闭语音识别服务\n   * @return + 0: 方法调用成功\n   * + <0: 方法调用失败<br>\n   */\n\n\n  stopASR() {\n    return this.instance.StopASR();\n  }\n  /** {zh}\n   * @type api\n   * @region 流消息\n   * @brief 给房间内所有的用户发送消息。\n   * @param message 用户发送的广播消息\n   * @return + 0: 方法调用成功\n   * + <0: 方法调用失败<br>\n   * @notes\n   * + 调用该函数后会收到一次 roomMessageSendResult 回调；\n   * + 同一房间内的其他用户会收到 roomMessageReceived 回调。\n   */\n\n\n  sendRoomMessage(message) {\n    return this.instance.SendRoomMessage(message);\n  }\n  /** {zh}\n     * @type api\n     * @region 多房间\n     * @brief 给房间内的所有其他用户发送广播消息。\n     * @param message  用户发送的二进制广播消息，每条消息不超过 46KB。\n     * @return + 0: 方法调用成功\n     * + <0: 方法调用失败<br>\n     * @notes\n     * + 在发送房间内二进制消息前，必须先调用 joinRoom{@link #JoinRoom} 加入房间。  <br>\n     * + 调用该函数后，会收到一次 onRoomMessageSendResult{@link 85533#OnRoomMessageSendResult} 回调。  <br>\n     * + 同一房间内的其他用户会收到 onRoomBinaryMessageReceived{@link 85533#OnRoomBinaryMessageReceived} 回调。\n     */\n\n\n  sendRoomBinaryMessage(message) {\n    return this.instance.SendRoomBinaryMessage(message);\n  }\n  /** {zh}\n   * @type api\n   * @region 实时消息通信\n   * @brief 必须先登录注册一个 uid，才能发送房间外消息和向业务服务器发送消息\n   * @param token 动态密钥  <br>用户登录必须携带的 Token，用于鉴权验证。  <br>本 Token 与加入房间时必须携带的 Token 不同。测试时可使用控制台生成临时 Token，正式上线需要使用密钥 SDK 在你的服务端生成并下发 Token。\n   * @param uid 用户 ID  <br>用户 ID 在 appid 的维度下是唯一的。\n   * @return + 0: 方法调用成功\n   * + <0: 方法调用失败<br>\n   * @notes\n   * + 在调用本接口登录后，如果想要登出，需要调用 Logout{@link #Logout}。  <br>\n   * + 本地用户调用此方法登录后，会收到 OnLoginResult{@link 85533#OnLoginResult} 回调通知登录结果，远端用户不会收到通知。\n   */\n\n\n  login(token, uid) {\n    return this.instance.Login(token, uid);\n  }\n  /** {zh}\n   * @type api\n   * @region 实时消息通信\n   * @brief 调用本接口登出后，无法调用房间外消息以及端到服务器消息相关的方法或收到相关回调。\n   * @return + 0: 方法调用成功\n   * + <0: 方法调用失败<br>\n   * @notes\n   * + 调用本接口登出前，必须先调用 login{@link #Login} 登录。  <br>\n   * + 本地用户调用此方法登出后，会收到 onLogout{@link 85533#OnLogout} 回调通知结果，远端用户不会收到通知。\n   */\n\n\n  logout() {\n    return this.instance.Logout();\n  }\n  /** {zh}\n   * @type api\n   * @region 实时消息通信\n   * @brief 更新用户用于登录的 Token  <br>Token 有一定的有效期，当 Token 过期时，需调用此方法更新登录的 Token 信息。  <br>调用 login{@link #Login} 方法登录时，如果使用了过期的 Token 将导致登录失败，并会收到 onLoginResult{@link 85533#OnLoginResult} 回调通知，错误码为 kLoginErrorCodeInvalidToken。此时需要重新获取 Token，并调用此方法更新 Token。\n   * @param token  <br>\n   *        更新的动态密钥\n   * @return + 0: 方法调用成功\n   * + <0: 方法调用失败<br>\n   * @notes\n   * + 如果 Token 无效导致登录失败，则调用此方法更新 Token 后，SDK 会自动重新登录，而用户不需要自己调用 login{@link #Login} 方法。  <br>\n   * + Token 过期时，如果已经成功登录，则不会受到影响。Token 过期的错误会在下一次使用过期 Token 登录时，或因本地网络状况不佳导致断网重新登录时通知给用户。\n   */\n\n\n  updateLoginToken(token) {\n    return this.instance.UpdateLoginToken(token);\n  }\n  /** {zh}\n     * @type api\n     * @region 实时消息通信\n     * @brief 设置业务服务器参数  <br>客户端调用 sendServerMessage{@link #SendServerMessage} 或 sendServerBinaryMessage{@link #SendServerBinaryMessage} 发送消息给业务服务器之前，必须设置有效签名和业务服务器地址。\n     * @param signature 动态签名  <br>业务服务器会使用该签名对请求进行鉴权验证。\n     * @param url 业务服务器的地址\n     * @return + 0: 方法调用成功\n     * + <0: 方法调用失败<br>\n     * @notes\n     * + 用户必须调用 login{@link #Login} 登录后，才能调用本接口。  <br>\n     * + 调用本接口后，SDK 会使用 onServerParamsSetResult{@link 85533#OnServerParamsSetResult} 返回相应结果。\n     */\n\n\n  setServerParams(signature, url) {\n    return this.instance.SetServerParams(signature, url);\n  }\n  /** {zh}\n     * @type api\n     * @region 实时消息通信\n     * @brief 查询对端用户或本端用户的登录状态\n     * @param peer_user_id 需要查询的用户 ID\n     * @return + 0: 方法调用成功\n     * + <0: 方法调用失败<br>\n     * @notes\n     * + 必须调用 login{@link #Login} 登录后，才能调用本接口。  <br>\n     * + 调用本接口后，SDK 会使用 onGetPeerOnlineStatus{@link 85533#OnGetPeerOnlineStatus} 回调通知查询结果。  <br>\n     * + 在发送房间外消息之前，用户可以通过本接口了解对端用户是否登录，从而决定是否发送消息。也可以通过本接口查询自己查看自己的登录状态。\n     */\n\n\n  getPeerOnlineStatus(peer_user_id) {\n    return this.instance.GetPeerOnlineStatus(peer_user_id);\n  }\n  /** {zh}\n     * @type api\n     * @region 实时消息通信\n     * @brief 给房间外指定的用户发送文本消息（P2P）\n     * @param uid 消息接收用户的 ID\n     * @param message 发送的文本消息内容，消息不超过 62KB。\n     * @return\n     * + \\>0：发送成功，返回这次发送消息的编号，从 1 开始递增\n     * + -1：发送失败，RtcEngine 实例未创建\n     * + -2：发送失败，uid 为空<br>\n     * @notes\n     * + 在发送房间外文本消息前，必须先调用 login{@link #Login} 完成登录。  <br>\n     * + 用户调用本接口发送文本信息后，会收到一次 onUserMessageSendResultOutsideRoom{@link 85533#OnUserMessageSendResultOutsideRoom} 回调，得知消息是否成功发送。  <br>\n     * + 若文本消息发送成功，则 uid 所指定的用户会通过 onUserMessageReceivedOutsideRoom{@link 85533#OnUserMessageReceivedOutsideRoom} 回调收到该消息。\n     */\n\n\n  SendUserMessageOutsideRoom(uid, message) {\n    return this.instance.SendUserMessageOutsideRoom(uid, message);\n  }\n  /** {zh}\n   * @type api\n   * @region 错误码\n   * @brief 获取错误码的描述\n   * @param code 需要获取描述的错误码\n   * @return 错误码的描述<br>\n   * @notes  该接口是通用功能，调用时不需要依赖引擎对象。\n   */\n\n\n  getErrorDescription(code) {\n    return this.instance.GetErrorDescription(code);\n  }\n  /** {zh}\n     * @type api\n     * @region 实时消息通信\n     * @brief 给房间外指定的用户发送二进制消息（P2P）\n     * @param uid 消息接收用户的 ID\n     * @param message 发送的二进制消息内容，每条消息不超过 46KB。\n     * @return\n     * + >0：发送成功，返回这次发送消息的编号，从 1 开始递增  <br>\n     * + -1：发送失败，RtcEngine 实例未创建  <br>\n     * + -2：发送失败，uid 为空<br>\n     * @notes\n     * + 在发送房间外二进制消息前，必须先调用 login{@link #Login} 完成登录。  <br>\n     * + 用户调用本接口发送二进制消息后，会收到一次 onUserMessageSendResultOutsideRoom{@link 85533#OnUserMessageSendResultOutsideRoom} 回调，通知消息是否发送成功。  <br>\n     * + 若二进制消息发送成功，则 uid 所指定的用户会通过 onUserBinaryMessageReceivedOutsideRoom{@link 85533#OnUserBinaryMessageReceivedOutsideRoom} 回调收到该条消息。\n     */\n\n\n  sendUserBinaryMessageOutsideRoom(uid, message) {\n    return this.instance.SendUserBinaryMessageOutsideRoom(uid, message);\n  }\n  /** {zh}\n   * @type api\n   * @region 实时消息通信\n   * @brief 客户端给业务服务器发送文本消息（P2Server）\n   * @param message 发送的文本消息内容，消息不超过 62KB。\n   * @return\n   * + \\>0：发送成功，返回这次发送消息的编号，从 1 开始递增\n   * + -1：发送失败，RtcEngine 实例未创建<br>\n   * @notes\n   * + 在向业务服务器发送文本消息前，必须先调用 Login{@link #Login} 完成登录，随后调用 SetServerParams{@link #SetServerParams} 设置业务服务器。  <br>\n   * + 调用本接口后，会收到一次 OnServerMessageSendResult{@link 85533#OnServerMessageSendResult} 回调，通知消息发送方是否发送成功。  <br>\n   * + 若文本消息发送成功，则之前调用 SetServerParams{@link #SetServerParams} 设置的业务服务器会收到该条消息。\n   */\n\n\n  sendServerMessage(message) {\n    return this.instance.SendServerMessage(message);\n  }\n  /** {zh}\n   * @type api\n   * @region 实时消息通信\n   * @brief 客户端给业务服务器发送二进制消息（P2Server）\n   * @param length 二进制字符串的长度\n   * @param message 发送的二进制消息内容，消息不超过 46KB。\n   * @return\n   * + \\>0：发送成功，返回这次发送消息的编号，从 1 开始递增  <br>\n   * + -1：发送失败，RtcEngine 实例未创建<br>\n   * @notes\n   * + 在向业务服务器发送二进制消息前，必须先调用 login{@link #Login} 完成登录，随后调用 setServerParams{@link #SetServerParams} 设置业务服务器。  <br>\n   * + 调用本接口后，会收到一次 onServerMessageSendResult{@link 85533#OnServerMessageSendResult} 回调，通知消息发送方发送成功或失败。  <br>\n   * + 若二进制消息发送成功，则之前调用 setServerParams{@link #SetServerParams} 设置的业务服务器会收到该条消息。\n   */\n\n\n  sendServerBinaryMessage(message) {\n    return this.instance.SendServerBinaryMessage(message);\n  }\n  /** {zh}\n   * @type api\n   * @region 视频特效\n   * @brief 关闭背景\n   * @return + 0: 方法调用成功\n   * + <0: 方法调用失败\n   */\n\n\n  disableBackground() {\n    return this.instance.DisableBackground();\n  }\n  /** {zh}\n   * @type api\n   * @region 视频特效\n   * @brief 视频特效许可证检查\n   * @param licensePath 许可证文件绝对路径\n   * @return\n   * + 0: 调用成功  <br>\n   * + 1000: 未集成 CV SDK  <br>\n   * + 1001: 本RTC版本不支持cv功能  <br>\n   * + <0: 调用失败，具体错误码请参考 [CV SDK 文档](http://ailab-cv-sdk.bytedance.com/docs/2036/99783/)。<br>\n   * @notes 开始使用视频特效前，你必须先调用这个方法进行许可证验证\n   */\n\n\n  checkLicense(licensePath) {\n    return this.instance.CheckLicense(licensePath);\n  }\n  /** {zh}\n   * @type api\n   * @region 视频特效\n   * @brief 从 CV SDK 获取授权消息，用于获取在线许可证。\n   * @return 授权消息字符串<br>\n   * @notes\n   * + 使用 CV 的功能前，你必须获取 CV SDK 的在线许可证。  <br>\n   * + 通过此接口获取授权消息后，你必须参考 [在线授权说明](http://ailab-cv-sdk.bytedance.com/docs/2036/99798/)。 <br>\n   * + 自行实现获取在线许可证的业务逻辑。获取许可证后，你必须调用 CheckLicense{@link #CheckLicense} 确认许可证有效。然后，你才可以使用 CV 功能。  <br>\n   * + 获取授权消息后，调用 FreeAuthMessage{@link #FreeAuthMessage} 释放内存。\n   */\n\n\n  getAuthMessage() {\n    return this.instance.GetAuthMessage();\n  }\n  /** {zh}\n   * @type api\n   * @region 视频特效\n   * @brief 开启关闭视频特效\n   * @param enabled 是否开启特效，true: 开启，false: 关闭\n   * @return\n   * + 0: 调用成功\n   * + 1000: 未集成 CV SDK\n   * + <0: 调用失败，具体错误码含义请参考 CV SDK 文档<br>\n   * @notes 在调用 checkLicense{@link #CheckLicense}和 setAlgoModelPath{@link #SetAlgoModelPath}后调用此方法\n   */\n\n\n  enableEffect(enabled) {\n    return this.instance.EnableEffect(enabled);\n  }\n  /** {zh}\n     * @region 视频特效\n     * @brief 设置视频特效算法模型路径\n     * @param modelPath 模型路径\n     * @return\n     * + 0：成功\n     * + !0：失败<br>\n     */\n\n\n  setAlgoModelPath(modelPath) {\n    return this.instance.SetAlgoModelPath(modelPath);\n  }\n  /** {zh}\n   * @type api\n   * @region 视频特效\n   * @brief 设置视频特效素材包，支持同时设置多个素材包\n   * @param effectNodePaths  特效素材包路径数组\n   * @return  <br>\n   * + 0: 调用成功  <br>\n   * + 1000: 未集成 CV SDK  <br>\n   * + 1001: 本 RTC 版本不支持 CV 功能  <br>\n   * + <0: 调用失败，具体错误码，请参考 [CV SDK 文档](http://ailab-cv-sdk.bytedance.com/docs/2036/99783/)。\n   */\n\n\n  setEffectNodes(effectNodePaths) {\n    return this.instance.SetEffectNodes(effectNodePaths);\n  }\n  /** {zh}\n   * @type api\n   * @region 视频特效\n   * @brief 设置特效强度\n   * @param nodePath 特效素材包路径\n   * @param nodeKey 需要设置的素材 key 名称，取值请参考 [素材 key 对应说明](http://ailab-cv-sdk.bytedance.com/docs/2036/99769/)。\n   * @param nodeValue 需要设置的强度值 取值范围 [0,1]，超出范围时设置无效。\n   * @return  <br>\n   * + 0: 调用成功  <br>\n   * + 1000: 未集成 CV SDK  <br>\n   * + 1001: 本 RTC 版本不支持 CV 功能  <br>\n   * + <0: 调用失败，具体错误码，请参考 [CV SDK 文档](http://ailab-cv-sdk.bytedance.com/docs/2036/99783/)。\n   */\n\n\n  updateNode(nodePath, nodeKey, nodeValue) {\n    return this.instance.UpdateNode(nodePath, nodeKey, nodeValue);\n  }\n  /** {zh}\n   * @type api\n   * @region 视频特效\n   * @brief 设置颜色滤镜\n   * @param resPath 滤镜资源包绝对路径。\n   * @return  <br>\n   * + 0: 调用成功  <br>\n   * + 1000: 未集成 CV SDK  <br>\n   * + 1001: 本 RTC 版本不支持 CV 功能  <br>\n   * + <0: 调用失败，具体错误码，请参考 [CV SDK 文档](http://ailab-cv-sdk.bytedance.com/docs/2036/99783/)。\n   */\n\n\n  setColorFilter(resPath) {\n    return this.instance.SetColorFilter(resPath);\n  }\n  /** {zh}\n   * @type api\n   * @region 视频特效\n   * @brief 设置已启用颜色滤镜的强度\n   * @param intensity 滤镜强度。取值范围 [0,1]，超出范围时设置无效。\n   * @return  <br>\n   * + 0: 调用成功  <br>\n   * + 1000: 未集成 CV SDK  <br>\n   * + 1001: 本 RTC 版本不支持 CV 功能  <br>\n   * + <0: 调用失败，具体错误码，请参考 [CV SDK 文档](http://ailab-cv-sdk.bytedance.com/docs/2036/99783/)。\n   */\n\n\n  setColorFilterIntensity(intensity) {\n    return this.instance.SetColorFilterIntensity(intensity);\n  }\n  /** {zh}\n   * @type api\n   * @region 视频管理\n   * @brief 在视频通信时，通过视频帧发送 SEI 数据。\n   * @param stream_index 媒体流类型<li>主流。包括：由摄像头/麦克风通过内部采集机制，采集到的视频/音频;通过自定义采集，采集到的视频/音频。</li><li>屏幕流。屏幕共享时共享的视频流，或来自声卡的本地播放音频流。</li>\n   * @param message SEI 消息。长度不超过 4 kB。<br>\n   * @param repeat_count 消息发送重复次数。取值范围是 [0, 30]。<br>调用此接口后，SEI 数据会添加到当前视频帧开始的连续 `repeatCount` 个视频帧中。\n   * @return\n   * + \\>=0: 将被添加到视频帧中的 SEI 的数量  <br>\n   * + < 0: 发送失败<br>\n   * @notes\n   * + 如果调用此接口之后的 2s 内，没有可带 SEI 的视频帧（比如没有开启视频采集和传输），那么，SEI 数据不会被加进视频帧中。\n   * + 消息发送成功后，远端会收到 OnSEIMessageReceived{@link 85533#OnSEIMessageReceived} 回调。\n   */\n\n\n  sendSEIMessage(stream_index, message, repeat_count) {\n    return this.instance.SendSEIMessage(stream_index, message, repeat_count);\n  }\n  /** {zh}\n   * @type api\n   * @region 屏幕共享\n   * @brief 在屏幕共享时，设置屏幕音频流和麦克风采集到的音频流的混流方式\n   * @param index 混流方式 <li> `0`: 将屏幕音频流和麦克风采集到的音频流混流 </li><li> `1`: 将屏幕音频流和麦克风采集到的音频流分为两路音频流</li>\n   * @notes 你应该在 publishScreen{@link #PublishScreen} 之前，调用此方法。否则，你将收到 onWarning{@link 85533#OnWarning} `的报错：kWarningCodeSetScreenAudioStreamIndexFailed`\n   */\n\n\n  setScreenAudioStreamIndex(index) {\n    return this.instance.SetScreenAudioStreamIndex(index);\n  }\n  /** {zh}\n   * @type api\n   * @region 屏幕共享\n   * @brief 在屏幕共享时，开始使用 RTC SDK 内部采集方式，采集屏幕音频\n   * @notes\n   * + 采集后，你还需要调用 publishScreen{@link #PublishScreen} 将采集到的屏幕音频推送到远端。<br>\n   * + 要关闭屏幕音频内部采集，调用 stopScreenAudioCapture{@link #StopScreenAudioCapture}。\n   */\n\n\n  startScreenAudioCapture() {\n    return this.instance.StartScreenAudioCapture();\n  }\n  /** {zh}\n    * @type api\n    * @region 屏幕共享\n    * @brief 在屏幕共享时，停止使用 RTC SDK 内部采集方式，采集屏幕音频。\n    * @notes 要开始屏幕音频内部采集，调用 startScreenAudioCapture{@link #StartScreenAudioCapture}。\n    */\n\n\n  stopScreenAudioCapture() {\n    return this.instance.StopScreenAudioCapture();\n  }\n  /** {zh}\n   * @type api\n   * @region 引擎管理\n   * @brief 本次通话质量打分评价\n   * @param data 上报的数据。JSON 格式，必须包含以下 4 个字段: <li>types: 预设的问题类型集合，可多选。具体的类型参考 [ProblemFeedbackOption](../70098/#problemfeedbackoption) </li><li>problem_desc: 预设问题以外的其他问题的具体描述 </li><li>os_version: 系统版本  </li><li>network_type: 网络类型：包括 WiFi, 2g, 3g, 4g, 5g。如果是台式机，填写 pc 。</li>\n   * @return\n   * +  0: 上报成功  <br>\n   * + -1: 上报失败，还没加入过房间  <br>\n   * + -2: 上报失败，传入 JSON 解析失败<br> <br>\n   * + -3: 上报失败，传入 JSON 字段缺失  <br>\n   * @notes + 如果用户上报时在房间内，那么问题会定位到用户当前所在的一个或多个房间；\n   * + 如果用户上报时不在房间内，那么问题会定位到引擎此前退出的房间。\n   */\n\n\n  feedback(data) {\n    return this.instance.Feedback(data);\n  }\n  /** {zh}\n     * @private\n     * @type api\n     * @region 引擎管理\n     * @brief 设置运行时的参数\n     * @param json_string  json 序列化之后的字符串，用以覆盖全局参数，详情可见 setParameters，业务方传入\n     */\n\n\n  setRuntimeParameters(json_string) {\n    return this.instance.SetRuntimeParameters(json_string);\n  }\n  /** {zh}\n     * @region 加密\n     * @brief 设置传输时使用内置加密的方式\n     * @param encrypt_type 加密算法 <li>`0`: 不使用内置加密</li><li>`1`:AES-128-CBC 加密算法</li><li>`2`:AES-256-CBC 加密算法</li><li>`3`:AES-128-ECB 加密算法</li><li>`4`:AES-256-ECB 加密算法</li>\n     * @param key 加密密钥，长度限制为 36 位，超出部分将会被截断\n     * @notes\n     * + 使用传输时内置加密时，使用此方法；如果需要使用传输时自定义加密，参看 onEncryptData{@link 85533#OnEncryptData}。 内置加密和自定义加密互斥，根据最后一个调用的方法确定传输是加密的方案。  <br>\n   *         + 该方法必须在进房之前调用，可重复调用，以最后调用的参数作为生效参数。  <br>\n   */\n\n\n  setEncryptInfo(encrypt_type, key) {\n    return this.instance.SetEncryptInfo(encrypt_type, key);\n  }\n  /**\n   * @type api\n   * @region 多房间\n   * @brief 开启转推直播，并设置合流的视频视图布局和音频属性。\n   * @param [in] param 转推直播配置参数。参看 ITranscoderParam{@link #ITranscoderParam}。\n   * @notes <br>\n   *        1.只有房间模式为直播模式的用户才能调用此方法。  <br>\n   *        2.调用该方法后，关于启动结果和推流过程中的错误，会收到 OnLiveTranscodingResult{@link #IRTCRoomEventHandler#OnLiveTranscodingResult} 回调。\n   *        3.调用 StopLiveTranscoding{@link #IRtcRoom#StopLiveTranscoding} 停止转推直播。\n   */\n\n\n  startLiveTranscoding(param) {\n    if (!param.audioParam) {\n      param.audioParam = {\n        i32_sample_rate: 48000,\n        i32_channel_num: 2,\n        i32_bitrate_kbps: 128,\n        audio_codec_profile: 0\n      };\n    }\n\n    if (!param.videoParam) {\n      param.videoParam = {\n        i32_width: 640,\n        i32_height: 360,\n        i32_fps: 15,\n        i32_gop: 60,\n        i32_bitrate_kpbs: 500,\n        video_codec_profile: 0,\n        lowLatency: false\n      };\n    }\n\n    if (!param.backgroundColor) {\n      param.backgroundColor = \"0x000000\";\n    }\n\n    return this.instance.StartLiveTranscoding(param);\n  }\n  /**\n   * @type api\n   * @region 多房间\n   * @brief 停止转推直播。<br>\n   *        关于启动转推直播，参看 StartLiveTranscoding{@link #IRtcRoom#StartLiveTranscoding}。\n   */\n\n\n  stopLiveTranscoding() {\n    return this.instance.StopLiveTranscoding();\n  }\n  /**\n     * @hidden(macOS,Windows)\n     * @type api\n     * @region 多房间\n     * @brief 更新转推直播参数。  <br>\n     *        使用 StartLiveTranscoding{@link #IRtcRoom#StartLiveTranscoding} 启用转推直播功能后，使用此方法更新功能配置参数。\n     * @param [in] param 配置参数，参看 ITranscoderParam{@link #ITranscoderParam}\n     */\n\n\n  updateLiveTranscoding(param) {\n    if (!param.audioParam) {\n      param.audioParam = {\n        i32_sample_rate: 48000,\n        i32_channel_num: 2,\n        i32_bitrate_kbps: 128,\n        audio_codec_profile: 0\n      };\n    }\n\n    if (!param.videoParam) {\n      param.videoParam = {\n        i32_width: 640,\n        i32_height: 360,\n        i32_fps: 15,\n        i32_gop: 60,\n        i32_bitrate_kpbs: 500,\n        video_codec_profile: 0,\n        lowLatency: false\n      };\n    }\n\n    if (!param.backgroundColor) {\n      param.backgroundColor = \"0x000000\";\n    }\n\n    return this.instance.UpdateLiveTranscoding(param);\n  } //////////////////////////////////////////////\n  // JS Render Frame\n\n  /**\n   * @private\n   */\n\n\n  setupLocalVideo(view, renderOptions = {\n    renderMode: types_1.RenderMode.FIT,\n    mirror: false\n  }) {\n    let ret = -1;\n\n    do {\n      if (!view) {\n        logger.warn(\"SetupLocalVideo, view is null\");\n        break;\n      }\n\n      let user = this.localUser;\n\n      if (user) {\n        user.renderOptions = renderOptions;\n\n        if (!user.videoRender) {\n          user.videoRender = new yuv_render_1.YUVRender(view, renderOptions.renderMode, renderOptions.mirror, {});\n        } else {\n          logger.info(\"videoRender is not null\");\n        }\n      } else {\n        logger.warn(\"user is null\");\n      }\n\n      ret = 0;\n    } while (false);\n\n    return ret;\n  }\n  /**\n   * @private\n   */\n\n\n  removeLocalVideo() {\n    let user = this.localUser;\n\n    if (user && user.videoRender) {\n      user.videoRender.destroy();\n      user.videoRender = null;\n    }\n\n    return 0;\n  }\n  /**\n   * @private\n   */\n\n\n  setupRemoteVideo(userId, view, renderOptions = {\n    renderMode: types_1.RenderMode.FIT,\n    mirror: false\n  }) {\n    let ret = -1;\n\n    do {\n      if (!view) {\n        logger.warn(\"SetupRemoteVideo, view is null\");\n        break;\n      }\n\n      let user = this.findUser(userId);\n\n      if (user) {\n        if (!user.videoRender) {\n          user.videoRender = new yuv_render_1.YUVRender(view, renderOptions.renderMode, renderOptions.mirror, {});\n        }\n      }\n\n      ret = 0;\n    } while (false);\n\n    return ret;\n  }\n  /**\n   * @private\n   */\n\n\n  removeRemoteVideo(userId) {\n    let user = this.findUser(userId);\n\n    if (user && user.videoRender) {\n      user.videoRender.destroy();\n      user.videoRender = null;\n    }\n\n    return 0;\n  }\n  /**\n   * @private\n   */\n\n\n  removeAllRemoteVideo(userId) {\n    var _a;\n\n    (_a = this.remoteUsers) === null || _a === void 0 ? void 0 : _a.forEach(user => {\n      if (user && user.videoRender) {\n        user.videoRender.destroy();\n        user.videoRender = null;\n      }\n    });\n    return 0;\n  }\n  /**\n   * @private\n   */\n\n\n  setupLocalScreen(view, renderOptions = {\n    renderMode: types_1.RenderMode.FIT,\n    mirror: false\n  }) {\n    let ret = -1;\n\n    do {\n      if (!view) {\n        logger.warn(\"SetupLocalScreen, view is null\");\n        break;\n      }\n\n      let user = this.localUser;\n\n      if (user) {\n        if (!user.screenRender) {\n          user.screenRender = new yuv_render_1.YUVRender(view, renderOptions.renderMode, renderOptions.mirror, {});\n        }\n      }\n\n      ret = 0;\n    } while (false);\n\n    return ret;\n  }\n  /**\n   * @private\n   */\n\n\n  removeLocalScreen() {\n    let user = this.localUser;\n\n    if (user && user.screenRender) {\n      user.screenRender.destroy();\n      user.screenRender = null;\n    }\n\n    return 0;\n  }\n  /**\n   * @private\n   */\n\n\n  setupRemoteScreen(userId, view, renderOptions = {\n    renderMode: types_1.RenderMode.FIT,\n    mirror: false\n  }) {\n    let ret = -1;\n\n    do {\n      if (!view) {\n        logger.warn(\"SetupRemoteScreen, view is null\");\n        break;\n      }\n\n      let user = this.findUser(userId);\n\n      if (user) {\n        if (!user.screenRender) {\n          user.screenRender = new yuv_render_1.YUVRender(view, renderOptions.renderMode, renderOptions.mirror, {});\n        }\n      }\n\n      ret = 0;\n    } while (false);\n\n    return ret;\n  }\n  /**\n   * @private\n   */\n\n\n  removeRemoteScreen(userId) {\n    let user = this.findUser(userId);\n\n    if (user && user.screenRender) {\n      user.screenRender.destroy();\n      user.screenRender = null;\n    }\n\n    return 0;\n  } ////////////////////////////////////////////////////////////////////////////////////////////////\n  // callback\n\n  /**\n   * @private\n   */\n\n\n  OnLeaveRoom() {\n    var _a;\n\n    this.removeLocalVideo();\n    this.removeLocalScreen();\n    (_a = this.remoteUsers) === null || _a === void 0 ? void 0 : _a.forEach(ele => {\n      this.removeRemoteVideo(ele.userId);\n      this.removeRemoteScreen(ele.userId);\n    });\n  }\n  /**\n   * @private\n   */\n\n\n  OnUserJoined(obj) {\n    var _a;\n\n    const {\n      userId\n    } = obj.Object;\n    this.removeRemoteVideo(userId);\n    this.removeRemoteScreen(userId);\n    let userInfo = {\n      userId: userId\n    };\n    (_a = this.remoteUsers) === null || _a === void 0 ? void 0 : _a.set(userId, userInfo);\n  }\n  /**\n   * @private\n   */\n\n\n  OnUserLeave(obj) {\n    var _a;\n\n    const {\n      userId\n    } = obj.Object;\n    this.removeRemoteVideo(userId);\n    this.removeRemoteScreen(userId);\n    (_a = this.remoteUsers) === null || _a === void 0 ? void 0 : _a.delete(userId);\n  }\n  /**\n   * @private\n   */\n\n\n  OnLocalVideoFrame(obj) {\n    let user = this.findUser(this.localUserId);\n\n    if (user && user.videoRender) {\n      let frame = yuv_render_1.YUVRender.buildYUVFrame(obj.Object);\n      user.videoRender.renderFrame(frame);\n    }\n  }\n  /**\n   * @private\n   */\n\n\n  OnLocalScreenFrame(obj) {\n    let user = this.findUser(this.localUserId);\n\n    if (user && user.screenRender) {\n      let frame = yuv_render_1.YUVRender.buildYUVFrame(obj.Object);\n      user.screenRender.renderFrame(frame);\n    }\n  }\n  /**\n   * @private\n   */\n\n\n  OnRemoteVideoFrame(obj) {\n    var _a, _b;\n\n    let user = (_a = this.remoteUsers) === null || _a === void 0 ? void 0 : _a.get(obj.Object.userId);\n\n    if ((_b = user) === null || _b === void 0 ? void 0 : _b.videoRender) {\n      let frame = yuv_render_1.YUVRender.buildYUVFrame(obj.Object);\n      user.videoRender.renderFrame(frame);\n    }\n  }\n  /**\n   * @private\n   */\n\n\n  OnRemoteScreenFrame(obj) {\n    var _a, _b;\n\n    let user = (_a = this.remoteUsers) === null || _a === void 0 ? void 0 : _a.get(obj.Object.userId);\n\n    if ((_b = user) === null || _b === void 0 ? void 0 : _b.screenRender) {\n      let frame = yuv_render_1.YUVRender.buildYUVFrame(obj.Object);\n      user.screenRender.renderFrame(frame);\n    }\n  }\n  /**\n   * @private\n   */\n\n\n  clearLocalVideoCanvas() {\n    let user = this.findUser(this.localUserId);\n\n    if (user && user.videoRender) {\n      user.videoRender.clearFrame();\n    }\n  }\n  /**\n   * @private\n   */\n\n\n  clearLocalScreenCanvas() {\n    let user = this.findUser(this.localUserId);\n\n    if (user && user.screenRender) {\n      user.screenRender.clearFrame();\n    }\n  }\n  /**\n   * @private\n   */\n\n\n  clearRemoteVideoCanvas(userId) {\n    var _a;\n\n    let user = (_a = this.remoteUsers) === null || _a === void 0 ? void 0 : _a.get(userId);\n\n    if (user && user.videoRender) {\n      user.videoRender.clearFrame();\n    }\n  }\n  /**\n   * @private\n   */\n\n\n  clearAllRemoteVideoCanvas() {\n    if (!this.remoteUsers) {\n      return;\n    }\n\n    this.remoteUsers.forEach(user => {\n      if (user && user.videoRender) {\n        user.videoRender.clearFrame();\n      }\n    });\n  }\n  /**\n   * @private\n   */\n\n\n  clearRemoteScreenCanvas(userId) {\n    var _a;\n\n    let user = (_a = this.remoteUsers) === null || _a === void 0 ? void 0 : _a.get(userId);\n\n    if (user && user.screenRender) {\n      user.screenRender.clearFrame();\n    }\n  }\n  /**\n   * @private\n   */\n\n\n  cbEngine(obj) {\n    this.processCallback(obj);\n  }\n  /**\n   * @private\n   */\n\n\n  fire(event, ...args) {\n    setImmediate(() => {\n      this.emit(event, ...args);\n    });\n  }\n\n  /**\n   * @private\n   */\n  processCallback(obj) {\n    let type = obj.Type;\n    let data = obj.Object;\n\n    switch (type) {\n      case \"OnLocalScreenFrame\":\n        {\n          this.convertYUVBuffer(obj);\n          this.OnLocalScreenFrame(obj);\n          this.fire('OnLocalScreenFrame', data);\n        }\n        break;\n\n      case \"OnLocalVideoFrame\":\n        {\n          this.convertYUVBuffer(obj);\n          this.OnLocalVideoFrame(obj);\n          this.fire('OnLocalVideoFrame', data);\n        }\n        break;\n\n      case \"OnRemoteScreenFrame\":\n        {\n          this.convertYUVBuffer(obj);\n          this.OnRemoteScreenFrame(obj);\n          this.fire('OnRemoteScreenFrame', data);\n        }\n        break;\n\n      case \"OnRemoteVideoFrame\":\n        {\n          this.convertYUVBuffer(obj);\n          this.OnRemoteVideoFrame(obj);\n          this.fire('OnRemoteVideoFrame', data);\n        }\n        break;\n\n      case \"OnWarning\":\n        {\n          this.fire('OnWarning', data.warn);\n        }\n        break;\n\n      case \"OnError\":\n        {\n          this.fire('OnError', data.error);\n        }\n        break;\n\n      case \"OnAudioMixingFinished\":\n        {\n          this.fire('OnAudioMixingFinished');\n        }\n        break;\n\n      case \"OnAudioMixingStateChanged\":\n        {\n          this.fire('OnAudioMixingStateChanged', data.id, data.state, data.error);\n        }\n        break;\n\n      case \"OnLogReport\":\n        {\n          this.fire('OnLogReport', data.logType, data.logContent);\n        }\n        break;\n\n      case \"OnConnectionStateChanged\":\n        {\n          this.fire('OnConnectionStateChanged', data.state);\n        }\n        break;\n\n      case \"OnNetworkTypeChanged\":\n        {\n          this.fire('OnNetworkTypeChanged', data.type);\n        }\n        break;\n\n      case \"OnPerformanceAlarms\":\n        {\n          this.fire('OnPerformanceAlarms', data.mode, data.roomId, data.reason, data.sourceWantedDataObject);\n        }\n        break;\n\n      case \"OnMediaDeviceStateChanged\":\n        {\n          this.fire('OnMediaDeviceStateChanged', data.deviceId, data.deviceType, data.deviceState, data.deviceError);\n        }\n        break;\n\n      case \"OnSysStats\":\n        {\n          let sysStats = data;\n          this.fire('OnSysStats', sysStats);\n        }\n        break;\n\n      case \"OnJoinRoomResult\":\n        {\n          this.fire('OnJoinRoomResult', data.roomId, data.userId, data.errorCode, data.joinType, data.elapsed);\n        }\n        break;\n\n      case \"OnLeaveRoom\":\n        {\n          this.OnLeaveRoom();\n          this.fire('OnLeaveRoom', data);\n        }\n        break;\n\n      case \"OnRoomWarning\":\n        {\n          this.fire('OnRoomWarning', data);\n        }\n        break;\n\n      case \"OnRoomError\":\n        {\n          this.fire('OnRoomError', data);\n        }\n        break;\n\n      case \"OnRoomStats\":\n        {\n          let rtcStats = data;\n          this.fire('OnRoomStats', rtcStats);\n        }\n        break;\n\n      case \"OnLocalStreamStats\":\n        {\n          let stats = data;\n          this.fire('OnLocalStreamStats', stats);\n        }\n        break;\n\n      case \"OnRemoteStreamStats\":\n        {\n          let stats = data;\n          this.fire('OnRemoteStreamStats', stats);\n        }\n        break;\n\n      case \"OnUserJoined\":\n        {\n          this.OnUserJoined(obj);\n          this.fire('OnUserJoined', data.userId, data.elapsed);\n        }\n        break;\n\n      case \"OnUserLeave\":\n        {\n          this.OnUserLeave(obj);\n          this.fire('OnUserLeave', data.userId, data.reason);\n        }\n        break;\n\n      case \"OnMuteAllRemoteAudio\":\n        {\n          this.fire('OnMuteAllRemoteAudio', data.userId, data.muteState);\n        }\n        break;\n\n      case \"OnMuteAllRemoteVideo\":\n        {\n          this.fire('OnMuteAllRemoteVideo', data.userId, data.muteState);\n        }\n        break;\n\n      case \"OnUserMuteAudio\":\n        {\n          this.fire('OnUserMuteAudio', data.userId, data.muteState);\n        }\n        break;\n\n      case \"OnUserStartAudioCapture\":\n        {\n          this.fire('OnUserStartAudioCapture', data.userId);\n        }\n        break;\n\n      case \"OnUserStopAudioCapture\":\n        {\n          this.fire('OnUserStopAudioCapture', data.userId);\n        }\n        break;\n\n      case \"OnFirstLocalAudioFrame\":\n        {\n          this.fire('OnFirstLocalAudioFrame', data.streamIndex);\n        }\n        break;\n\n      case \"OnFirstRemoteAudioFrame\":\n        {\n          let remoteKey = data;\n          this.fire('OnFirstRemoteAudioFrame', remoteKey);\n        }\n        break;\n\n      case \"OnStreamRemove\":\n        {\n          this.fire('OnStreamRemove', data.stream, data.reason);\n        }\n        break;\n\n      case \"OnStreamAdd\":\n        {\n          this.fire('OnStreamAdd', data.stream);\n        }\n        break;\n\n      case \"OnSimulcastSubscribeFallback\":\n        {\n          let streamSwitch = data;\n          this.fire('OnSimulcastSubscribeFallback', streamSwitch);\n        }\n        break;\n\n      case \"OnFirstLocalVideoFrameCaptured\":\n        {\n          let videoFrameInfo = data.info;\n          this.fire('OnFirstLocalVideoFrameCaptured', data.index, videoFrameInfo);\n        }\n        break;\n\n      case \"OnLocalVideoSizeChanged\":\n        {\n          let videoFrameInfo = data.info;\n          this.fire('OnLocalVideoSizeChanged', data.index, videoFrameInfo);\n        }\n        break;\n\n      case \"OnRemoteVideoSizeChanged\":\n        {\n          let videoFrameInfo = data.info;\n          this.fire('OnRemoteVideoSizeChanged', data.key, videoFrameInfo);\n        }\n        break;\n\n      case \"OnFirstRemoteVideoFrameRendered\":\n        {\n          let videoFrameInfo = data.info;\n          this.fire('OnFirstRemoteVideoFrameRendered', data.key, videoFrameInfo);\n        }\n        break;\n\n      case \"OnUserMuteVideo\":\n        {\n          this.fire('OnUserMuteVideo', data.userId, data.muteState);\n        }\n        break;\n\n      case \"OnUserStartVideoCapture\":\n        {\n          this.fire('OnUserStartVideoCapture', data.userId);\n        }\n        break;\n\n      case \"OnUserStopVideoCapture\":\n        {\n          this.fire('OnUserStopVideoCapture', data.userId);\n        }\n        break;\n\n      case \"OnLocalAudioStateChanged\":\n        {\n          let state = data.state;\n          let error = data.error;\n          this.fire('OnLocalAudioStateChanged', state, error);\n        }\n        break;\n\n      case \"OnRemoteAudioStateChanged\":\n        {\n          let state = data.state;\n          let reason = data.reason;\n          this.fire('OnRemoteAudioStateChanged', data.key, state, reason);\n        }\n        break;\n\n      case \"OnLocalVideoStateChanged\":\n        {\n          let state = data.state;\n          let error = data.error;\n          this.fire('OnLocalVideoStateChanged', data.streamIndex, state, error);\n        }\n        break;\n\n      case \"OnRemoteVideoStateChanged\":\n        {\n          let state = data.state;\n          let reason = data.reason;\n          this.fire('OnRemoteVideoStateChanged', data.key, state, reason);\n        }\n        break;\n\n      case \"OnAudioFrameSendStateChanged\":\n        {\n          let user = data.user;\n          let state = data.state;\n          this.fire('OnAudioFrameSendStateChanged', user, state);\n        }\n        break;\n\n      case \"OnVideoFrameSendStateChanged\":\n        {\n          let user = data.user;\n          let state = data.state;\n          this.fire('OnVideoFrameSendStateChanged', user, state);\n        }\n        break;\n\n      case \"OnScreenVideoFrameSendStateChanged\":\n        {\n          let user = data.user;\n          let state = data.state;\n          this.fire('OnScreenVideoFrameSendStateChanged', user, state);\n        }\n        break;\n\n      case \"OnAudioFramePlayStateChanged\":\n        {\n          let user = data.user;\n          let state = data.state;\n          this.fire('OnAudioFramePlayStateChanged', user, state);\n        }\n        break;\n\n      case \"OnVideoFramePlayStateChanged\":\n        {\n          let user = data.user;\n          let state = data.state;\n          this.fire('OnVideoFramePlayStateChanged', user, state);\n        }\n        break;\n\n      case \"OnScreenVideoFramePlayStateChanged\":\n        {\n          let user = data.user;\n          let state = data.state;\n          this.fire('OnScreenVideoFramePlayStateChanged', user, state);\n        }\n        break;\n\n      case \"OnUserEnableLocalAudio\":\n        {\n          this.fire('OnUserEnableLocalAudio', data.userId, data.enabled);\n        }\n        break;\n\n      case \"OnUserEnableLocalVideo\":\n        {\n          this.fire('OnUserEnableLocalVideo', data.userId, data.enabled);\n        }\n        break;\n\n      case \"OnStreamSubscribed\":\n        {\n          this.fire('OnStreamSubscribed', data.stateCode, data.userId, data.config);\n        }\n        break;\n      // message\n\n      case 'OnRoomMessageReceived':\n        {\n          this.fire('OnRoomMessageReceived', data.uid, data.message);\n        }\n        break;\n\n      case 'OnRoomBinaryMessageReceived':\n        {\n          this.fire('OnRoomBinaryMessageReceived', data.uid, data.message);\n        }\n        break;\n\n      case 'OnRoomMessageSendResult':\n        {\n          this.fire('OnRoomMessageSendResult', data.msgId, data.error);\n        }\n        break;\n\n      case 'OnUserMessageReceived':\n        {\n          this.fire('OnUserMessageReceived', data.userId, data.message);\n        }\n        break;\n\n      case 'OnUserBinaryMessageReceived':\n        {\n          this.fire('OnUserBinaryMessageReceived', data.uid, data.message);\n        }\n        break;\n\n      case 'OnUserMessageSendResult':\n        {\n          this.fire('OnUserMessageSendResult', data.msgId, data.error);\n        }\n        break;\n\n      case 'OnAudioVolumeIndication':\n        {\n          this.fire('OnAudioVolumeIndication', data.speakers, data.totalVolume);\n        }\n        break;\n\n      case 'OnSEIMessageReceived':\n        {\n          let stream_key = data.stream_key;\n          this.fire('OnSEIMessageReceived', stream_key, data.message);\n        }\n        break;\n\n      case 'OnStreamMixingEvent':\n        {\n          this.fire('OnStreamMixingEvent', data.event, data.event_data, data.error, data.mix_type);\n        }\n        break;\n\n      case 'OnStreamMixingVideoFrame':\n        {\n          this.fire('OnStreamMixingVideoFrame', data);\n        }\n        break;\n\n      case 'OnRecordingStateUpdate':\n        {\n          let type = data.type;\n          let info = data.info;\n          this.fire('OnRecordingStateUpdate', type, data.state, data.error_code, info);\n        }\n        break;\n\n      case 'OnRecordingProgressUpdate':\n        {\n          let type = data.type;\n          let process = data.process;\n          let info = data.info;\n          this.fire('OnRecordingProgressUpdate', type, process, info);\n        }\n        break;\n\n      case 'OnRecordAudioFrame':\n        {\n          let audioFrame = data.audio_frame;\n          this.fire('OnRecordAudioFrame', audioFrame);\n        }\n        break;\n\n      case 'OnPlaybackAudioFrame':\n        {\n          let audioFrame = data.audio_frame;\n          this.fire('OnPlaybackAudioFrame', audioFrame);\n        }\n        break;\n\n      case 'OnMixedAudioFrame':\n        {\n          let audioFrame = data.audio_frame;\n          this.fire('OnMixedAudioFrame', audioFrame);\n        }\n        break;\n\n      case 'OnLoginResult':\n        {\n          this.fire('OnLoginResult', data.uid, data.error_code, data.elapsed);\n        }\n        break;\n\n      case 'OnLogout':\n        {\n          this.fire('OnLogout');\n        }\n        break;\n\n      case 'OnServerParamsSetResult':\n        {\n          this.fire('OnServerParamsSetResult', data.error);\n        }\n        break;\n\n      case 'OnGetPeerOnlineStatus':\n        {\n          this.fire('OnGetPeerOnlineStatus', data.peer_user_id, data.status);\n        }\n        break;\n\n      case 'OnUserMessageReceivedOutsideRoom':\n        {\n          this.fire('OnUserMessageReceivedOutsideRoom', data.uid, data.message);\n        }\n        break;\n\n      case 'OnUserBinaryMessageReceivedOutsideRoom':\n        {\n          this.fire('OnUserBinaryMessageReceivedOutsideRoom', data.uid, data.message);\n        }\n        break;\n\n      case 'OnUserMessageSendResultOutsideRoom':\n        {\n          this.fire('OnUserMessageSendResultOutsideRoom', data.msgid, data.error);\n        }\n        break;\n\n      case 'OnServerMessageSendResult':\n        {\n          this.fire('OnServerMessageSendResult', data.msgid, data.error);\n        }\n        break;\n\n      case 'OnASRSuccess':\n        {\n          this.fire('OnASRSuccess');\n        }\n        break;\n\n      case 'OnMessage':\n        {\n          this.fire('OnMessage', data.message);\n        }\n        break;\n\n      case 'OnASRError':\n        {\n          this.fire('OnASRError', data.error_code, data.error_message);\n        }\n        break;\n\n      case 'OnLiveTranscodingResult':\n        {\n          this.fire('OnLiveTranscodingResult', data.url, data.error);\n        }\n\n      default:\n        {}\n        break;\n    }\n  }\n  /**\n   * @private\n   */\n\n\n  convertYUVBuffer(obj) {\n    const bufferY = Buffer.from(obj.Object.planeY);\n    const bufferU = Buffer.from(obj.Object.planeU);\n    const bufferV = Buffer.from(obj.Object.planeV);\n    obj.Object.planeY = null;\n    obj.Object.planeU = null;\n    obj.Object.planeV = null;\n    obj.Object.planeY = bufferY;\n    obj.Object.planeU = bufferU;\n    obj.Object.planeV = bufferV;\n  }\n  /**\n   * @private\n   */\n\n\n  findUser(userId) {\n    var _a;\n\n    let ret = undefined;\n\n    if (userId === this.localUserId) {\n      ret = this.localUser;\n    } else {\n      ret = (_a = this.remoteUsers) === null || _a === void 0 ? void 0 : _a.get(userId);\n    }\n\n    return ret;\n  }\n\n}\n\n__decorate([checkInit], veRTCEngine.prototype, \"uninit\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"setBusinessId\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"joinRoom\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"setUserVisibility\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"leaveRoom\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"updateToken\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"setPlaybackVolume\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"startAudioCapture\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"muteLocalAudio\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"setAudioVolumeIndicationInterval\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"setRemoteAudioPlaybackVolume\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"resumeAllSubscribedStream\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"startVideoCapture\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"stopVideoCapture\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"muteLocalVideo\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"switchCamera\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"setLocalVideoMirrorMode\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"pauseAllSubscribedStream\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"startScreenVideoCapture\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"updateScreenCaptureRegion\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"UpdateScreenCaptureMouseCursor\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"updateScreenCaptureHighlightConfig\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"updateScreenCaptureFilterConfig\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"stopScreenVideoCapture\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"getScreenCaptureSourceList\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"getThumbnail\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"publishScreen\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"unpublishScreen\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"setCaptureVolume\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"startAudioMixing\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"stopAudioMixing\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"pauseAudioMixing\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"resumeAudioMixing\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"setAudioMixingVolume\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"getAudioMixingDuration\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"getAudioMixingCurrentPosition\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"setAudioMixingPosition\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"enumerateVideoCaptureDevices\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"setVideoCaptureDevice\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"getVideoCaptureDevice\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"enumerateAudioPlaybackDevices\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"enumerateAudioCaptureDevices\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"setAudioPlaybackDevice\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"setAudioCaptureDevice\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"getAudioPlaybackDevice\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"getAudioCaptureDevice\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"startAudioPlaybackDeviceTest\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"stopAudioPlaybackDeviceTest\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"startAudioCaptureDeviceTest\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"stopAudioCaptureDeviceTest\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"setAudioPlaybackDeviceVolume\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"getAudioPlaybackDeviceVolume\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"setAudioCaptureDeviceVolume\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"getAudioCaptureDeviceVolume\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"startAudioDeviceLoopbackTest\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"stopAudioDeviceLoopbackTest\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"initAudioPlaybackDeviceForTest\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"initAudioCaptureDeviceForTest\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"setAudioPlaybackDeviceMute\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"getAudioPlaybackDeviceMute\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"setAudioCaptureDeviceMute\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"getAudioCaptureDeviceMute\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"setPublishFallbackOption\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"setRemoteUserPriority\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"publish\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"unpublish\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"subscribeUserStream\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"unsubscribe\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"sendUserMessage\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"sendUserBinaryMessage\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"setRecordingAudioFrameParameters\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"setPlaybackAudioFrameParameters\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"setMixedAudioFrameParameters\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"sendRoomMessage\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"sendRoomBinaryMessage\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"login\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"logout\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"updateLoginToken\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"setServerParams\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"getPeerOnlineStatus\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"SendUserMessageOutsideRoom\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"getErrorDescription\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"sendUserBinaryMessageOutsideRoom\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"sendServerMessage\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"sendServerBinaryMessage\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"disableBackground\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"checkLicense\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"getAuthMessage\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"enableEffect\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"setAlgoModelPath\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"setEffectNodes\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"updateNode\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"setColorFilter\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"setColorFilterIntensity\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"sendSEIMessage\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"setScreenAudioStreamIndex\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"startScreenAudioCapture\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"stopScreenAudioCapture\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"feedback\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"setRuntimeParameters\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"setupLocalVideo\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"removeLocalVideo\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"setupRemoteVideo\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"removeRemoteVideo\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"removeAllRemoteVideo\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"setupLocalScreen\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"removeLocalScreen\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"setupRemoteScreen\", null);\n\n__decorate([checkInit], veRTCEngine.prototype, \"removeRemoteScreen\", null);\n\nexports.veRTCEngine = veRTCEngine;\n\n//# sourceURL=webpack:///./node_modules/@byted/vertc-electron-sdk/js/main/index.js?");

/***/ }),

/***/ "./node_modules/@byted/vertc-electron-sdk/js/types.js":
/*!************************************************************!*\
  !*** ./node_modules/@byted/vertc-electron-sdk/js/types.js ***!
  \************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval(" //\n//  types.ts\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n}); //////////////////////////////////////////////////////////////////////////\n// VRTCEngine here\n\nvar RenderMode;\n\n(function (RenderMode) {\n  RenderMode[RenderMode[\"FIT\"] = 1] = \"FIT\";\n  RenderMode[RenderMode[\"HIDDEN\"] = 2] = \"HIDDEN\";\n})(RenderMode = exports.RenderMode || (exports.RenderMode = {}));\n/**\n * @type keytype\n * @brief 用户角色。\n *        房间模式为直播、游戏、云游戏模式时的用户角色。\n */\n\n\nvar ClientRoleType;\n\n(function (ClientRoleType) {\n  /**\n   * @brief 主播角色，用户既可以发布流到房间中，也可以从房间中订阅流。\n   */\n  ClientRoleType[ClientRoleType[\"CLIENT_ROLE_BROADCASTER\"] = 1] = \"CLIENT_ROLE_BROADCASTER\";\n  /**\n   * @brief 观众角色，用户只能从房间中订阅流，不能向房间中发布流，房间中的其他用户可以感知到该用户在房间中。\n   */\n\n  ClientRoleType[ClientRoleType[\"CLIENT_ROLE_AUDIENCE\"] = 2] = \"CLIENT_ROLE_AUDIENCE\";\n  /**\n   * @brief\n   * 静默观众模式，除了满足观众角色的限制外，房间中的其他用户无法感知到该用户在房间中，即该用户加入退出房间和更新用户属性等行为不会通知给房间中的其他用户。\n   */\n\n  ClientRoleType[ClientRoleType[\"CLIENT_ROLE_SILENT_AUDIENCE\"] = 3] = \"CLIENT_ROLE_SILENT_AUDIENCE\";\n})(ClientRoleType = exports.ClientRoleType || (exports.ClientRoleType = {}));\n\n;\n/**\n * @type keytype\n * @brief 房间模式\n *        房间使用的场景模式。\n */\n\nvar ChannelProfileType;\n\n(function (ChannelProfileType) {\n  /**\n   * @brief 通信模式。该房间模式下，房间内所有用户都可以发布和订阅流。适用于 IM 场景。\n   */\n  ChannelProfileType[ChannelProfileType[\"CHANNEL_PROFILE_COMMUNICATION\"] = 0] = \"CHANNEL_PROFILE_COMMUNICATION\";\n  /**\n   * @brief 直播模式。该房间模式下，用户有主播、观众、静默观众三种可选角色，可以通过调用方法 setClientRole{@link 85532#SetClientRole} 设置。主播可以发布和订阅流，观众和静默观众只能订阅流。适用于直播、教育大班课等场景。\n   */\n\n  ChannelProfileType[ChannelProfileType[\"CHANNEL_PROFILE_LIVE_BROADCASTING\"] = 1] = \"CHANNEL_PROFILE_LIVE_BROADCASTING\";\n  /**\n   * @brief 游戏模式。该房间模式下，用户角色同直播模式。适用于游戏场景。\n   */\n\n  ChannelProfileType[ChannelProfileType[\"CHANNEL_PROFILE_GAME\"] = 2] = \"CHANNEL_PROFILE_GAME\";\n  /**\n   * @brief 云游戏模式。该房间模式下，用户角色同直播模式。同时 SDK 会开启 DataChannel （详见方法 sendDataChannnelMessage{@link 85532#SendDataChannnelMessage} ）功能并使用低延时设置。适用于云游戏场景。\n   */\n\n  ChannelProfileType[ChannelProfileType[\"CHANNEL_PROFILE_CLOUD_GAME\"] = 3] = \"CHANNEL_PROFILE_CLOUD_GAME\";\n})(ChannelProfileType = exports.ChannelProfileType || (exports.ChannelProfileType = {}));\n\n;\n/**\n * @hidden\n */\n\nvar SubscribeState;\n\n(function (SubscribeState) {\n  /**\n   * @brief 订阅流成功\n   */\n  SubscribeState[SubscribeState[\"SUB_STATE_SUCCESS\"] = 0] = \"SUB_STATE_SUCCESS\";\n  /**\n   * @hidden\n   */\n\n  SubscribeState[SubscribeState[\"SUB_STATE_FAILED_NOT_IN_ROOM\"] = 1] = \"SUB_STATE_FAILED_NOT_IN_ROOM\";\n  /**\n   * @brief 没有找到流\n   */\n\n  SubscribeState[SubscribeState[\"SUB_STATE_FAILED_STREAM_NOT_FOUND\"] = 2] = \"SUB_STATE_FAILED_STREAM_NOT_FOUND\";\n})(SubscribeState = exports.SubscribeState || (exports.SubscribeState = {}));\n\n;\n;\n;\n;\nvar ScaleMode;\n\n(function (ScaleMode) {\n  /**\n   * @brief 自由模式，默认使用FitWithCropping模式\n   */\n  ScaleMode[ScaleMode[\"Auto\"] = 0] = \"Auto\";\n  /**\n   * @brief 视频尺寸进行缩放和拉伸以充满显示视窗\n   */\n\n  ScaleMode[ScaleMode[\"Stretch\"] = 1] = \"Stretch\";\n  /**\n   * @brief\n   * 优先保证视窗被填满。视频尺寸等比缩放，直至整个视窗被视频填满。如果视频长宽与显示窗口不同，多出的视频将被截掉\n   */\n\n  ScaleMode[ScaleMode[\"FitWithCropping\"] = 2] = \"FitWithCropping\";\n  /**\n   * @brief\n   * 优先保证视频内容全部显示。视频尺寸等比缩放，直至视频窗口的一边与视窗边框对齐。如果视频长宽与显示窗口不同，视窗上未被填满的区域将被涂黑\n   */\n\n  ScaleMode[ScaleMode[\"FitWithFilling\"] = 3] = \"FitWithFilling\";\n})(ScaleMode = exports.ScaleMode || (exports.ScaleMode = {}));\n/**\n * @type keytype\n * @brief 视频的编码类型\n */\n\n\nvar VideoCodecType;\n\n(function (VideoCodecType) {\n  /**\n   * @brief 未知类型\n   */\n  VideoCodecType[VideoCodecType[\"kVideoCodecUnknown\"] = 0] = \"kVideoCodecUnknown\";\n  /**\n   * @brief 标准H264\n   */\n\n  VideoCodecType[VideoCodecType[\"kVideoCodecH264\"] = 1] = \"kVideoCodecH264\";\n  /**\n   * @brief 标准ByteVC1\n   */\n\n  VideoCodecType[VideoCodecType[\"kVideoCodecByteVC1\"] = 2] = \"kVideoCodecByteVC1\";\n})(VideoCodecType = exports.VideoCodecType || (exports.VideoCodecType = {}));\n\n;\n/**\n * @type keytype\n * @brief 视频编码模式\n */\n\nvar CodecMode;\n\n(function (CodecMode) {\n  /**\n   * @brief 自动选择\n   */\n  CodecMode[CodecMode[\"AutoMode\"] = 0] = \"AutoMode\";\n  /**\n   * @brief 硬编码\n   */\n\n  CodecMode[CodecMode[\"HardwareMode\"] = 1] = \"HardwareMode\";\n  /**\n   * @brief 软编码\n   */\n\n  CodecMode[CodecMode[\"SoftwareMode\"] = 2] = \"SoftwareMode\";\n})(CodecMode = exports.CodecMode || (exports.CodecMode = {}));\n\n;\n/**\n * @type keytype\n * @brief 视频编码质量偏好\n *      网络不好时的编码策略\n */\n\nvar EncodePreference;\n\n(function (EncodePreference) {\n  /**\n   * @brief 关闭\n   */\n  EncodePreference[EncodePreference[\"EncodePreferenceDisabled\"] = 0] = \"EncodePreferenceDisabled\";\n  /**\n   * @brief 保持帧率\n   */\n\n  EncodePreference[EncodePreference[\"EncodePreferenceFramerate\"] = 1] = \"EncodePreferenceFramerate\";\n  /**\n   * @brief 保持画质\n   */\n\n  EncodePreference[EncodePreference[\"EncodePreferenceQuality\"] = 2] = \"EncodePreferenceQuality\";\n  /**\n   * @brief 平衡模式\n   */\n\n  EncodePreference[EncodePreference[\"EncodePreferenceBalance\"] = 3] = \"EncodePreferenceBalance\";\n})(EncodePreference = exports.EncodePreference || (exports.EncodePreference = {}));\n\n;\nexports.SEND_KBPS_AUTO_CALCULATE = -1;\nexports.SEND_KBPS_DISABLE_VIDEO_SEND = 0;\n;\n;\n;\n;\n/**\n * @type keytype\n * @region 音频管理\n * @brief 音频帧类型\n */\n\nvar AudioFrameType;\n\n(function (AudioFrameType) {\n  /**\n   * @brief PCM 16bit\n   */\n  AudioFrameType[AudioFrameType[\"kFrameTypePCM16\"] = 0] = \"kFrameTypePCM16\";\n})(AudioFrameType = exports.AudioFrameType || (exports.AudioFrameType = {}));\n\n;\n/**\n* @type keytype\n* @brief 音频声道。\n*/\n\nvar AudioChannel;\n\n(function (AudioChannel) {\n  /**\n   * @brief 自动声道，适用于从 SDK 获取音频数据，使用 SDK 内部处理的声道，不经过 resample。  <br>\n   *        当你需要从 SDK 获取音频数据时，若对声道没有强依赖，建议设置成该值，可以通过避免 resample 带来一些性能优化。\n   */\n  AudioChannel[AudioChannel[\"kAudioChannelAuto\"] = -1] = \"kAudioChannelAuto\";\n  /**\n   * @brief 单声道\n   */\n\n  AudioChannel[AudioChannel[\"kAudioChannelMono\"] = 1] = \"kAudioChannelMono\";\n  /**\n   * @brief 双声道\n   */\n\n  AudioChannel[AudioChannel[\"kAudioChannelStereo\"] = 2] = \"kAudioChannelStereo\";\n})(AudioChannel = exports.AudioChannel || (exports.AudioChannel = {}));\n\n;\n/**\n * @type keytype\n * @brief 音频采样率，单位为 HZ。 <br>\n */\n\nvar AudioSampleRate;\n\n(function (AudioSampleRate) {\n  /**\n   * @brief 自动采样率，适用于从 SDK 获取音频数据，使用 SDK 内部处理的采样率，不经过 resample。  <br>\n   *        当你需要从 SDK 获取音频数据时，若对采样率没有强依赖，建议设置成该值，可以通过避免 resample 带来一些性能优化。\n   */\n  AudioSampleRate[AudioSampleRate[\"kAudioSampleRateAuto\"] = -1] = \"kAudioSampleRateAuto\";\n  /**\n   * @brief 8000 采样率\n   */\n\n  AudioSampleRate[AudioSampleRate[\"kAudioSampleRate8000\"] = 8000] = \"kAudioSampleRate8000\";\n  /**\n   * @brief 16000 采样率\n   */\n\n  AudioSampleRate[AudioSampleRate[\"kAudioSampleRate16000\"] = 16000] = \"kAudioSampleRate16000\";\n  /**\n   * @brief 32000 采样率\n   */\n\n  AudioSampleRate[AudioSampleRate[\"kAudioSampleRate32000\"] = 32000] = \"kAudioSampleRate32000\";\n  /**\n   * @brief 44100 采样率\n   */\n\n  AudioSampleRate[AudioSampleRate[\"kAudioSampleRate44100\"] = 44100] = \"kAudioSampleRate44100\";\n  /**\n   * @brief 48000 采样率\n   */\n\n  AudioSampleRate[AudioSampleRate[\"kAudioSampleRate48000\"] = 48000] = \"kAudioSampleRate48000\";\n})(AudioSampleRate = exports.AudioSampleRate || (exports.AudioSampleRate = {}));\n\n;\n/**\n * @type keytype\n * @brief SDK 与信令服务器连接状态。\n */\n\nvar ConnectionState;\n\n(function (ConnectionState) {\n  /**\n   * @brief 连接断开。\n   */\n  ConnectionState[ConnectionState[\"kConnectionStateDisconnected\"] = 1] = \"kConnectionStateDisconnected\";\n  /**\n   * @brief 首次连接，正在连接中。\n   */\n\n  ConnectionState[ConnectionState[\"kConnectionStateConnecting\"] = 2] = \"kConnectionStateConnecting\";\n  /**\n   * @brief 首次连接成功。\n   */\n\n  ConnectionState[ConnectionState[\"kConnectionStateConnected\"] = 3] = \"kConnectionStateConnected\";\n  /**\n   * @brief 连接断开后重新连接中。\n   */\n\n  ConnectionState[ConnectionState[\"kConnectionStateReconnecting\"] = 4] = \"kConnectionStateReconnecting\";\n  /**\n   * @brief 连接断开后重连成功。\n   */\n\n  ConnectionState[ConnectionState[\"kConnectionStateReconnected\"] = 5] = \"kConnectionStateReconnected\";\n  /**\n   * @brief 网络连接断开超过 10 秒，仍然会继续重连。\n   */\n\n  ConnectionState[ConnectionState[\"kConnectionStateLost\"] = 6] = \"kConnectionStateLost\";\n})(ConnectionState = exports.ConnectionState || (exports.ConnectionState = {}));\n\n;\n/**\n * @type keytype\n * @brief 用户离线原因。<br>\n *        房间内的远端用户离开房间时，本端用户会收到 OnUserOffline{@link #OnUserOffline} 回调通知，此枚举类型为回调的用户离线原因。\n */\n\nvar UserOfflineReasonType;\n\n(function (UserOfflineReasonType) {\n  /**\n   * @brief 用户主动离开。\n   */\n  UserOfflineReasonType[UserOfflineReasonType[\"USER_OFFLINE_QUIT\"] = 0] = \"USER_OFFLINE_QUIT\";\n  /**\n   * @brief 用户掉线。\n   */\n\n  UserOfflineReasonType[UserOfflineReasonType[\"USER_OFFLINE_DROPPED\"] = 1] = \"USER_OFFLINE_DROPPED\";\n  /**\n   * @hidden\n   */\n\n  UserOfflineReasonType[UserOfflineReasonType[\"USER_OFFLINE_BECOME_AUDIENCE\"] = 2] = \"USER_OFFLINE_BECOME_AUDIENCE\";\n})(UserOfflineReasonType = exports.UserOfflineReasonType || (exports.UserOfflineReasonType = {}));\n\n;\n/**\n * @type keytype\n * @brief 当前媒体设备类型\n */\n\nvar MediaDeviceType;\n\n(function (MediaDeviceType) {\n  /**\n   * @brief 音频渲染设备类型\n   */\n  MediaDeviceType[MediaDeviceType[\"kMediaDeviceTypeAudioRenderDevice\"] = 0] = \"kMediaDeviceTypeAudioRenderDevice\";\n  /**\n   * @brief 音频采集设备类型\n   */\n\n  MediaDeviceType[MediaDeviceType[\"kMediaDeviceTypeAudioCaptureDevice\"] = 1] = \"kMediaDeviceTypeAudioCaptureDevice\";\n  /**\n   *@hidden\n   *@brief 视频渲染设备类型，该类型暂无使用\n   */\n\n  MediaDeviceType[MediaDeviceType[\"kMediaDeviceTypeVideoRenderDevice\"] = 2] = \"kMediaDeviceTypeVideoRenderDevice\";\n  /**\n   *@brief 视频采集设备类型\n   */\n\n  MediaDeviceType[MediaDeviceType[\"kMediaDeviceTypeVideoCaptureDevice\"] = 3] = \"kMediaDeviceTypeVideoCaptureDevice\";\n})(MediaDeviceType = exports.MediaDeviceType || (exports.MediaDeviceType = {}));\n\n;\n/**\n * @type keytype\n * @brief 媒体设备事件类型\n */\n\nvar MediaDeviceNotification;\n\n(function (MediaDeviceNotification) {\n  /**\n   *@brief 设备已就绪\n   */\n  MediaDeviceNotification[MediaDeviceNotification[\"kMediaDeviceNotificationActive\"] = 1] = \"kMediaDeviceNotificationActive\";\n  /**\n   *@brief 设备被禁用\n   */\n\n  MediaDeviceNotification[MediaDeviceNotification[\"kMediaDeviceNotificationDisabled\"] = 2] = \"kMediaDeviceNotificationDisabled\";\n  /**\n   *@brief 没有此设备\n   */\n\n  MediaDeviceNotification[MediaDeviceNotification[\"kMediaDeviceNotificationNotPresent\"] = 4] = \"kMediaDeviceNotificationNotPresent\";\n  /**\n   *@brief 设备被拔出\n   */\n\n  MediaDeviceNotification[MediaDeviceNotification[\"kMediaDeviceNotificationUnplugged\"] = 8] = \"kMediaDeviceNotificationUnplugged\";\n})(MediaDeviceNotification = exports.MediaDeviceNotification || (exports.MediaDeviceNotification = {}));\n\n;\n/**\n * @type keytype\n * @brief 媒体设备状态类型\n */\n\nvar MediaDeviceState;\n\n(function (MediaDeviceState) {\n  /**\n   *@brief 设备已开启\n   */\n  MediaDeviceState[MediaDeviceState[\"kMediaDeviceStateStarted\"] = 1] = \"kMediaDeviceStateStarted\";\n  /**\n   *@brief 设备已停止\n   */\n\n  MediaDeviceState[MediaDeviceState[\"kMediaDeviceStateStopped\"] = 2] = \"kMediaDeviceStateStopped\";\n  /**\n   *@brief 设备运行时错误\n   */\n\n  MediaDeviceState[MediaDeviceState[\"kMediaDeviceStateRuntimeError\"] = 3] = \"kMediaDeviceStateRuntimeError\";\n  /**\n   *@brief 设备已插入\n   */\n\n  MediaDeviceState[MediaDeviceState[\"kMediaDeviceStateAdded\"] = 10] = \"kMediaDeviceStateAdded\";\n  /**\n   *@brief 设备被移除\n   */\n\n  MediaDeviceState[MediaDeviceState[\"kMediaDeviceStateRemoved\"] = 11] = \"kMediaDeviceStateRemoved\";\n})(MediaDeviceState = exports.MediaDeviceState || (exports.MediaDeviceState = {}));\n\n;\n/**\n * @type keytype\n * @brief 屏幕采集对象的类型\n */\n\nvar ScreenCaptureSourceType;\n\n(function (ScreenCaptureSourceType) {\n  /**\n   * @brief 类型未知\n   */\n  ScreenCaptureSourceType[ScreenCaptureSourceType[\"kScreenCaptureSourceTypeUnknown\"] = 0] = \"kScreenCaptureSourceTypeUnknown\";\n  /**\n   * @brief 应用程序的窗口\n   */\n\n  ScreenCaptureSourceType[ScreenCaptureSourceType[\"kScreenCaptureSourceTypeWindow\"] = 1] = \"kScreenCaptureSourceTypeWindow\";\n  /**\n   * @brief 桌面\n   */\n\n  ScreenCaptureSourceType[ScreenCaptureSourceType[\"kScreenCaptureSourceTypeScreen\"] = 2] = \"kScreenCaptureSourceTypeScreen\";\n})(ScreenCaptureSourceType = exports.ScreenCaptureSourceType || (exports.ScreenCaptureSourceType = {}));\n\n;\nvar RemoteUserPriority;\n\n(function (RemoteUserPriority) {\n  /**\n     * @brief 用户优先级为低（默认值）\n     */\n  RemoteUserPriority[RemoteUserPriority[\"kRemoteUserPriorityLow\"] = 0] = \"kRemoteUserPriorityLow\";\n  /**\n   * @brief 用户优先级为正常\n   */\n\n  RemoteUserPriority[RemoteUserPriority[\"kRemoteUserPriorityMedium\"] = 100] = \"kRemoteUserPriorityMedium\";\n  /**\n   * @brief 用户优先级为高\n   */\n\n  RemoteUserPriority[RemoteUserPriority[\"kRemoteUserPriorityHigh\"] = 200] = \"kRemoteUserPriorityHigh\";\n})(RemoteUserPriority = exports.RemoteUserPriority || (exports.RemoteUserPriority = {}));\n\n;\nvar TranscoderContentControlType;\n\n(function (TranscoderContentControlType) {\n  TranscoderContentControlType[TranscoderContentControlType[\"kHasAudioAndVideo\"] = 0] = \"kHasAudioAndVideo\";\n  TranscoderContentControlType[TranscoderContentControlType[\"kHasAudioOnly\"] = 1] = \"kHasAudioOnly\";\n  TranscoderContentControlType[TranscoderContentControlType[\"kHasVideoOnly\"] = 2] = \"kHasVideoOnly\";\n})(TranscoderContentControlType = exports.TranscoderContentControlType || (exports.TranscoderContentControlType = {}));\n\n;\nvar TranscoderVideoCodecProfile;\n\n(function (TranscoderVideoCodecProfile) {\n  TranscoderVideoCodecProfile[TranscoderVideoCodecProfile[\"kByteH264ProfileBaseline\"] = 0] = \"kByteH264ProfileBaseline\";\n  TranscoderVideoCodecProfile[TranscoderVideoCodecProfile[\"kByteH264ProfileMain\"] = 1] = \"kByteH264ProfileMain\";\n  TranscoderVideoCodecProfile[TranscoderVideoCodecProfile[\"kByteH264ProfileHigh\"] = 2] = \"kByteH264ProfileHigh\";\n  TranscoderVideoCodecProfile[TranscoderVideoCodecProfile[\"kByteVC1ProfileBaseline\"] = 3] = \"kByteVC1ProfileBaseline\";\n  TranscoderVideoCodecProfile[TranscoderVideoCodecProfile[\"kByteVC1ProfileMain\"] = 4] = \"kByteVC1ProfileMain\";\n  TranscoderVideoCodecProfile[TranscoderVideoCodecProfile[\"kByteVC1ProfileHigh\"] = 5] = \"kByteVC1ProfileHigh\";\n})(TranscoderVideoCodecProfile = exports.TranscoderVideoCodecProfile || (exports.TranscoderVideoCodecProfile = {}));\n/**\n *@brief 合流类型\n */\n\n\nvar TranscoderAudioCodecProfile;\n\n(function (TranscoderAudioCodecProfile) {\n  TranscoderAudioCodecProfile[TranscoderAudioCodecProfile[\"kByteAACProfileLC\"] = 0] = \"kByteAACProfileLC\";\n  TranscoderAudioCodecProfile[TranscoderAudioCodecProfile[\"kByteAACProfileMain\"] = 1] = \"kByteAACProfileMain\";\n  TranscoderAudioCodecProfile[TranscoderAudioCodecProfile[\"kByteAACProfileHEv1\"] = 2] = \"kByteAACProfileHEv1\";\n  TranscoderAudioCodecProfile[TranscoderAudioCodecProfile[\"kByteAACProfileHEv2\"] = 3] = \"kByteAACProfileHEv2\";\n})(TranscoderAudioCodecProfile = exports.TranscoderAudioCodecProfile || (exports.TranscoderAudioCodecProfile = {}));\n\n;\nvar TranscoderRenderMode;\n\n(function (TranscoderRenderMode) {\n  TranscoderRenderMode[TranscoderRenderMode[\"kRenderUnknown\"] = 0] = \"kRenderUnknown\";\n  /**\n   *  @brief 视频尺寸等比缩放，优先保证窗口被填满。当视频尺寸与显示窗口尺寸不一致时，多出的视频将被截掉。\n   */\n\n  TranscoderRenderMode[TranscoderRenderMode[\"kRenderHidden\"] = 1] = \"kRenderHidden\";\n  /**\n   *  @brief\n   * 视频尺寸等比缩放，优先保证视频内容全部显示。当视频尺寸与显示窗口尺寸不一致时，会把窗口未被填满的区域填充成黑色。\n   */\n\n  TranscoderRenderMode[TranscoderRenderMode[\"kRenderFit\"] = 2] = \"kRenderFit\";\n  /**\n   *  @brief 视频尺寸非等比例缩放，把窗口充满。当视频尺寸与显示窗口尺寸不一致时，视频高或宽方向会被拉伸。\n   */\n\n  TranscoderRenderMode[TranscoderRenderMode[\"kRenderAdaptive\"] = 3] = \"kRenderAdaptive\";\n})(TranscoderRenderMode = exports.TranscoderRenderMode || (exports.TranscoderRenderMode = {}));\n\n;\n/**\n * @type keytype\n * @brief 媒体流网络质量。\n */\n\nvar NetworkQuality;\n\n(function (NetworkQuality) {\n  /**\n   * @brief 网络质量未知。\n   */\n  NetworkQuality[NetworkQuality[\"kNetworkQualityUnknown\"] = 0] = \"kNetworkQualityUnknown\";\n  /**\n   * @brief 网络质量极好。\n   */\n\n  NetworkQuality[NetworkQuality[\"kNetworkQualityExcellent\"] = 1] = \"kNetworkQualityExcellent\";\n  /**\n   * @brief 主观感觉和 kNetworkQualityExcellent 差不多，但码率可能略低。\n   */\n\n  NetworkQuality[NetworkQuality[\"kNetworkQualityGood\"] = 2] = \"kNetworkQualityGood\";\n  /**\n   * @brief 主观感受有瑕疵但不影响沟通。\n   */\n\n  NetworkQuality[NetworkQuality[\"kNetworkQualityPoor\"] = 3] = \"kNetworkQualityPoor\";\n  /**\n   * @brief 勉强能沟通但不顺畅。\n   */\n\n  NetworkQuality[NetworkQuality[\"kNetworkQualityBad\"] = 4] = \"kNetworkQualityBad\";\n  /**\n   * @brief 网络质量非常差，基本不能沟通。\n   */\n\n  NetworkQuality[NetworkQuality[\"kNetworkQualityVbad\"] = 5] = \"kNetworkQualityVbad\";\n})(NetworkQuality = exports.NetworkQuality || (exports.NetworkQuality = {}));\n\n;\n/**\n * @type keytype\n * @brief 发送用户消息或者房间消息的结果\n */\n\nvar MessageSendResultCode;\n\n(function (MessageSendResultCode) {\n  /**\n   * @brief 用户P2P消息发送成功\n   */\n  MessageSendResultCode[MessageSendResultCode[\"MESSAGE_CODE_SUCCESS\"] = 0] = \"MESSAGE_CODE_SUCCESS\";\n  /**\n   * @brief 房间Broadcast消息发送成功\n   */\n\n  MessageSendResultCode[MessageSendResultCode[\"MESSAGE_CODE_ROOM_SUCCESS\"] = 200] = \"MESSAGE_CODE_ROOM_SUCCESS\";\n  /**\n   * @brief 发送超时，没有发送\n   */\n\n  MessageSendResultCode[MessageSendResultCode[\"MESSAGE_CODE_ERROR_TIMEOUT\"] = 1] = \"MESSAGE_CODE_ERROR_TIMEOUT\";\n  /**\n   * @brief 通道断开，没有发送\n   */\n\n  MessageSendResultCode[MessageSendResultCode[\"MESSAGE_CODE_ERROR_BROKEN\"] = 2] = \"MESSAGE_CODE_ERROR_BROKEN\";\n  /**\n   * @brief 找不到接收方\n   */\n\n  MessageSendResultCode[MessageSendResultCode[\"MESSAGE_CODE_ERROR_NOT_RECEIVER\"] = 3] = \"MESSAGE_CODE_ERROR_NOT_RECEIVER\";\n  /**\n   * @brief 没有加入房间\n   */\n\n  MessageSendResultCode[MessageSendResultCode[\"MESSAGE_CODE_ERROR_NOT_JOIN\"] = 100] = \"MESSAGE_CODE_ERROR_NOT_JOIN\";\n  /**\n   * @brief 没有可用的数据传输通道连接\n   */\n\n  MessageSendResultCode[MessageSendResultCode[\"MESSAGE_CODE_ERROR_NO_CONNECTION\"] = 102] = \"MESSAGE_CODE_ERROR_NO_CONNECTION\";\n  /**\n   * @brief 消息超过最大长度，当前为64KB\n   */\n\n  MessageSendResultCode[MessageSendResultCode[\"MESSAGE_CODE_ERROR_EXCEED_MAX_LENGTH\"] = 103] = \"MESSAGE_CODE_ERROR_EXCEED_MAX_LENGTH\";\n  /**\n   * @brief 用户id为空\n   */\n\n  MessageSendResultCode[MessageSendResultCode[\"MESSAGE_CODE_ERROR_EMPTY_USER\"] = 104] = \"MESSAGE_CODE_ERROR_EMPTY_USER\";\n  /**\n   * @brief 未知错误\n   */\n\n  MessageSendResultCode[MessageSendResultCode[\"MESSAGE_CODE_ERROR_UNKNOWN\"] = 1000] = \"MESSAGE_CODE_ERROR_UNKNOWN\";\n})(MessageSendResultCode = exports.MessageSendResultCode || (exports.MessageSendResultCode = {}));\n\n;\n/**\n * @type keytype\n * @brief 音频混音文件播放状态。\n */\n\nvar AudioMixingState;\n\n(function (AudioMixingState) {\n  /**\n   * @brief 混音已加载\n   */\n  AudioMixingState[AudioMixingState[\"kAudioMixingStatePreloaded\"] = 0] = \"kAudioMixingStatePreloaded\";\n  /**\n   * @brief 混音正在播放\n   */\n\n  AudioMixingState[AudioMixingState[\"kAudioMixingStatePlaying\"] = 1] = \"kAudioMixingStatePlaying\";\n  /**\n   * @brief 混音暂停\n   */\n\n  AudioMixingState[AudioMixingState[\"kAudioMixingStatePaused\"] = 2] = \"kAudioMixingStatePaused\";\n  /**\n   * @brief 混音停止\n   */\n\n  AudioMixingState[AudioMixingState[\"kAudioMixingStateStopped\"] = 3] = \"kAudioMixingStateStopped\";\n  /**\n   * @brief 混音播放失败\n   */\n\n  AudioMixingState[AudioMixingState[\"kAudioMixingStateFailed\"] = 4] = \"kAudioMixingStateFailed\";\n  /**\n   * @brief 混音播放结束\n   */\n\n  AudioMixingState[AudioMixingState[\"kAudioMixingStateFinished\"] = 5] = \"kAudioMixingStateFinished\";\n})(AudioMixingState = exports.AudioMixingState || (exports.AudioMixingState = {}));\n\n;\n/**\n * @type keytype\n * @brief 音频混音文件播放错误码。\n */\n\nvar AudioMixingError;\n\n(function (AudioMixingError) {\n  /**\n   * @brief 混音错误码，正常\n   */\n  AudioMixingError[AudioMixingError[\"kAudioMixingErrorOk\"] = 0] = \"kAudioMixingErrorOk\";\n  /**\n   * @brief 预加载失败，找不到混音文件或者文件长度超出 20s\n   */\n\n  AudioMixingError[AudioMixingError[\"kAudioMixingErrorPreloadFailed\"] = 1] = \"kAudioMixingErrorPreloadFailed\";\n  /**\n   * @brief 混音开启失败，找不到混音文件或者混音文件打开失败\n   */\n\n  AudioMixingError[AudioMixingError[\"kAudioMixingErrorStartFailed\"] = 2] = \"kAudioMixingErrorStartFailed\";\n  /**\n   * @brief 混音 ID 异常\n   */\n\n  AudioMixingError[AudioMixingError[\"kAudioMixingErrorIdNotFound\"] = 3] = \"kAudioMixingErrorIdNotFound\";\n  /**\n   * @brief \b设置混音文件的播放位置出错\n   */\n\n  AudioMixingError[AudioMixingError[\"kAudioMixingErrorSetPositionFailed\"] = 4] = \"kAudioMixingErrorSetPositionFailed\";\n  /**\n   * @brief 音量参数不合法，仅支持设置的音量值为[0 400]\n   */\n\n  AudioMixingError[AudioMixingError[\"kAudioMixingErrorInValidVolume\"] = 5] = \"kAudioMixingErrorInValidVolume\";\n  /**\n   * @brief 播放的文件与预加载的文件不一致，请先使用 UnloadAudioMixing{@link #UnloadAudioMixing} 卸载文件\n   */\n\n  AudioMixingError[AudioMixingError[\"kAudioMixingErrorLoadConflict\"] = 6] = \"kAudioMixingErrorLoadConflict\";\n  /**\n   * @hidden\n   * @deprecated\n   * @brief 混音错误码，失败，已废弃\n   */\n\n  AudioMixingError[AudioMixingError[\"kAudioMixingErrorCanNotOpen\"] = 701] = \"kAudioMixingErrorCanNotOpen\";\n})(AudioMixingError = exports.AudioMixingError || (exports.AudioMixingError = {}));\n\n;\n/** {zh}\n * @detail 85534\n * @brief 回调警告码。警告码说明 SDK 内部遇到问题正在尝试恢复。警告码仅起通知作用。\n */\n\nvar WarningCode;\n\n(function (WarningCode) {\n  /**\n     * @hidden\n     */\n  WarningCode[WarningCode[\"kWarningCodeGetRoomFailed\"] = -2000] = \"kWarningCodeGetRoomFailed\";\n  /**\n  * @brief 进房失败。  <br>\n  *        当你调用初次加入房间或者由于网络状况不佳断网重连时，由于服务器错误导致进房失败。SDK 会自动重试进房。\n  */\n\n  WarningCode[WarningCode[\"kWarningCodeJoinRoomFailed\"] = -2001] = \"kWarningCodeJoinRoomFailed\";\n  /**\n  * @brief 发布音视频流失败。  <br>\n  *        当你在所在房间中发布音视频流时，由于服务器错误导致发布失败。SDK 会自动重试发布。\n  */\n\n  WarningCode[WarningCode[\"kWarningCodePublishStreamFailed\"] = -2002] = \"kWarningCodePublishStreamFailed\";\n  /**\n  * @brief 订阅音视频流失败。  <br>\n  *        当前房间中找不到订阅的音视频流导致订阅失败。SDK 会自动重试订阅，若仍订阅失败则建议你退出重试。\n  */\n\n  WarningCode[WarningCode[\"kWarningCodeSubscribeStreamFailed404\"] = -2003] = \"kWarningCodeSubscribeStreamFailed404\";\n  /**\n  * @brief 订阅音视频流失败。  <br>\n  *        当你订阅所在房间中的音视频流时，由于服务器错误导致订阅失败。SDK 会自动重试订阅。\n  */\n\n  WarningCode[WarningCode[\"kWarningCodeSubscribeStreamFailed5xx\"] = -2004] = \"kWarningCodeSubscribeStreamFailed5xx\";\n  /**\n  * @hidden\n  * @brief 函数调用顺序错误。\n  */\n\n  WarningCode[WarningCode[\"kWarningCodeInvokeError\"] = -2005] = \"kWarningCodeInvokeError\";\n  /**\n  * @hidden\n  * @brief 调度异常，服务器返回的媒体服务器地址不可用。\n  */\n\n  WarningCode[WarningCode[\"kWarningCodeInvalidExpectMediaServerAddress\"] = -2007] = \"kWarningCodeInvalidExpectMediaServerAddress\";\n  /**\n   * @brief 当调用 SetUserVisibility{@link 85532#SetUserVisibility} 将自身可见性设置为 false 后，再尝试发布流会触发此警告。\n   */\n\n  WarningCode[WarningCode[\"kWarningCodePublishStreamForbiden\"] = -2009] = \"kWarningCodePublishStreamForbiden\";\n  /**\n   * @hidden\n   * @brief 自动订阅模式未关闭时，尝试开启手动订阅模式会触发此警告。  <br>你需在进房前调用 EnableAutoSubscribe{@link 85532#EnableAutoSubscribe} 方法关闭自动订阅模式，再调用 SubscribeStream{@link 85532#SubscribeStream} 方法手动订阅音视频流。\n   */\n\n  WarningCode[WarningCode[\"kWarningCodeSubscribeStreamForbiden\"] = -2010] = \"kWarningCodeSubscribeStreamForbiden\";\n  /**\n  * @brief 发送自定义广播消息失败，当前你未在房间中。\n  */\n\n  WarningCode[WarningCode[\"kWarningCodeSendCustomMessage\"] = -2011] = \"kWarningCodeSendCustomMessage\";\n  /**\n  * @brief 当房间内人数超过 500 人时，停止向房间内已有用户发送 OnUserJoined{@link #IRTCRoomEventHandler#OnUserJoined} 和 OnUserLeave{@link #IRTCRoomEventHandler#OnUserLeave} 回调，并通过广播提示房间内所有用户。\n  */\n\n  WarningCode[WarningCode[\"kWarningCodeUserNotifyStop\"] = -2013] = \"kWarningCodeUserNotifyStop\";\n  /**\n  * @brief 摄像头权限异常，当前应用没有获取摄像头权限。\n  */\n\n  WarningCode[WarningCode[\"kWarningCodeNoCameraPermission\"] = -5001] = \"kWarningCodeNoCameraPermission\";\n  /**\n  * @brief 麦克风权限异常，当前应用没有获取麦克风权限。\n  */\n\n  WarningCode[WarningCode[\"kWarningCodeNoMicrophonePermission\"] = -5002] = \"kWarningCodeNoMicrophonePermission\";\n  /**\n    * @brief 音频采集设备启动失败，当前设备可能被其他应用占用。\n    */\n\n  WarningCode[WarningCode[\"kWarningCodeRecodingDeviceStartFailed\"] = -5003] = \"kWarningCodeRecodingDeviceStartFailed\";\n  /**\n  * @brief 音频播放设备启动失败警告，可能由于系统资源不足，或参数错误。\n  */\n\n  WarningCode[WarningCode[\"kWarningCodePlayoutDeviceStartFailed\"] = -5004] = \"kWarningCodePlayoutDeviceStartFailed\";\n  /**\n  * @brief 无可用音频采集设备，请插入可用的音频采集设备。\n  */\n\n  WarningCode[WarningCode[\"kWarningCodeNoRecordingDevice\"] = -5005] = \"kWarningCodeNoRecordingDevice\";\n  /**\n  * @brief 无可用音频播放设备，请插入可用的音频播放设备。\n  */\n\n  WarningCode[WarningCode[\"kWarningCodeNoPlayoutDevice\"] = -5006] = \"kWarningCodeNoPlayoutDevice\";\n  /**\n  * @brief 当前音频设备没有采集到有效的声音数据，请检查更换音频采集设备。\n  */\n\n  WarningCode[WarningCode[\"kWarningCodeRecordingSilence\"] = -5007] = \"kWarningCodeRecordingSilence\";\n  /**\n  * @brief 媒体设备误操作警告。  <br>\n  *        使用自定义采集时，不可调用内部采集开关，调用时将触发此警告。\n  */\n\n  WarningCode[WarningCode[\"kWarningCodeMediaDeviceOperationDenied\"] = -5008] = \"kWarningCodeMediaDeviceOperationDenied\";\n  /**\n   * @hidden\n   * @brief 不支持在 publishScreen{@link 85532#PublishScreen} 之后设置屏幕音频采集类型\n   *        setScreenAudioSourceType{@link 85532#SetScreenAudioSourceType}，请在 PublishScreen 之前设置\n   */\n\n  WarningCode[WarningCode[\"kWarningCodeSetScreenAudioSourceTypeFailed\"] = -5009] = \"kWarningCodeSetScreenAudioSourceTypeFailed\";\n  /**\n   * @brief 不支持在 publishScreen{@link 85532#PublishScreen} 之后，\n   *        通过 setScreenAudioStreamIndex{@link 85532#SetScreenAudioStreamIndex} 设置屏幕共享时的音频采集方式。\n   */\n\n  WarningCode[WarningCode[\"kWarningCodeSetScreenAudioStreamIndexFailed\"] = -5010] = \"kWarningCodeSetScreenAudioStreamIndexFailed\";\n  /**\n  * @brief 指定的内部渲染画布句柄无效。  <br> 当你调用 setLocalVideoCanvas{@link 85532#SetLocalVideoCanvas} 或 SetRemoteVideoCanvas{@link 85532#SetRemoteVideoCanvas} 时指定了无效的画布句柄，触发此回调。\n  */\n\n  WarningCode[WarningCode[\"kWarningCodeInvalidCanvasHandle\"] = -6001] = \"kWarningCodeInvalidCanvasHandle\";\n})(WarningCode = exports.WarningCode || (exports.WarningCode = {}));\n\n;\n;\n/**\n * @detail 85534\n * @brief 回调错误码。  <br>SDK 内部遇到不可恢复的错误时，会通过 OnError{@link 85533#OnError} 回调通知用户。\n */\n\nvar ErrorCode;\n\n(function (ErrorCode) {\n  /**\n   * @brief Token 无效。<br>调用 joinRoom{@link 85532#JoinRoom} 方法时使用的 Token 无效或过期失效。需要用户重新获取 Token，并调用 UpdateToken{@link #UpdateToken} 方法更新 Token。\n   */\n  ErrorCode[ErrorCode[\"kErrorCodeInvalidToken\"] = -1000] = \"kErrorCodeInvalidToken\";\n  /**\n   * @brief 加入房间错误。<br>调用 joinRoom{@link 85532#JoinRoom} 方法时发生未知错误导致加入房间失败。需要用户重新加入房间。\n   */\n\n  ErrorCode[ErrorCode[\"kErrorCodeJoinRoom\"] = -1001] = \"kErrorCodeJoinRoom\";\n  /**\n   * @brief 没有发布音视频流权限。<br>用户在所在房间中发布音视频流失败，失败原因为用户没有发布流的权限。\n   */\n\n  ErrorCode[ErrorCode[\"kErrorCodeNoPublishPermission\"] = -1002] = \"kErrorCodeNoPublishPermission\";\n  /**\n   * @brief 没有订阅音视频流权限。<br>用户订阅所在房间中的音视频流失败，失败原因为用户没有订阅流的权限。\n   */\n\n  ErrorCode[ErrorCode[\"kErrorCodeNoSubscribePermission\"] = -1003] = \"kErrorCodeNoSubscribePermission\";\n  /**\n   * @brief 用户重复登录。<br>本地用户所在房间中有相同用户 ID 的用户加入房间，导致本地用户被踢出房间。\n   */\n\n  ErrorCode[ErrorCode[\"kErrorCodeDuplicateLogin\"] = -1004] = \"kErrorCodeDuplicateLogin\";\n  /**\n   * @brief 用户被踢出房间。<br> 本端用户被主动踢出所在房间时，回调此错误。\n   */\n\n  ErrorCode[ErrorCode[\"kBrerrKickedOut\"] = -1006] = \"kBrerrKickedOut\";\n  /**\n   * @brief 订阅音视频流失败，订阅音视频流总数超过上限。<br>游戏场景下，为了保证音视频通话的性能和质量，服务器会限制用户订阅的音视频流总数。当用户订阅的音视频流总数已达上限时，继续订阅更多流时会失败，同时用户会收到此错误通知。\n   */\n\n  ErrorCode[ErrorCode[\"kErrorCodeOverStreamSubscribeLimit\"] = -1070] = \"kErrorCodeOverStreamSubscribeLimit\";\n  /**\n   * @brief 发布流失败，发布流总数超过上限。<br> RTC 系统会限制单个房间内发布的总流数，总流数包括视频流、音频流和屏幕流。如果房间内发布流数已达上限时，本地用户再向房间中发布流时会失败，同时会收到此错误通知。\n   */\n\n  ErrorCode[ErrorCode[\"kErrorCodeOverStreamPublishLimit\"] = -1080] = \"kErrorCodeOverStreamPublishLimit\";\n  /**\n   * @brief 发布屏幕流失败，发布流总数超过上限。<br>RTC 系统会限制单个房间内发布的总流数，总流数包括视频流、音频流和屏幕流。如果房间内发布流数已达上限时，本地用户再向房间中发布流时会失败，同时会收到此错误通知。\n   */\n\n  ErrorCode[ErrorCode[\"kErrorCodeOverScreenPublishLimit\"] = -1081] = \"kErrorCodeOverScreenPublishLimit\";\n  /**\n   * @brief 发布视频流总数超过上限。<br>RTC 系统会限制单个房间内发布的视频流数。如果房间内发布视频流数已达上限时，本地用户再向房间中发布视频流时会失败，同时会收到此错误通知。\n   */\n\n  ErrorCode[ErrorCode[\"kErrorCodeOverVideoPublishLimit\"] = -1082] = \"kErrorCodeOverVideoPublishLimit\";\n})(ErrorCode = exports.ErrorCode || (exports.ErrorCode = {}));\n\n;\n/**\n * @type keytype\n * @brief SDK 网络连接类型。\n */\n\nvar NetworkType;\n\n(function (NetworkType) {\n  /**\n   * @brief 网络连接类型未知。\n   */\n  NetworkType[NetworkType[\"kNetworkTypeUnknown\"] = -1] = \"kNetworkTypeUnknown\";\n  /**\n   * @brief 网络连接已断开。\n   */\n\n  NetworkType[NetworkType[\"kNetworkTypeDisconnected\"] = 0] = \"kNetworkTypeDisconnected\";\n  /**\n   * @brief 网络连接类型为 LAN 。\n   */\n\n  NetworkType[NetworkType[\"kNetworkTypeLAN\"] = 1] = \"kNetworkTypeLAN\";\n  /**\n   * @brief 网络连接类型为 Wi-Fi（包含热点）。\n   */\n\n  NetworkType[NetworkType[\"kNetworkTypeWIFI\"] = 2] = \"kNetworkTypeWIFI\";\n  /**\n   * @brief 网络连接类型为 2G 移动网络。\n   */\n\n  NetworkType[NetworkType[\"kNetworkTypeMobile2G\"] = 3] = \"kNetworkTypeMobile2G\";\n  /**\n   * @brief 网络连接类型为 3G 移动网络。\n   */\n\n  NetworkType[NetworkType[\"kNetworkTypeMobile3G\"] = 4] = \"kNetworkTypeMobile3G\";\n  /**\n   * @brief 网络连接类型为 4G 移动网络。\n   */\n\n  NetworkType[NetworkType[\"kNetworkTypeMobile4G\"] = 5] = \"kNetworkTypeMobile4G\";\n  /**\n   * @brief 网络连接类型为 5G 移动网络。\n   */\n\n  NetworkType[NetworkType[\"kNetworkTypeMobile5G\"] = 6] = \"kNetworkTypeMobile5G\";\n})(NetworkType = exports.NetworkType || (exports.NetworkType = {}));\n\n;\n/**\n * @type keytype\n * @brief 是否开启发布性能回退\n */\n\nvar PerformanceAlarmMode;\n\n(function (PerformanceAlarmMode) {\n  /**\n   * @brief 未开启发布性能回退\n   */\n  PerformanceAlarmMode[PerformanceAlarmMode[\"kPerformanceAlarmModeNormal\"] = 0] = \"kPerformanceAlarmModeNormal\";\n  /**\n   * @brief 已开启发布性能回退\n   */\n\n  PerformanceAlarmMode[PerformanceAlarmMode[\"kPerformanceAlarmModeSimulcast\"] = 1] = \"kPerformanceAlarmModeSimulcast\";\n})(PerformanceAlarmMode = exports.PerformanceAlarmMode || (exports.PerformanceAlarmMode = {}));\n\n;\n/**\n * @type keytype\n * @brief onPerformanceAlarms{@link 85533#OnPerformanceAlarms} 告警的原因\n */\n\nvar PerformanceAlarmReason;\n\n(function (PerformanceAlarmReason) {\n  /**\n   * @brief 网络原因差，造成了发送性能回退。仅在开启发送性能回退时，会收到此原因。\n   */\n  PerformanceAlarmReason[PerformanceAlarmReason[\"kPerformanceAlarmReasonBandwidthFallbacked\"] = 0] = \"kPerformanceAlarmReasonBandwidthFallbacked\";\n  /**\n   * @brief 网络性能恢复，发送性能回退恢复。仅在开启发送性能回退时，会收到此原因。\n   */\n\n  PerformanceAlarmReason[PerformanceAlarmReason[\"kPerformanceAlarmReasonBandwidthResumed\"] = 1] = \"kPerformanceAlarmReasonBandwidthResumed\";\n  /**\n   * @brief 如果未开启发送性能回退，收到此告警时，意味着性能不足；<br>如果开启了发送性能回退，收到此告警时，意味着性能不足，且已发生发送性能回退。\n   */\n\n  PerformanceAlarmReason[PerformanceAlarmReason[\"kPerformanceAlarmReasonPerformanceFallbacked\"] = 2] = \"kPerformanceAlarmReasonPerformanceFallbacked\";\n  /**\n   * @brief 如果未开启发送性能回退，收到此告警时，意味着性能不足已恢复；<br> 如果开启了发送性能回退，收到此告警时，意味着性能不足已恢复，且已发生发送性能回退恢复。\n   */\n\n  PerformanceAlarmReason[PerformanceAlarmReason[\"kPerformanceAlarmReasonPerformanceResumed\"] = 3] = \"kPerformanceAlarmReasonPerformanceResumed\";\n})(PerformanceAlarmReason = exports.PerformanceAlarmReason || (exports.PerformanceAlarmReason = {}));\n\n;\n/**\n * @type keytype\n * @brief 媒体设备错误类型\n */\n\nvar MediaDeviceError;\n\n(function (MediaDeviceError) {\n  /**\n   *@brief 媒体设备正常\n   */\n  MediaDeviceError[MediaDeviceError[\"kMediaDeviceErrorOK\"] = 0] = \"kMediaDeviceErrorOK\";\n  /**\n   *@brief 没有权限启动媒体设备\n   */\n\n  MediaDeviceError[MediaDeviceError[\"kMediaDeviceErrorDeviceNoPermission\"] = 1] = \"kMediaDeviceErrorDeviceNoPermission\";\n  /**\n   *@brief 媒体设备已经在使用中\n   */\n\n  MediaDeviceError[MediaDeviceError[\"kMediaDeviceErrorDeviceBusy\"] = 2] = \"kMediaDeviceErrorDeviceBusy\";\n  /**\n   *@brief 媒体设备错误\n   */\n\n  MediaDeviceError[MediaDeviceError[\"kMediaDeviceErrorDeviceFailure\"] = 3] = \"kMediaDeviceErrorDeviceFailure\";\n  /**\n   *@brief 未找到指定的媒体设备\n   */\n\n  MediaDeviceError[MediaDeviceError[\"kMediaDeviceErrorDeviceNotFound\"] = 4] = \"kMediaDeviceErrorDeviceNotFound\";\n  /**\n   *@brief 媒体设备被移除\n   */\n\n  MediaDeviceError[MediaDeviceError[\"kMediaDeviceErrorDeviceDisconnected\"] = 5] = \"kMediaDeviceErrorDeviceDisconnected\";\n})(MediaDeviceError = exports.MediaDeviceError || (exports.MediaDeviceError = {}));\n\n;\n/**\n * @type keytype\n * @brief 用户加入房间的类型。\n */\n\nvar JoinRoomType;\n\n(function (JoinRoomType) {\n  /**\n   * @brief 首次加入房间。用户手动调用 joinRoom{@link 85532#JoinRoom}，收到加入成功。\n   */\n  JoinRoomType[JoinRoomType[\"kJoinRoomTypeFirst\"] = 0] = \"kJoinRoomTypeFirst\";\n  /**\n   * @brief 重新加入房间。用户网络较差，失去与服务器的连接，进行重连时收到加入成功。\n   */\n\n  JoinRoomType[JoinRoomType[\"kJoinRoomTypeReconnected\"] = 1] = \"kJoinRoomTypeReconnected\";\n})(JoinRoomType = exports.JoinRoomType || (exports.JoinRoomType = {}));\n\n;\n;\n;\n;\n;\n;\n;\n/**\n * @type keytype\n * @brief 停止/启动发送音/视频流的状态\n */\n\nvar MuteState;\n\n(function (MuteState) {\n  /**\n   * @brief 启动发送音/视频流的状态\n   */\n  MuteState[MuteState[\"kMuteStateOff\"] = 0] = \"kMuteStateOff\";\n  /**\n   * @brief 停止发送音/视频流的状态\n   */\n\n  MuteState[MuteState[\"kMuteStateOn\"] = 1] = \"kMuteStateOn\";\n})(MuteState = exports.MuteState || (exports.MuteState = {}));\n\n;\n/**\n * @type keytype\n * @brief 流属性\n */\n\nvar StreamIndex;\n\n(function (StreamIndex) {\n  /**\n   * @brief 主流。<br>\n   *        包括：<br>\n   *        + 通过默认摄像头/麦克风采集到的视频/音频; <br>\n   *        + 通过自定义设备采集到的视频/音频。\n   */\n  StreamIndex[StreamIndex[\"kStreamIndexMain\"] = 0] = \"kStreamIndexMain\";\n  /**\n   * @brief 屏幕流。 <br>\n   *        屏幕共享时共享的视频流，或来自声卡的本地播放音频流。\n   */\n\n  StreamIndex[StreamIndex[\"kStreamIndexScreen\"] = 1] = \"kStreamIndexScreen\";\n})(StreamIndex = exports.StreamIndex || (exports.StreamIndex = {}));\n\n;\n/**\n *@brief 合流事件事件类型\n */\n\nvar StreamMixingEvent;\n\n(function (StreamMixingEvent) {\n  /**\n   * @hidden\n   */\n  StreamMixingEvent[StreamMixingEvent[\"kStreamMixingBase\"] = 0] = \"kStreamMixingBase\";\n  /**\n   * @brief 合流开始\n   */\n\n  StreamMixingEvent[StreamMixingEvent[\"kStreamMixingStart\"] = 1] = \"kStreamMixingStart\";\n  /**\n   * @brief 合流启动成功\n   */\n\n  StreamMixingEvent[StreamMixingEvent[\"kStreamMixingStartSuccess\"] = 2] = \"kStreamMixingStartSuccess\";\n  /**\n   * @brief 合流启动失败\n   */\n\n  StreamMixingEvent[StreamMixingEvent[\"kStreamMixingStartFailed\"] = 3] = \"kStreamMixingStartFailed\";\n  /**\n   * @brief 更新合流\n   */\n\n  StreamMixingEvent[StreamMixingEvent[\"kStreamMixingUpdate\"] = 4] = \"kStreamMixingUpdate\";\n  /**\n   * @brief 合流结束\n   */\n\n  StreamMixingEvent[StreamMixingEvent[\"kStreamMixingStop\"] = 5] = \"kStreamMixingStop\";\n  /**\n   * @brief 服务端合流/端云一体合流\n   */\n\n  StreamMixingEvent[StreamMixingEvent[\"kStreamMixingChangeMixType\"] = 6] = \"kStreamMixingChangeMixType\";\n  /**\n   * @brief 收到客户端合流音频首帧\n   */\n\n  StreamMixingEvent[StreamMixingEvent[\"kStreamMixingFirstAudioFrameByClientMix\"] = 7] = \"kStreamMixingFirstAudioFrameByClientMix\";\n  /**\n   * @brief 收到客户端合流视频首帧\n   */\n\n  StreamMixingEvent[StreamMixingEvent[\"kStreamMixingFirstVideoFrameByClientMix\"] = 8] = \"kStreamMixingFirstVideoFrameByClientMix\";\n  /**\n   * @brief 停止服务端合流超时\n   */\n\n  StreamMixingEvent[StreamMixingEvent[\"kStreamMixingStopTimeoutByServer\"] = 9] = \"kStreamMixingStopTimeoutByServer\";\n  /**\n   * @brief 更新合流超时\n   */\n\n  StreamMixingEvent[StreamMixingEvent[\"kStreamMixingUpdateTimeout\"] = 10] = \"kStreamMixingUpdateTimeout\";\n  /**\n  * @brief 合流布局参数错误\n  */\n\n  StreamMixingEvent[StreamMixingEvent[\"kStreamMixingRequestParamError\"] = 11] = \"kStreamMixingRequestParamError\";\n  /**\n   * @hidden\n   */\n\n  StreamMixingEvent[StreamMixingEvent[\"kStreamMixingMax\"] = 15] = \"kStreamMixingMax\";\n})(StreamMixingEvent = exports.StreamMixingEvent || (exports.StreamMixingEvent = {}));\n\n;\n/**\n *@brief 合流类型\n */\n\nvar StreamMixingType;\n\n(function (StreamMixingType) {\n  /**\n   * @brief 服务端合流\n   */\n  StreamMixingType[StreamMixingType[\"kStreamMixingTypeByServer\"] = 0] = \"kStreamMixingTypeByServer\";\n  /**\n   * @brief 客户端合流\n   */\n\n  StreamMixingType[StreamMixingType[\"kStreamMixingTypeByClient\"] = 1] = \"kStreamMixingTypeByClient\";\n})(StreamMixingType = exports.StreamMixingType || (exports.StreamMixingType = {}));\n\n;\n;\n;\n;\n/**\n * @type keytype\n * @brief 房间内远端流被移除的原因。\n */\n\nvar StreamRemoveReason;\n\n(function (StreamRemoveReason) {\n  /**\n   * @brief 远端用户停止发布流。\n   */\n  StreamRemoveReason[StreamRemoveReason[\"kStreamRemoveReasonUnpublish\"] = 0] = \"kStreamRemoveReasonUnpublish\";\n  /**\n   * @brief 远端用户发布流失败。\n   */\n\n  StreamRemoveReason[StreamRemoveReason[\"kStreamRemoveReasonPublishFailed\"] = 1] = \"kStreamRemoveReasonPublishFailed\";\n  /**\n   * @brief 保活失败。\n   */\n\n  StreamRemoveReason[StreamRemoveReason[\"kStreamRemoveReasonKeepLiveFailed\"] = 2] = \"kStreamRemoveReasonKeepLiveFailed\";\n  /**\n   * @brief 远端用户断网。\n   */\n\n  StreamRemoveReason[StreamRemoveReason[\"kStreamRemoveReasonClientDisconnected\"] = 3] = \"kStreamRemoveReasonClientDisconnected\";\n  /**\n   * @brief 远端用户重新发布流。\n   */\n\n  StreamRemoveReason[StreamRemoveReason[\"kStreamRemoveReasonRepublish\"] = 4] = \"kStreamRemoveReasonRepublish\";\n  /**\n   * @brief 其他原因。\n   */\n\n  StreamRemoveReason[StreamRemoveReason[\"kStreamRemoveReasonOther\"] = 5] = \"kStreamRemoveReasonOther\";\n})(StreamRemoveReason = exports.StreamRemoveReason || (exports.StreamRemoveReason = {}));\n\n;\n/**\n * @type keytype\n * @brief 远端订阅流发生回退或恢复的原因\n */\n\nvar FallbackOrRecoverReason;\n\n(function (FallbackOrRecoverReason) {\n  /**\n   * @brief 其他原因，非带宽和性能原因引起的回退或恢复。默认值\n   */\n  FallbackOrRecoverReason[FallbackOrRecoverReason[\"kFallbackOrRecoverReasonUnknown\"] = -1] = \"kFallbackOrRecoverReasonUnknown\";\n  /**\n   * @brief 由带宽不足导致的订阅端音视频流回退。\n   */\n\n  FallbackOrRecoverReason[FallbackOrRecoverReason[\"kFallbackOrRecoverReasonSubscribeFallbackByBandwidth\"] = 0] = \"kFallbackOrRecoverReasonSubscribeFallbackByBandwidth\";\n  /**\n   * @brief 由性能不足导致的订阅端音视频流回退。\n   */\n\n  FallbackOrRecoverReason[FallbackOrRecoverReason[\"kFallbackOrRecoverReasonSubscribeFallbackByPerformance\"] = 1] = \"kFallbackOrRecoverReasonSubscribeFallbackByPerformance\";\n  /**\n   * @brief 由带宽恢复导致的订阅端音视频流恢复。\n   */\n\n  FallbackOrRecoverReason[FallbackOrRecoverReason[\"kFallbackOrRecoverReasonSubscribeRecoverByBandwidth\"] = 2] = \"kFallbackOrRecoverReasonSubscribeRecoverByBandwidth\";\n  /**\n   * @brief 由性能恢复导致的订阅端音视频流恢复。\n   */\n\n  FallbackOrRecoverReason[FallbackOrRecoverReason[\"kFallbackOrRecoverReasonSubscribeRecoverByPerformance\"] = 3] = \"kFallbackOrRecoverReasonSubscribeRecoverByPerformance\";\n  /**\n   * @brief 由带宽不足导致的发布端音视频流回退。\n   */\n\n  FallbackOrRecoverReason[FallbackOrRecoverReason[\"kFallbackOrRecoverReasonPublishFallbackByBandwidth\"] = 4] = \"kFallbackOrRecoverReasonPublishFallbackByBandwidth\";\n  /**\n   * @brief 由性能不足导致的发布端音视频流回退。\n   */\n\n  FallbackOrRecoverReason[FallbackOrRecoverReason[\"kFallbackOrRecoverReasonPublishFallbackByPerformance\"] = 5] = \"kFallbackOrRecoverReasonPublishFallbackByPerformance\";\n  /**\n   * @brief 由带宽恢复导致的发布端音视频流恢复。\n   */\n\n  FallbackOrRecoverReason[FallbackOrRecoverReason[\"kFallbackOrRecoverReasonPublishRecoverByBandwidth\"] = 6] = \"kFallbackOrRecoverReasonPublishRecoverByBandwidth\";\n  /**\n   * @brief 由性能恢复导致的发布端音视频流恢复。\n   */\n\n  FallbackOrRecoverReason[FallbackOrRecoverReason[\"kFallbackOrRecoverReasonPublishRecoverByPerformance\"] = 7] = \"kFallbackOrRecoverReasonPublishRecoverByPerformance\";\n})(FallbackOrRecoverReason = exports.FallbackOrRecoverReason || (exports.FallbackOrRecoverReason = {}));\n\n;\n;\n/**\n* @type keytype\n* @brief 视频帧旋转角度\n*/\n\nvar VideoRotation;\n\n(function (VideoRotation) {\n  /**\n   * @brief 顺时针旋转 0 度\n  */\n  VideoRotation[VideoRotation[\"kVideoRotation0\"] = 0] = \"kVideoRotation0\";\n  /**\n   * @brief 顺时针旋转 90 度\n  */\n\n  VideoRotation[VideoRotation[\"kVideoRotation90\"] = 90] = \"kVideoRotation90\";\n  /**\n   * @brief 顺时针旋转 180 度\n  */\n\n  VideoRotation[VideoRotation[\"kVideoRotation180\"] = 180] = \"kVideoRotation180\";\n  /**\n   * @brief 顺时针旋转 270 度\n  */\n\n  VideoRotation[VideoRotation[\"kVideoRotation270\"] = 270] = \"kVideoRotation270\";\n})(VideoRotation = exports.VideoRotation || (exports.VideoRotation = {}));\n\n;\n;\n/**\n * @type keytype\n * @brief 本地音频流状态。<br>\n *        SDK 通过 onLocalAudioStateChanged{@link 85533#OnLocalAudioStateChanged} 回调本地音频流状态\n */\n\nvar LocalAudioStreamState;\n\n(function (LocalAudioStreamState) {\n  /**\n   * @brief 本地音频默认初始状态。\n   *        麦克风停止工作时回调该状态，对应错误码 kLocalAudioStreamErrorOk\n   */\n  LocalAudioStreamState[LocalAudioStreamState[\"kLocalAudioStreamStateStopped\"] = 0] = \"kLocalAudioStreamStateStopped\";\n  /**\n   * @brief 本地音频录制设备启动成功。\n   *        采集到音频首帧时回调该状态，对应错误码 kLocalAudioStreamErrorOk\n   */\n\n  LocalAudioStreamState[LocalAudioStreamState[\"kLocalAudioStreamStateRecording\"] = 1] = \"kLocalAudioStreamStateRecording\";\n  /**\n   * @brief 本地音频首帧编码成功。\n   *        音频首帧编码成功时回调该状态，对应错误码 kLocalAudioStreamErrorOk\n   */\n\n  LocalAudioStreamState[LocalAudioStreamState[\"kLocalAudioStreamStateEncoding\"] = 2] = \"kLocalAudioStreamStateEncoding\";\n  /**\n   * @brief  本地音频启动失败，在以下时机回调该状态：  <br>\n   *       + 本地录音设备启动失败，对应错误码 kLocalAudioStreamErrorRecordFailure <br>\n   *       + 检测到没有录音设备权限，对应错误码 kLocalAudioStreamErrorDeviceNoPermission  <br>\n   *       + 音频编码失败，对应错误码 kLocalAudioStreamErrorEncodeFailure\n   */\n\n  LocalAudioStreamState[LocalAudioStreamState[\"kLocalAudioStreamStateFailed\"] = 3] = \"kLocalAudioStreamStateFailed\";\n})(LocalAudioStreamState = exports.LocalAudioStreamState || (exports.LocalAudioStreamState = {}));\n\n;\n/**\n * @detail 85534\n * @brief 本地音频流状态改变时的错误码。\n *        SDK 通过 onLocalAudioStateChanged{@link 85533#OnLocalAudioStateChanged} 回调该错误码。\n */\n\nvar LocalAudioStreamError;\n\n(function (LocalAudioStreamError) {\n  /**\n   * @brief 本地音频状态正常\n   */\n  LocalAudioStreamError[LocalAudioStreamError[\"kLocalAudioStreamErrorOk\"] = 0] = \"kLocalAudioStreamErrorOk\";\n  /**\n   * @brief 本地音频出错原因未知\n   */\n\n  LocalAudioStreamError[LocalAudioStreamError[\"kLocalAudioStreamErrorFailure\"] = 1] = \"kLocalAudioStreamErrorFailure\";\n  /**\n   * @brief 没有权限启动本地音频录制设备\n   */\n\n  LocalAudioStreamError[LocalAudioStreamError[\"kLocalAudioStreamErrorDeviceNoPermission\"] = 2] = \"kLocalAudioStreamErrorDeviceNoPermission\";\n  /**\n   * @brief 本地音频录制设备已经在使用中\n   * @notes 该错误码暂未使用\n   */\n\n  LocalAudioStreamError[LocalAudioStreamError[\"kLocalAudioStreamErrorDeviceBusy\"] = 3] = \"kLocalAudioStreamErrorDeviceBusy\";\n  /**\n   * @brief 本地音频录制失败，建议你检查录制设备是否正常工作\n   */\n\n  LocalAudioStreamError[LocalAudioStreamError[\"kLocalAudioStreamErrorRecordFailure\"] = 4] = \"kLocalAudioStreamErrorRecordFailure\";\n  /**\n   * @brief 本地音频编码失败\n   */\n\n  LocalAudioStreamError[LocalAudioStreamError[\"kLocalAudioStreamErrorEncodeFailure\"] = 5] = \"kLocalAudioStreamErrorEncodeFailure\";\n  /**\n   *@brief 没有可用的音频录制设备\n   */\n\n  LocalAudioStreamError[LocalAudioStreamError[\"kLocalAudioStreamErrorNoRecordingDevice\"] = 6] = \"kLocalAudioStreamErrorNoRecordingDevice\";\n})(LocalAudioStreamError = exports.LocalAudioStreamError || (exports.LocalAudioStreamError = {}));\n\n;\n/**\n * @type keytype\n * @brief 远端音频流状态。<br>\n *        用户可以通过 onRemoteAudioStateChanged{@link 85533#OnRemoteAudioStateChanged} 了解该状态。\n */\n\nvar RemoteAudioState;\n\n(function (RemoteAudioState) {\n  /**\n   * @brief  不接收远端音频流。 <br>\n   *         以下情况下会触发回调 onRemoteAudioStateChanged{@link 85533#OnRemoteAudioStateChanged}：  <br>\n   *       + 本地用户停止接收远端音频流，对应原因是： kRemoteAudioStateChangeReasonLocalMuted  <br>\n   *       + 远端用户停止发送音频流，对应原因是： kRemoteAudioStateChangeReasonRemoteMuted  <br>\n   *       + 远端用户离开房间，对应原因是： kRemoteAudioStateChangeReasonRemoteOffline  <br>\n   */\n  RemoteAudioState[RemoteAudioState[\"kRemoteAudioStateStopped\"] = 0] = \"kRemoteAudioStateStopped\";\n  /**\n   * @brief 开始接收远端音频流首包。<br>\n   *        刚收到远端音频流首包会触发回调 onRemoteAudioStateChanged{@link 85533#OnRemoteAudioStateChanged}，\n   *        对应原因是： kRemoteAudioStateChangeReasonLocalUnmuted。\n   */\n\n  RemoteAudioState[RemoteAudioState[\"kRemoteAudioStateStarting\"] = 1] = \"kRemoteAudioStateStarting\";\n  /**\n   * @brief  远端音频流正在解码，正常播放。 <br>\n   *         以下情况下会触发回调 OnRemoteAudioStateChanged{@link 85533#OnRemoteAudioStateChanged}：  <br>\n   *       + 成功解码远端音频首帧，对应原因是： kRemoteAudioStateChangeReasonLocalUnmuted\n   *       + 网络由阻塞恢复正常，对应原因是： kRemoteAudioStateChangeReasonNetworkRecovery\n   *       + 本地用户恢复接收远端音频流，对应原因是： kRemoteAudioStateChangeReasonLocalUnmuted\n   *       + 远端用户恢复发送音频流，对应原因是： kRemoteAudioStateChangeReasonRemoteUnmuted\n   */\n\n  RemoteAudioState[RemoteAudioState[\"kRemoteAudioStateDecoding\"] = 2] = \"kRemoteAudioStateDecoding\";\n  /**\n   * @brief 远端音频流卡顿。<br>\n   *        网络阻塞、丢包率大于 40% 时，会触发回调 onRemoteAudioStateChanged{@link 85533#OnRemoteAudioStateChanged}，\n   *        对应原因是： kRemoteAudioStateChangeReasonNetworkCongestion\n   */\n\n  RemoteAudioState[RemoteAudioState[\"kRemoteAudioStateFrozen\"] = 3] = \"kRemoteAudioStateFrozen\";\n  /**\n   * @hidden\n   * @brief 远端音频流播放失败\n   * @notes 该错误码暂未使用\n   */\n\n  RemoteAudioState[RemoteAudioState[\"kRemoteAudioStateFailed\"] = 4] = \"kRemoteAudioStateFailed\";\n})(RemoteAudioState = exports.RemoteAudioState || (exports.RemoteAudioState = {}));\n\n;\n/**\n* @type keytype\n* @brief 接收远端音频流状态改变的原因。  <br>\n*        用户可以通过 onRemoteAudioStateChanged{@link 85533#OnRemoteAudioStateChanged} 了解该原因。\n*/\n\nvar RemoteAudioStateChangeReason;\n\n(function (RemoteAudioStateChangeReason) {\n  /**\n   * @brief 内部原因\n   */\n  RemoteAudioStateChangeReason[RemoteAudioStateChangeReason[\"kRemoteAudioStateChangeReasonInternal\"] = 0] = \"kRemoteAudioStateChangeReasonInternal\";\n  /**\n   * @brief 网络阻塞\n   */\n\n  RemoteAudioStateChangeReason[RemoteAudioStateChangeReason[\"kRemoteAudioStateChangeReasonNetworkCongestion\"] = 1] = \"kRemoteAudioStateChangeReasonNetworkCongestion\";\n  /**\n   * @brief 网络恢复正常\n   */\n\n  RemoteAudioStateChangeReason[RemoteAudioStateChangeReason[\"kRemoteAudioStateChangeReasonNetworkRecovery\"] = 2] = \"kRemoteAudioStateChangeReasonNetworkRecovery\";\n  /**\n   * @brief 本地用户停止接收远端音频流\n   */\n\n  RemoteAudioStateChangeReason[RemoteAudioStateChangeReason[\"kRemoteAudioStateChangeReasonLocalMuted\"] = 3] = \"kRemoteAudioStateChangeReasonLocalMuted\";\n  /**\n   * @brief 本地用户恢复接收远端音频流\n   */\n\n  RemoteAudioStateChangeReason[RemoteAudioStateChangeReason[\"kRemoteAudioStateChangeReasonLocalUnmuted\"] = 4] = \"kRemoteAudioStateChangeReasonLocalUnmuted\";\n  /**\n   * @brief 远端用户停止发送音频流\n   */\n\n  RemoteAudioStateChangeReason[RemoteAudioStateChangeReason[\"kRemoteAudioStateChangeReasonRemoteMuted\"] = 5] = \"kRemoteAudioStateChangeReasonRemoteMuted\";\n  /**\n   * @brief 远端用户恢复发送音频流\n   */\n\n  RemoteAudioStateChangeReason[RemoteAudioStateChangeReason[\"kRemoteAudioStateChangeReasonRemoteUnmuted\"] = 6] = \"kRemoteAudioStateChangeReasonRemoteUnmuted\";\n  /**\n   * @brief 远端用户离开房间\n   */\n\n  RemoteAudioStateChangeReason[RemoteAudioStateChangeReason[\"kRemoteAudioStateChangeReasonRemoteOffline\"] = 7] = \"kRemoteAudioStateChangeReasonRemoteOffline\";\n})(RemoteAudioStateChangeReason = exports.RemoteAudioStateChangeReason || (exports.RemoteAudioStateChangeReason = {}));\n\n;\n/**\n * @type keytype\n * @brief 本地视频流状态\n */\n\nvar LocalVideoStreamState;\n\n(function (LocalVideoStreamState) {\n  /**\n   * @brief 本地视频采集停止状态\n   */\n  LocalVideoStreamState[LocalVideoStreamState[\"kLocalVideoStreamStateStopped\"] = 0] = \"kLocalVideoStreamStateStopped\";\n  /**\n   * @brief 本地视频采集设备启动成功\n   */\n\n  LocalVideoStreamState[LocalVideoStreamState[\"kLocalVideoStreamStateRecording\"] = 1] = \"kLocalVideoStreamStateRecording\";\n  /**\n   * @brief 本地视频采集后，首帧编码成功\n   */\n\n  LocalVideoStreamState[LocalVideoStreamState[\"kLocalVideoStreamStateEncoding\"] = 2] = \"kLocalVideoStreamStateEncoding\";\n  /**\n   * @brief 本地视频采集设备启动失败\n   */\n\n  LocalVideoStreamState[LocalVideoStreamState[\"kLocalVideoStreamStateFailed\"] = 3] = \"kLocalVideoStreamStateFailed\";\n})(LocalVideoStreamState = exports.LocalVideoStreamState || (exports.LocalVideoStreamState = {}));\n\n;\n/**\n* @errorcodes\n* @brief 本地视频状态改变时的错误码\n*/\n\n/** {en}\n* @errorcodes\n* @brief Error Codes for the local video state changed\n*/\n\nvar LocalVideoStreamError;\n\n(function (LocalVideoStreamError) {\n  /**\n   * @brief 状态正常\n   */\n\n  /** {en}\n   * @brief Normal\n   */\n  LocalVideoStreamError[LocalVideoStreamError[\"kLocalVideoStreamErrorOk\"] = 0] = \"kLocalVideoStreamErrorOk\";\n  /**\n   * @brief 本地视频流发布失败\n   */\n\n  LocalVideoStreamError[LocalVideoStreamError[\"kLocalVideoStreamErrorFailure\"] = 1] = \"kLocalVideoStreamErrorFailure\";\n  /**\n   * @brief 没有权限启动本地视频采集设备\n   */\n\n  LocalVideoStreamError[LocalVideoStreamError[\"kLocalVideoStreamErrorDeviceNoPermission\"] = 2] = \"kLocalVideoStreamErrorDeviceNoPermission\";\n  /**\n   * @brief 本地视频采集设备被占用\n   */\n\n  LocalVideoStreamError[LocalVideoStreamError[\"kLocalVideoStreamErrorDeviceBusy\"] = 3] = \"kLocalVideoStreamErrorDeviceBusy\";\n  /**\n   * @brief 本地视频采集设备不存在\n   */\n\n  LocalVideoStreamError[LocalVideoStreamError[\"kLocalVideoStreamErrorDeviceNotFound\"] = 4] = \"kLocalVideoStreamErrorDeviceNotFound\";\n  /**\n   * @brief 本地视频采集失败，建议检查采集设备是否正常工作\n   */\n\n  LocalVideoStreamError[LocalVideoStreamError[\"kLocalVideoStreamErrorCaptureFailure\"] = 5] = \"kLocalVideoStreamErrorCaptureFailure\";\n  /**\n   * @brief 本地视频编码失败\n   */\n\n  LocalVideoStreamError[LocalVideoStreamError[\"kLocalVideoStreamErrorEncodeFailure\"] = 6] = \"kLocalVideoStreamErrorEncodeFailure\";\n  /**\n   * @brief 本地视频采集设备被移除\n   */\n\n  LocalVideoStreamError[LocalVideoStreamError[\"kLocalVideoStreamErrorDeviceDisconnected\"] = 7] = \"kLocalVideoStreamErrorDeviceDisconnected\";\n})(LocalVideoStreamError = exports.LocalVideoStreamError || (exports.LocalVideoStreamError = {}));\n\n;\n/**\n * @type keytype\n * @brief 远端视频流状态。状态改变时，会收到回调： onRemoteVideoStateChanged{@link 85533#OnRemoteVideoStateChanged}\n */\n\nvar RemoteVideoState;\n\n(function (RemoteVideoState) {\n  /**\n   * @brief 远端视频流默认初始状态，视频尚未开始播放。\n   */\n  RemoteVideoState[RemoteVideoState[\"kRemoteVideoStateStopped\"] = 0] = \"kRemoteVideoStateStopped\";\n  /**\n   * @brief 本地用户已接收远端视频流首包。\n   */\n\n  RemoteVideoState[RemoteVideoState[\"kRemoteVideoStateStarting\"] = 1] = \"kRemoteVideoStateStarting\";\n  /**\n   * @brief 远端视频流正在解码，正常播放。\n   */\n\n  RemoteVideoState[RemoteVideoState[\"kRemoteVideoStateDecoding\"] = 2] = \"kRemoteVideoStateDecoding\";\n  /**\n   * @brief 远端视频流卡顿，可能有网络等原因。\n   */\n\n  RemoteVideoState[RemoteVideoState[\"kRemoteVideoStateFrozen\"] = 3] = \"kRemoteVideoStateFrozen\";\n  /**\n   * @brief 远端视频流播放失败。\n   */\n\n  RemoteVideoState[RemoteVideoState[\"kRemoteVideoStateFailed\"] = 4] = \"kRemoteVideoStateFailed\";\n})(RemoteVideoState = exports.RemoteVideoState || (exports.RemoteVideoState = {}));\n\n;\n/**\n* @type keytype\n* @brief 远端视频流状态改变的原因\n*/\n\nvar RemoteVideoStateChangeReason;\n\n(function (RemoteVideoStateChangeReason) {\n  /**\n   * @brief 内部原因\n   */\n  RemoteVideoStateChangeReason[RemoteVideoStateChangeReason[\"kRemoteVideoStateChangeReasonInternal\"] = 0] = \"kRemoteVideoStateChangeReasonInternal\";\n  /**\n   * @brief 网络阻塞\n   */\n\n  RemoteVideoStateChangeReason[RemoteVideoStateChangeReason[\"kRemoteVideoStateChangeReasonNetworkCongestion\"] = 1] = \"kRemoteVideoStateChangeReasonNetworkCongestion\";\n  /**\n   * @brief 网络恢复正常\n   */\n\n  RemoteVideoStateChangeReason[RemoteVideoStateChangeReason[\"kRemoteVideoStateChangeReasonNetworkRecovery\"] = 2] = \"kRemoteVideoStateChangeReasonNetworkRecovery\";\n  /**\n   * @brief 本地用户停止接收远端视频流或本地用户禁用视频模块\n   */\n\n  RemoteVideoStateChangeReason[RemoteVideoStateChangeReason[\"kRemoteVideoStateChangeReasonLocalMuted\"] = 3] = \"kRemoteVideoStateChangeReasonLocalMuted\";\n  /**\n   * @brief 本地用户恢复接收远端视频流或本地用户启用视频模块\n   */\n\n  RemoteVideoStateChangeReason[RemoteVideoStateChangeReason[\"kRemoteVideoStateChangeReasonLocalUnmuted\"] = 4] = \"kRemoteVideoStateChangeReasonLocalUnmuted\";\n  /**\n   * @brief 远端用户停止发送视频流或远端用户禁用视频模块\n   */\n\n  RemoteVideoStateChangeReason[RemoteVideoStateChangeReason[\"kRemoteVideoStateChangeReasonRemoteMuted\"] = 5] = \"kRemoteVideoStateChangeReasonRemoteMuted\";\n  /**\n   * @brief 远端用户恢复发送视频流或远端用户启用视频模块\n   */\n\n  RemoteVideoStateChangeReason[RemoteVideoStateChangeReason[\"kRemoteVideoStateChangeReasonRemoteUnmuted\"] = 6] = \"kRemoteVideoStateChangeReasonRemoteUnmuted\";\n  /**\n   * @brief 远端用户离开频道。\n   *        状态转换参考 onStreamRemove{@link 85533#OnStreamRemove}\n   */\n\n  RemoteVideoStateChangeReason[RemoteVideoStateChangeReason[\"kRemoteVideoStateChangeReasonRemoteOffline\"] = 7] = \"kRemoteVideoStateChangeReasonRemoteOffline\";\n})(RemoteVideoStateChangeReason = exports.RemoteVideoStateChangeReason || (exports.RemoteVideoStateChangeReason = {}));\n\n;\n;\n;\n;\n/**\n * @type keytype\n * @region 房间管理\n * @brief 首帧发送状态\n */\n\nvar FirstFrameSendState;\n\n(function (FirstFrameSendState) {\n  /**\n   * @brief 发送中\n   */\n  FirstFrameSendState[FirstFrameSendState[\"kFirstFrameSendStateSending\"] = 0] = \"kFirstFrameSendStateSending\";\n  /**\n   * @brief 发送成功\n   */\n\n  FirstFrameSendState[FirstFrameSendState[\"kFirstFrameSendStateSent\"] = 1] = \"kFirstFrameSendStateSent\";\n  /**\n   * @brief 发送失败\n   */\n\n  FirstFrameSendState[FirstFrameSendState[\"kFirstFrameSendStateEnd\"] = 2] = \"kFirstFrameSendStateEnd\";\n})(FirstFrameSendState = exports.FirstFrameSendState || (exports.FirstFrameSendState = {}));\n\n;\n/**\n* @type keytype\n* @region 房间管理\n* @brief 首帧播放状态\n*/\n\nvar FirstFramePlayState;\n\n(function (FirstFramePlayState) {\n  /**\n   * @brief 播放中\n   */\n  FirstFramePlayState[FirstFramePlayState[\"kFirstFramePlayStatePlaying\"] = 0] = \"kFirstFramePlayStatePlaying\";\n  /**\n   * @brief 播放成功\n   */\n\n  FirstFramePlayState[FirstFramePlayState[\"kFirstFramePlayStatePlayed\"] = 1] = \"kFirstFramePlayStatePlayed\";\n  /**\n   * @brief 播放失败\n   */\n\n  FirstFramePlayState[FirstFramePlayState[\"kFirstFramePlayStateEnd\"] = 2] = \"kFirstFramePlayStateEnd\";\n})(FirstFramePlayState = exports.FirstFramePlayState || (exports.FirstFramePlayState = {}));\n\n;\n/**\n * @brief 混音播放类型\n */\n\nvar AudioMixingType;\n\n(function (AudioMixingType) {\n  /**\n   * @brief 仅本地播放\n   */\n  AudioMixingType[AudioMixingType[\"kAudioMixingTypePlayout\"] = 0] = \"kAudioMixingTypePlayout\";\n  /**\n   * @brief 仅远端播放\n   */\n\n  AudioMixingType[AudioMixingType[\"kAudioMixingTypePublish\"] = 1] = \"kAudioMixingTypePublish\";\n  /**\n   * @brief 本地和远端同时播放\n   */\n\n  AudioMixingType[AudioMixingType[\"kAudioMixingTypePlayoutAndPublish\"] = 2] = \"kAudioMixingTypePlayoutAndPublish\";\n})(AudioMixingType = exports.AudioMixingType || (exports.AudioMixingType = {}));\n\n;\nvar PublishFallbackOption;\n\n(function (PublishFallbackOption) {\n  /**\n   * @brief 关闭发送视频流时的性能回退功能，默认值\n   */\n  PublishFallbackOption[PublishFallbackOption[\"kPublishFallbackOptionDisabled\"] = 0] = \"kPublishFallbackOptionDisabled\";\n  /**\n   * @brief 开启在网络情况不佳或设备性能不足时只发送小流的功能\n   */\n\n  PublishFallbackOption[PublishFallbackOption[\"kPublishFallbackOptionSimulcast\"] = 1] = \"kPublishFallbackOptionSimulcast\";\n})(PublishFallbackOption = exports.PublishFallbackOption || (exports.PublishFallbackOption = {}));\n\n;\n/**\n * @type keytype\n * @brief 房间模式\n */\n\nvar RoomProfileType;\n\n(function (RoomProfileType) {\n  /**\n   * @brief 普通音视频通话模式。<br>\n   *        你应在 1V1 音视频通话时，使用此设置。<br>\n   *        此设置下，弱网抗性较好。\n   */\n  RoomProfileType[RoomProfileType[\"kRoomProfileTypeCommunication\"] = 0] = \"kRoomProfileTypeCommunication\";\n  /**\n   * @brief 直播模式。<br>\n   *        当你对音视频通话的音质和画质要求较高时，应使用此设置。<br>\n   *        此设置下，当用户使用蓝牙耳机收听时，蓝牙耳机使用媒体模式。\n   */\n\n  RoomProfileType[RoomProfileType[\"kRoomProfileTypeLiveBroadcasting\"] = 1] = \"kRoomProfileTypeLiveBroadcasting\";\n  /**\n   * @brief 游戏语音模式。此模式下延时较低。<br>\n   *        低端机在此模式下运行时，进行了额外的性能优化：<br>\n   *            + 采集播放采用 16kHz 单通道采样 <br>\n   *            + 部分低端机型配置编码帧长 40/60 <br>\n   *            + 部分低端机型关闭软件 3A 音频处理 <br>\n   *        增强对 iOS 其他屏幕录制进行的兼容性，避免音频录制被 RTC 打断。\n   */\n\n  RoomProfileType[RoomProfileType[\"kRoomProfileTypeGame\"] = 2] = \"kRoomProfileTypeGame\";\n  /**\n   * @brief 云游戏模式。<br>\n   *        如果你需要低延迟、高码率的设置时，你可以使用此设置。<br>\n   *        此设置下，弱网抗性较差。\n   */\n\n  RoomProfileType[RoomProfileType[\"kRoomProfileTypeCloudGame\"] = 3] = \"kRoomProfileTypeCloudGame\";\n  /**\n   * @brief 低时延模式。SDK 会使用低延时设置。  <br>\n   *        当你的场景非游戏或云游戏场景，又需要极低延时的体验时，可以使用该模式。 <br>\n   *        该模式下，音视频通话延时会明显降低，但同时弱网抗性、通话音质等均会受到一定影响。  <br>\n   *        在使用此模式前，强烈建议咨询技术支持同学。\n   */\n\n  RoomProfileType[RoomProfileType[\"kRoomProfileTypeLowLatency\"] = 4] = \"kRoomProfileTypeLowLatency\";\n})(RoomProfileType = exports.RoomProfileType || (exports.RoomProfileType = {}));\n\n;\n/**\n * @type keytype\n * @brief 订阅回退选项\n */\n\nvar SubscribeFallbackOption;\n\n(function (SubscribeFallbackOption) {\n  /**\n   * @brief 下行网络较弱时，关闭订阅音视频流时的性能回退功能，默认值\n   */\n  SubscribeFallbackOption[SubscribeFallbackOption[\"kSubscribeFallbackOptionDisable\"] = 0] = \"kSubscribeFallbackOptionDisable\";\n  /**\n   * @brief 下行网络较弱时，只接收视频小流\n   */\n\n  SubscribeFallbackOption[SubscribeFallbackOption[\"kSubscribeFallbackOptionVideoStreamLow\"] = 1] = \"kSubscribeFallbackOptionVideoStreamLow\";\n  /**\n   * @brief 下行网络较弱时，先尝试只接收视频小流；如果网络环境无法显示视频，则再回退到只接收远端订阅的音频流\n   */\n\n  SubscribeFallbackOption[SubscribeFallbackOption[\"kSubscribeFallbackOptionAudioOnly\"] = 2] = \"kSubscribeFallbackOptionAudioOnly\";\n})(SubscribeFallbackOption = exports.SubscribeFallbackOption || (exports.SubscribeFallbackOption = {}));\n\n;\n;\n;\n/**\n * @type keytype\n * @brief 是否开启镜像模式\n */\n\nvar MirrorMode;\n\n(function (MirrorMode) {\n  /**\n   * @brief 不开启\n   */\n  MirrorMode[MirrorMode[\"kMirrorModeOff\"] = 0] = \"kMirrorModeOff\";\n  /**\n   * @brief 开启\n   */\n\n  MirrorMode[MirrorMode[\"kMirrorModeOn\"] = 1] = \"kMirrorModeOn\";\n})(MirrorMode = exports.MirrorMode || (exports.MirrorMode = {}));\n\n;\n/**\n * @type keytype\n * @brief 暂停/恢复接收远端的媒体流类型。\n */\n\nvar PauseResumeControlMediaType;\n\n(function (PauseResumeControlMediaType) {\n  /**\n   * @brief 只控制音频，不影响视频\n   */\n  PauseResumeControlMediaType[PauseResumeControlMediaType[\"kRTCPauseResumeControlMediaTypeAudio\"] = 0] = \"kRTCPauseResumeControlMediaTypeAudio\";\n  /**\n   * @brief 只控制视频，不影响音频\n   */\n\n  PauseResumeControlMediaType[PauseResumeControlMediaType[\"kRTCPauseResumeControlMediaTypeVideo\"] = 1] = \"kRTCPauseResumeControlMediaTypeVideo\";\n  /**\n   * @brief 同时控制音频和视频\n   */\n\n  PauseResumeControlMediaType[PauseResumeControlMediaType[\"kRTCPauseResumeControlMediaTypeVideoAndAudio\"] = 2] = \"kRTCPauseResumeControlMediaTypeVideoAndAudio\";\n})(PauseResumeControlMediaType = exports.PauseResumeControlMediaType || (exports.PauseResumeControlMediaType = {}));\n\n;\n;\n/**\n * @type keytype\n * @region 屏幕共享\n * @brief 内部采集屏幕视频流时，是否采集鼠标信息\n */\n\nvar MouseCursorCaptureState;\n\n(function (MouseCursorCaptureState) {\n  /**\n   * @brief 采集鼠标信息\n   */\n  MouseCursorCaptureState[MouseCursorCaptureState[\"kMouseCursorCaptureStateOn\"] = 0] = \"kMouseCursorCaptureStateOn\";\n  /**\n   * @brief 不采集鼠标信息\n   */\n\n  MouseCursorCaptureState[MouseCursorCaptureState[\"kMouseCursorCaptureStateOff\"] = 1] = \"kMouseCursorCaptureStateOff\";\n})(MouseCursorCaptureState = exports.MouseCursorCaptureState || (exports.MouseCursorCaptureState = {}));\n\n;\n;\n\n//# sourceURL=webpack:///./node_modules/@byted/vertc-electron-sdk/js/types.js?");

/***/ }),

/***/ "./node_modules/@byted/vertc-electron-sdk/js/utils/logger.js":
/*!*******************************************************************!*\
  !*** ./node_modules/@byted/vertc-electron-sdk/js/utils/logger.js ***!
  \*******************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nvar Events = {};\nconst LOG_LEVEL_NONE = 0;\nconst LOG_LEVEL_FATAL = 1;\nconst LOG_LEVEL_ERROR = 2;\nconst LOG_LEVEL_WARNING = 3;\nconst LOG_LEVEL_INFO = 4;\nconst LOG_LEVEL_DEBUG = 5;\n\nfunction Logger(context) {\n  if (context) {\n    this.context_ = context;\n    Events = this.context_.Events;\n    this.eventbus_ = this.context_.eventbus;\n  }\n\n  this.logToBrowserConsole_ = true;\n  this.showLogTimestamp_ = true;\n  this.showCalleeName_ = true;\n  this.startTime_ = new Date().getTime();\n}\n/**\n * Prepends a timestamp in milliseconds to each log message.\n * @param {boolean} value Set to true if you want to see a timestamp in each log message.\n * @default false\n * @memberof module:Logger\n * @instance\n */\n\n\nLogger.prototype.setLogTimestampVisible = function (value) {\n  this.showLogTimestamp_ = value;\n};\n/**\n * Prepends the callee object name, and media type if available, to each log message.\n * @param {boolean} value Set to true if you want to see the callee object name and media type in each log message.\n * @default false\n * @memberof module:Logger\n * @instance\n */\n\n\nLogger.prototype.setCalleeNameVisible = function (value) {\n  this.showCalleeName_ = value;\n};\n/**\n * Toggles logging to the browser's javascript console.  If you set to false you will still receive a log event with the same message.\n * @param {boolean} value Set to false if you want to turn off logging to the browser's console.\n * @default true\n * @memberof module:Logger\n * @instance\n */\n\n\nLogger.prototype.setLogToBrowserConsole = function (value) {\n  this.logToBrowserConsole_ = value;\n};\n/**\n * Use this method to get the state of this.logToBrowserConsole_.\n * @returns {boolean} The current value of this.logToBrowserConsole_\n * @memberof module:Logger\n * @instance\n */\n\n\nLogger.prototype.getLogToBrowserConsole = function () {\n  return this.logToBrowserConsole_;\n};\n\nLogger.prototype.fatal = function (...params) {\n  this.doLog(LOG_LEVEL_FATAL, ...params);\n};\n\nLogger.prototype.error = function (...params) {\n  this.doLog(LOG_LEVEL_ERROR, ...params);\n};\n\nLogger.prototype.warn = function (...params) {\n  this.doLog(LOG_LEVEL_WARNING, ...params);\n};\n\nLogger.prototype.info = function (...params) {\n  this.doLog(LOG_LEVEL_INFO, ...params);\n};\n\nLogger.prototype.debug = function (...params) {\n  this.doLog(LOG_LEVEL_DEBUG, ...params);\n};\n\nLogger.prototype.log = function (...params) {\n  this.doLog(LOG_LEVEL_DEBUG, ...params);\n};\n\nLogger.prototype.getTime = function () {\n  let logTime = new Date();\n  message += '[' + ('0' + (logTime.getMonth() + 1)).slice(-2) + '-' + ('0' + logTime.getDate()).slice(-2) + ' ' + ('0' + logTime.getHours()).slice(-2) + ':' + ('0' + logTime.getMinutes()).slice(-2) + ':' + ('0' + logTime.getSeconds()).slice(-2) + '.' + ('0' + logTime.getMilliseconds()).slice(-3) + ']';\n  return logTime;\n};\n/**\n * This method will allow you send log messages to either the browser's console and/or dispatch an event to capture at the media player level.\n * @param {...*} arguments The message you want to log. The Arguments object is supported for this method so you can send in comma separated logging items.\n * @memberof module:Logger\n * @instance\n */\n\n\nLogger.prototype.doLog = function (level, ...params) {\n  let message = '';\n  let logTime = null;\n\n  if (this.showLogTimestamp_) {\n    // old from dashjs\n    //logTime = new Date().getTime();\n    //message += '[' + (logTime - this.startTime_) + ']';\n    // new by oldmtn\n    logTime = new Date();\n    message += '[' + ('0' + (logTime.getMonth() + 1)).slice(-2) + '-' + ('0' + logTime.getDate()).slice(-2) + ' ' + ('0' + logTime.getHours()).slice(-2) + ':' + ('0' + logTime.getMinutes()).slice(-2) + ':' + ('0' + logTime.getSeconds()).slice(-2) + '.' + ('0' + logTime.getMilliseconds()).slice(-3) + ']';\n  } // if (this.showCalleeName_ && this && this.getClassName) {\n  //   message += '[' + this.getClassName() + ']';\n  //   if (this.getType) {\n  //     message += '[' + this.getType() + ']';\n  //   }\n  // }\n  // if (message.length > 0) {\n  //     message += ' ';\n  // }\n  // Array.apply(null, params).forEach(function (item) {\n  //     message += item + ' ';\n  // });\n\n\n  switch (level) {\n    case LOG_LEVEL_NONE:\n    case LOG_LEVEL_FATAL:\n    case LOG_LEVEL_ERROR:\n    case LOG_LEVEL_WARNING:\n      {\n        console.warn(message, ...params);\n      }\n      break;\n\n    case LOG_LEVEL_INFO:\n    case LOG_LEVEL_DEBUG:\n      {\n        console.log(message, ...params);\n      }\n      break;\n\n    default:\n      break;\n  }\n\n  if (this.eventbus_) {\n    this.eventbus_.trigger(Events.LOG, {\n      message: message\n    });\n  }\n};\n\nvar logger = new Logger();\nmodule.exports = {\n  logger\n};\n\n//# sourceURL=webpack:///./node_modules/@byted/vertc-electron-sdk/js/utils/logger.js?");

/***/ }),

/***/ "./node_modules/@byted/vertc-electron-sdk/js/utils/yuv_render.js":
/*!***********************************************************************!*\
  !*** ./node_modules/@byted/vertc-electron-sdk/js/utils/yuv_render.js ***!
  \***********************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\n\nconst YUVCanvas = __webpack_require__(/*! yuv-canvas */ \"./node_modules/@byted/vertc-electron-sdk/node_modules/yuv-canvas/src/yuv-canvas.js\");\n\nconst {\n  RenderMode\n} = __webpack_require__(/*! ../types */ \"./node_modules/@byted/vertc-electron-sdk/js/types.js\");\n\nclass YUVRender {\n  constructor(element, renderMode = RenderMode.FIT, mirror, context) {\n    this.destroied = false; // [parentDiv [containerDiv [canvas ]]];\n\n    this.parent = element;\n    this.renderMode = renderMode;\n    this.container = document.createElement('div');\n    this.mirror_ = mirror;\n    this.context_ = context || {}; // this.container.style.border = '1px solid red'; // for debug\n\n    Object.assign(this.container.style, {\n      width: '100%',\n      height: '100%',\n      display: 'flex',\n      justifyContent: 'center',\n      alignItems: 'center',\n      backgroundColor: \"black\"\n    });\n    this.canvas = document.createElement('canvas');\n    this.canvas.setAttribute(\"style\", \"overflow: hidden\");\n    this.parent.appendChild(this.container);\n    this.container.appendChild(this.canvas); // by my test results, it will cose averate 12ms to render a YUV frame if webGL is off\n    // and it will cost about 3ms whe webGL is on\n\n    let flagUseWebGLRenderer = true;\n\n    if (this.context_.flagUseWebGLRenderer) {\n      flagUseWebGLRenderer = true;\n    }\n\n    this.yuvCanvas = YUVCanvas.attach(this.canvas, {\n      webGL: flagUseWebGLRenderer\n    });\n  } // see https://github.com/brion/yuv-buffer\n\n\n  static buildYUVFrame(data) {\n    const {\n      format,\n      width,\n      height,\n      planeY,\n      planeU,\n      planeV,\n      planeSizeY,\n      planeSizeU,\n      planeSizeV,\n      rotation,\n      displayWidth,\n      displayHeight\n    } = data;\n    let frame = {\n      format: {\n        width: width,\n        height: height,\n        chromaWidth: width / 2,\n        chromaHeight: height / 2,\n        cropLeft: 0,\n        cropTop: 0,\n        cropWidth: width,\n        cropHeight: height,\n        displayWidth: width,\n        displayHeight: height,\n        rotation: rotation\n      },\n      y: {\n        bytes: planeY,\n        stride: planeSizeY\n      },\n      u: {\n        bytes: planeU,\n        stride: planeSizeU\n      },\n      v: {\n        bytes: planeV,\n        stride: planeSizeV\n      }\n    };\n    return frame;\n  }\n\n  destroy() {\n    if (!this.destroied) {\n      this.yuvCanvas.clear();\n      this.canvas.remove();\n      this.container.remove();\n      this.yuvCanvas = null;\n      this.destroied = true;\n    }\n  }\n\n  setMirrorType(mirror) {\n    this.mirror_ = mirror;\n  }\n\n  clearFrame() {\n    if (this.yuvCanvas) {\n      this.yuvCanvas.clear();\n    } else {\n      console.warn('call renderFrame, but yuvCanvas is null');\n    }\n  }\n\n  renderFrame(frame) {\n    if (this.yuvCanvas) {\n      this.adjustRender(frame);\n\n      if (this.context_.flagNotRenderFrame) {} else {\n        this.yuvCanvas.drawFrame(frame);\n      }\n    } else {\n      console.warn('call renderFrame, but yuvCanvas is null');\n    }\n  }\n\n  adjustRender(frame) {\n    let containerRatio = this.container.clientWidth / this.container.clientHeight;\n    let targetWidth;\n    let targetHeight;\n\n    if (frame.format.rotation === 90 || frame.format.rotation === 270) {\n      targetWidth = frame.format.height;\n      targetHeight = frame.format.width;\n    } else {\n      targetWidth = frame.format.width;\n      targetHeight = frame.format.height;\n    }\n\n    let frameRatio = targetWidth / targetHeight;\n\n    switch (this.renderMode) {\n      case RenderMode.HIDDEN:\n        {\n          if (containerRatio >= frameRatio) {\n            this.canvas.style.zoom = String(this.container.clientWidth / targetWidth);\n          } else {\n            this.canvas.style.zoom = String(this.container.clientHeight / targetHeight);\n          }\n\n          let transform = 'rotateZ(' + frame.format.rotation.toString() + 'deg)';\n\n          if (this.mirror_) {\n            transform += ' rotateY(180deg)';\n          }\n\n          this.canvas.style.transform = transform;\n        }\n        break;\n\n      case RenderMode.FIT:\n        {\n          if (containerRatio >= frameRatio) {\n            this.canvas.style.zoom = String(this.container.clientHeight / targetHeight);\n          } else {\n            this.canvas.style.zoom = String(this.container.clientWidth / targetWidth);\n          }\n\n          let transform = 'rotateZ(' + frame.format.rotation.toString() + 'deg)';\n\n          if (this.mirror_) {\n            transform += ' rotateY(180deg)';\n          }\n\n          this.canvas.style.transform = transform;\n        }\n        break;\n\n      default:\n        {\n          console.warn(`adjustRender with an unexpected mode: ${this.renderMode}`);\n        }\n        break;\n    }\n  }\n\n}\n\nexports.YUVRender = YUVRender;\n\n//# sourceURL=webpack:///./node_modules/@byted/vertc-electron-sdk/js/utils/yuv_render.js?");

/***/ }),

/***/ "./node_modules/@byted/vertc-electron-sdk/node_modules/yuv-canvas/build/shaders.js":
/*!*****************************************************************************************!*\
  !*** ./node_modules/@byted/vertc-electron-sdk/node_modules/yuv-canvas/build/shaders.js ***!
  \*****************************************************************************************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("module.exports = {\n  vertex: \"precision lowp float;\\n\\nattribute vec2 aPosition;\\nattribute vec2 aLumaPosition;\\nattribute vec2 aChromaPosition;\\nvarying vec2 vLumaPosition;\\nvarying vec2 vChromaPosition;\\nvoid main() {\\n    gl_Position = vec4(aPosition, 0, 1);\\n    vLumaPosition = aLumaPosition;\\n    vChromaPosition = aChromaPosition;\\n}\\n\",\n  fragment: \"// inspired by https://github.com/mbebenita/Broadway/blob/master/Player/canvas.js\\n\\nprecision lowp float;\\n\\nuniform sampler2D uTextureY;\\nuniform sampler2D uTextureCb;\\nuniform sampler2D uTextureCr;\\nvarying vec2 vLumaPosition;\\nvarying vec2 vChromaPosition;\\nvoid main() {\\n   // Y, Cb, and Cr planes are uploaded as LUMINANCE textures.\\n   float fY = texture2D(uTextureY, vLumaPosition).x;\\n   float fCb = texture2D(uTextureCb, vChromaPosition).x;\\n   float fCr = texture2D(uTextureCr, vChromaPosition).x;\\n\\n   // Premultipy the Y...\\n   float fYmul = fY * 1.1643828125;\\n\\n   // And convert that to RGB!\\n   gl_FragColor = vec4(\\n     fYmul + 1.59602734375 * fCr - 0.87078515625,\\n     fYmul - 0.39176171875 * fCb - 0.81296875 * fCr + 0.52959375,\\n     fYmul + 2.017234375   * fCb - 1.081390625,\\n     1\\n   );\\n}\\n\",\n  vertexStripe: \"precision lowp float;\\n\\nattribute vec2 aPosition;\\nattribute vec2 aTexturePosition;\\nvarying vec2 vTexturePosition;\\n\\nvoid main() {\\n    gl_Position = vec4(aPosition, 0, 1);\\n    vTexturePosition = aTexturePosition;\\n}\\n\",\n  fragmentStripe: \"// extra 'stripe' texture fiddling to work around IE 11's poor performance on gl.LUMINANCE and gl.ALPHA textures\\n\\nprecision lowp float;\\n\\nuniform sampler2D uStripe;\\nuniform sampler2D uTexture;\\nvarying vec2 vTexturePosition;\\nvoid main() {\\n   // Y, Cb, and Cr planes are mapped into a pseudo-RGBA texture\\n   // so we can upload them without expanding the bytes on IE 11\\n   // which doesn't allow LUMINANCE or ALPHA textures\\n   // The stripe textures mark which channel to keep for each pixel.\\n   // Each texture extraction will contain the relevant value in one\\n   // channel only.\\n\\n   float fLuminance = dot(\\n      texture2D(uStripe, vTexturePosition),\\n      texture2D(uTexture, vTexturePosition)\\n   );\\n\\n   gl_FragColor = vec4(fLuminance, fLuminance, fLuminance, 1);\\n}\\n\"\n};\n\n//# sourceURL=webpack:///./node_modules/@byted/vertc-electron-sdk/node_modules/yuv-canvas/build/shaders.js?");

/***/ }),

/***/ "./node_modules/@byted/vertc-electron-sdk/node_modules/yuv-canvas/src/FrameSink.js":
/*!*****************************************************************************************!*\
  !*** ./node_modules/@byted/vertc-electron-sdk/node_modules/yuv-canvas/src/FrameSink.js ***!
  \*****************************************************************************************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("(function () {\n  \"use strict\";\n  /**\n   * Create a YUVCanvas and attach it to an HTML5 canvas element.\n   *\n   * This will take over the drawing context of the canvas and may turn\n   * it into a WebGL 3d canvas if possible. Do not attempt to use the\n   * drawing context directly after this.\n   *\n   * @param {HTMLCanvasElement} canvas - HTML canvas element to attach to\n   * @param {YUVCanvasOptions} options - map of options\n   * @throws exception if WebGL requested but unavailable\n   * @constructor\n   * @abstract\n   */\n\n  function FrameSink(canvas, options) {\n    throw new Error('abstract');\n  }\n  /**\n   * Draw a single YUV frame on the underlying canvas, converting to RGB.\n   * If necessary the canvas will be resized to the optimal pixel size\n   * for the given buffer's format.\n   *\n   * @param {YUVBuffer} buffer - the YUV buffer to draw\n   * @see {@link https://www.npmjs.com/package/yuv-buffer|yuv-buffer} for format\n   */\n\n\n  FrameSink.prototype.drawFrame = function (buffer) {\n    throw new Error('abstract');\n  };\n  /**\n   * Clear the canvas using appropriate underlying 2d or 3d context.\n   */\n\n\n  FrameSink.prototype.clear = function () {\n    throw new Error('abstract');\n  };\n\n  module.exports = FrameSink;\n})();\n\n//# sourceURL=webpack:///./node_modules/@byted/vertc-electron-sdk/node_modules/yuv-canvas/src/FrameSink.js?");

/***/ }),

/***/ "./node_modules/@byted/vertc-electron-sdk/node_modules/yuv-canvas/src/SoftwareFrameSink.js":
/*!*************************************************************************************************!*\
  !*** ./node_modules/@byted/vertc-electron-sdk/node_modules/yuv-canvas/src/SoftwareFrameSink.js ***!
  \*************************************************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

eval("/*\nCopyright (c) 2014-2016 Brion Vibber <brion@pobox.com>\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of\nthis software and associated documentation files (the \"Software\"), to deal in\nthe Software without restriction, including without limitation the rights to\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\nthe Software, and to permit persons to whom the Software is furnished to do so,\nsubject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\nFOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\nCOPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\nIN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\nCONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n*/\n(function () {\n  \"use strict\";\n\n  var FrameSink = __webpack_require__(/*! ./FrameSink.js */ \"./node_modules/@byted/vertc-electron-sdk/node_modules/yuv-canvas/src/FrameSink.js\"),\n      YCbCr = __webpack_require__(/*! ./YCbCr.js */ \"./node_modules/@byted/vertc-electron-sdk/node_modules/yuv-canvas/src/YCbCr.js\");\n  /**\n   * @param {HTMLCanvasElement} canvas - HTML canvas eledment to attach to\n   * @constructor\n   */\n\n\n  function SoftwareFrameSink(canvas) {\n    var self = this,\n        ctx = canvas.getContext('2d'),\n        imageData = null,\n        resampleCanvas = null,\n        resampleContext = null;\n\n    function initImageData(width, height) {\n      imageData = ctx.createImageData(width, height); // Prefill the alpha to opaque\n\n      var data = imageData.data,\n          pixelCount = width * height * 4;\n\n      for (var i = 0; i < pixelCount; i += 4) {\n        data[i + 3] = 255;\n      }\n    }\n\n    function initResampleCanvas(cropWidth, cropHeight) {\n      resampleCanvas = document.createElement('canvas');\n      resampleCanvas.width = cropWidth;\n      resampleCanvas.height = cropHeight;\n      resampleContext = resampleCanvas.getContext('2d');\n    }\n    /**\n     * Actually draw a frame into the canvas.\n     * @param {YUVFrame} buffer - YUV frame buffer object to draw\n     */\n\n\n    self.drawFrame = function drawFrame(buffer) {\n      var format = buffer.format;\n\n      if (canvas.width !== format.displayWidth || canvas.height !== format.displayHeight) {\n        // Keep the canvas at the right size...\n        canvas.width = format.displayWidth;\n        canvas.height = format.displayHeight;\n      }\n\n      if (imageData === null || imageData.width != format.width || imageData.height != format.height) {\n        initImageData(format.width, format.height);\n      } // YUV -> RGB over the entire encoded frame\n\n\n      YCbCr.convertYCbCr(buffer, imageData.data);\n      var resample = format.cropWidth != format.displayWidth || format.cropHeight != format.displayHeight;\n      var drawContext;\n\n      if (resample) {\n        // hack for non-square aspect-ratio\n        // putImageData doesn't resample, so we have to draw in two steps.\n        if (!resampleCanvas) {\n          initResampleCanvas(format.cropWidth, format.cropHeight);\n        }\n\n        drawContext = resampleContext;\n      } else {\n        drawContext = ctx;\n      } // Draw cropped frame to either the final or temporary canvas\n\n\n      drawContext.putImageData(imageData, -format.cropLeft, -format.cropTop, // must offset the offset\n      format.cropLeft, format.cropTop, format.cropWidth, format.cropHeight);\n\n      if (resample) {\n        ctx.drawImage(resampleCanvas, 0, 0, format.displayWidth, format.displayHeight);\n      }\n    };\n\n    self.clear = function () {\n      ctx.clearRect(0, 0, canvas.width, canvas.height);\n    };\n\n    return self;\n  }\n\n  SoftwareFrameSink.prototype = Object.create(FrameSink.prototype);\n  module.exports = SoftwareFrameSink;\n})();\n\n//# sourceURL=webpack:///./node_modules/@byted/vertc-electron-sdk/node_modules/yuv-canvas/src/SoftwareFrameSink.js?");

/***/ }),

/***/ "./node_modules/@byted/vertc-electron-sdk/node_modules/yuv-canvas/src/WebGLFrameSink.js":
/*!**********************************************************************************************!*\
  !*** ./node_modules/@byted/vertc-electron-sdk/node_modules/yuv-canvas/src/WebGLFrameSink.js ***!
  \**********************************************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

eval("/*\nCopyright (c) 2014-2016 Brion Vibber <brion@pobox.com>\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of\nthis software and associated documentation files (the \"Software\"), to deal in\nthe Software without restriction, including without limitation the rights to\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\nthe Software, and to permit persons to whom the Software is furnished to do so,\nsubject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\nFOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\nCOPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\nIN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\nCONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n*/\n(function () {\n  \"use strict\";\n\n  var FrameSink = __webpack_require__(/*! ./FrameSink.js */ \"./node_modules/@byted/vertc-electron-sdk/node_modules/yuv-canvas/src/FrameSink.js\"),\n      shaders = __webpack_require__(/*! ../build/shaders.js */ \"./node_modules/@byted/vertc-electron-sdk/node_modules/yuv-canvas/build/shaders.js\");\n  /**\n   * Warning: canvas must not have been used for 2d drawing prior!\n   *\n   * @param {HTMLCanvasElement} canvas - HTML canvas element to attach to\n   * @constructor\n   */\n\n\n  function WebGLFrameSink(canvas) {\n    var self = this,\n        gl = WebGLFrameSink.contextForCanvas(canvas),\n        debug = false; // swap this to enable more error checks, which can slow down rendering\n\n    if (gl === null) {\n      throw new Error('WebGL unavailable');\n    } // GL!\n\n\n    function checkError() {\n      if (debug) {\n        err = gl.getError();\n\n        if (err !== 0) {\n          throw new Error(\"GL error \" + err);\n        }\n      }\n    }\n\n    function compileShader(type, source) {\n      var shader = gl.createShader(type);\n      gl.shaderSource(shader, source);\n      gl.compileShader(shader);\n\n      if (!gl.getShaderParameter(shader, gl.COMPILE_STATUS)) {\n        var err = gl.getShaderInfoLog(shader);\n        gl.deleteShader(shader);\n        throw new Error('GL shader compilation for ' + type + ' failed: ' + err);\n      }\n\n      return shader;\n    }\n\n    var program, unpackProgram, err; // In the world of GL there are no rectangles.\n    // There are only triangles.\n    // THERE IS NO SPOON.\n\n    var rectangle = new Float32Array([// First triangle (top left, clockwise)\n    -1.0, -1.0, +1.0, -1.0, -1.0, +1.0, // Second triangle (bottom right, clockwise)\n    -1.0, +1.0, +1.0, -1.0, +1.0, +1.0]);\n    var textures = {};\n    var framebuffers = {};\n    var stripes = {};\n    var buf, positionLocation, unpackPositionLocation;\n    var unpackTexturePositionBuffer, unpackTexturePositionLocation;\n    var stripeLocation, unpackTextureLocation;\n    var lumaPositionBuffer, lumaPositionLocation;\n    var chromaPositionBuffer, chromaPositionLocation;\n\n    function createOrReuseTexture(name, formatUpdate) {\n      if (!textures[name] || formatUpdate) {\n        textures[name] = gl.createTexture();\n      }\n\n      return textures[name];\n    }\n\n    function uploadTexture(name, formatUpdate, width, height, data) {\n      var texture = createOrReuseTexture(name, formatUpdate);\n      gl.activeTexture(gl.TEXTURE0);\n\n      if (WebGLFrameSink.stripe) {\n        var uploadTemp = !textures[name + '_temp'] || formatUpdate;\n        var tempTexture = createOrReuseTexture(name + '_temp', formatUpdate);\n        gl.bindTexture(gl.TEXTURE_2D, tempTexture);\n\n        if (uploadTemp) {\n          // new texture\n          gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_S, gl.CLAMP_TO_EDGE);\n          gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_T, gl.CLAMP_TO_EDGE);\n          gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.NEAREST);\n          gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MAG_FILTER, gl.NEAREST);\n          gl.texImage2D(gl.TEXTURE_2D, 0, // mip level\n          gl.RGBA, // internal format\n          width / 4, height, 0, // border\n          gl.RGBA, // format\n          gl.UNSIGNED_BYTE, // type\n          data // data!\n          );\n        } else {\n          // update texture\n          gl.texSubImage2D(gl.TEXTURE_2D, 0, // mip level\n          0, // x offset\n          0, // y offset\n          width / 4, height, gl.RGBA, // format\n          gl.UNSIGNED_BYTE, // type\n          data // data!\n          );\n        }\n\n        var stripeTexture = textures[name + '_stripe'];\n        var uploadStripe = !stripeTexture || formatUpdate;\n\n        if (uploadStripe) {\n          stripeTexture = createOrReuseTexture(name + '_stripe', formatUpdate);\n        }\n\n        gl.bindTexture(gl.TEXTURE_2D, stripeTexture);\n\n        if (uploadStripe) {\n          gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_S, gl.CLAMP_TO_EDGE);\n          gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_T, gl.CLAMP_TO_EDGE);\n          gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.NEAREST);\n          gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MAG_FILTER, gl.NEAREST);\n          gl.texImage2D(gl.TEXTURE_2D, 0, // mip level\n          gl.RGBA, // internal format\n          width, 1, 0, // border\n          gl.RGBA, // format\n          gl.UNSIGNED_BYTE, //type\n          buildStripe(width, 1) // data!\n          );\n        }\n      } else {\n        gl.bindTexture(gl.TEXTURE_2D, texture);\n        gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_S, gl.CLAMP_TO_EDGE);\n        gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_T, gl.CLAMP_TO_EDGE);\n        gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.LINEAR);\n        gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MAG_FILTER, gl.LINEAR);\n        gl.texImage2D(gl.TEXTURE_2D, 0, // mip level\n        gl.LUMINANCE, // internal format\n        width, height, 0, // border\n        gl.LUMINANCE, // format\n        gl.UNSIGNED_BYTE, //type\n        data // data!\n        );\n      }\n    }\n\n    function unpackTexture(name, formatUpdate, width, height) {\n      var texture = textures[name]; // Upload to a temporary RGBA texture, then unpack it.\n      // This is faster than CPU-side swizzling in ANGLE on Windows.\n\n      gl.useProgram(unpackProgram);\n      var fb = framebuffers[name];\n\n      if (!fb || formatUpdate) {\n        // Create a framebuffer and an empty target size\n        gl.activeTexture(gl.TEXTURE0);\n        gl.bindTexture(gl.TEXTURE_2D, texture);\n        gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_S, gl.CLAMP_TO_EDGE);\n        gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_T, gl.CLAMP_TO_EDGE);\n        gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.LINEAR);\n        gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MAG_FILTER, gl.LINEAR);\n        gl.texImage2D(gl.TEXTURE_2D, 0, // mip level\n        gl.RGBA, // internal format\n        width, height, 0, // border\n        gl.RGBA, // format\n        gl.UNSIGNED_BYTE, //type\n        null // data!\n        );\n        fb = framebuffers[name] = gl.createFramebuffer();\n      }\n\n      gl.bindFramebuffer(gl.FRAMEBUFFER, fb);\n      gl.framebufferTexture2D(gl.FRAMEBUFFER, gl.COLOR_ATTACHMENT0, gl.TEXTURE_2D, texture, 0);\n      var tempTexture = textures[name + '_temp'];\n      gl.activeTexture(gl.TEXTURE1);\n      gl.bindTexture(gl.TEXTURE_2D, tempTexture);\n      gl.uniform1i(unpackTextureLocation, 1);\n      var stripeTexture = textures[name + '_stripe'];\n      gl.activeTexture(gl.TEXTURE2);\n      gl.bindTexture(gl.TEXTURE_2D, stripeTexture);\n      gl.uniform1i(stripeLocation, 2); // Rectangle geometry\n\n      gl.bindBuffer(gl.ARRAY_BUFFER, buf);\n      gl.enableVertexAttribArray(positionLocation);\n      gl.vertexAttribPointer(positionLocation, 2, gl.FLOAT, false, 0, 0); // Set up the texture geometry...\n\n      gl.bindBuffer(gl.ARRAY_BUFFER, unpackTexturePositionBuffer);\n      gl.enableVertexAttribArray(unpackTexturePositionLocation);\n      gl.vertexAttribPointer(unpackTexturePositionLocation, 2, gl.FLOAT, false, 0, 0); // Draw into the target texture...\n\n      gl.viewport(0, 0, width, height);\n      gl.drawArrays(gl.TRIANGLES, 0, rectangle.length / 2);\n      gl.bindFramebuffer(gl.FRAMEBUFFER, null);\n    }\n\n    function attachTexture(name, register, index) {\n      gl.activeTexture(register);\n      gl.bindTexture(gl.TEXTURE_2D, textures[name]);\n      gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_S, gl.CLAMP_TO_EDGE);\n      gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_T, gl.CLAMP_TO_EDGE);\n      gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.LINEAR);\n      gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MAG_FILTER, gl.LINEAR);\n      gl.uniform1i(gl.getUniformLocation(program, name), index);\n    }\n\n    function buildStripe(width) {\n      if (stripes[width]) {\n        return stripes[width];\n      }\n\n      var len = width,\n          out = new Uint32Array(len);\n\n      for (var i = 0; i < len; i += 4) {\n        out[i] = 0x000000ff;\n        out[i + 1] = 0x0000ff00;\n        out[i + 2] = 0x00ff0000;\n        out[i + 3] = 0xff000000;\n      }\n\n      return stripes[width] = new Uint8Array(out.buffer);\n    }\n\n    function initProgram(vertexShaderSource, fragmentShaderSource) {\n      var vertexShader = compileShader(gl.VERTEX_SHADER, vertexShaderSource);\n      var fragmentShader = compileShader(gl.FRAGMENT_SHADER, fragmentShaderSource);\n      var program = gl.createProgram();\n      gl.attachShader(program, vertexShader);\n      gl.attachShader(program, fragmentShader);\n      gl.linkProgram(program);\n\n      if (!gl.getProgramParameter(program, gl.LINK_STATUS)) {\n        var err = gl.getProgramInfoLog(program);\n        gl.deleteProgram(program);\n        throw new Error('GL program linking failed: ' + err);\n      }\n\n      return program;\n    }\n\n    function init() {\n      if (WebGLFrameSink.stripe) {\n        unpackProgram = initProgram(shaders.vertexStripe, shaders.fragmentStripe);\n        unpackPositionLocation = gl.getAttribLocation(unpackProgram, 'aPosition');\n        unpackTexturePositionBuffer = gl.createBuffer();\n        var textureRectangle = new Float32Array([0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1]);\n        gl.bindBuffer(gl.ARRAY_BUFFER, unpackTexturePositionBuffer);\n        gl.bufferData(gl.ARRAY_BUFFER, textureRectangle, gl.STATIC_DRAW);\n        unpackTexturePositionLocation = gl.getAttribLocation(unpackProgram, 'aTexturePosition');\n        stripeLocation = gl.getUniformLocation(unpackProgram, 'uStripe');\n        unpackTextureLocation = gl.getUniformLocation(unpackProgram, 'uTexture');\n      }\n\n      program = initProgram(shaders.vertex, shaders.fragment);\n      buf = gl.createBuffer();\n      gl.bindBuffer(gl.ARRAY_BUFFER, buf);\n      gl.bufferData(gl.ARRAY_BUFFER, rectangle, gl.STATIC_DRAW);\n      positionLocation = gl.getAttribLocation(program, 'aPosition');\n      lumaPositionBuffer = gl.createBuffer();\n      lumaPositionLocation = gl.getAttribLocation(program, 'aLumaPosition');\n      chromaPositionBuffer = gl.createBuffer();\n      chromaPositionLocation = gl.getAttribLocation(program, 'aChromaPosition');\n    }\n    /**\n     * Actually draw a frame.\n     * @param {YUVFrame} buffer - YUV frame buffer object\n     */\n\n\n    self.drawFrame = function (buffer) {\n      var format = buffer.format;\n      var formatUpdate = !program || canvas.width !== format.displayWidth || canvas.height !== format.displayHeight;\n\n      if (formatUpdate) {\n        // Keep the canvas at the right size...\n        canvas.width = format.displayWidth;\n        canvas.height = format.displayHeight;\n        self.clear();\n      }\n\n      if (!program) {\n        init();\n      }\n\n      if (formatUpdate) {\n        var setupTexturePosition = function (buffer, location, texWidth) {\n          // Warning: assumes that the stride for Cb and Cr is the same size in output pixels\n          var textureX0 = format.cropLeft / texWidth;\n          var textureX1 = (format.cropLeft + format.cropWidth) / texWidth;\n          var textureY0 = (format.cropTop + format.cropHeight) / format.height;\n          var textureY1 = format.cropTop / format.height;\n          var textureRectangle = new Float32Array([textureX0, textureY0, textureX1, textureY0, textureX0, textureY1, textureX0, textureY1, textureX1, textureY0, textureX1, textureY1]);\n          gl.bindBuffer(gl.ARRAY_BUFFER, buffer);\n          gl.bufferData(gl.ARRAY_BUFFER, textureRectangle, gl.STATIC_DRAW);\n        };\n\n        setupTexturePosition(lumaPositionBuffer, lumaPositionLocation, buffer.y.stride);\n        setupTexturePosition(chromaPositionBuffer, chromaPositionLocation, buffer.u.stride * format.width / format.chromaWidth);\n      } // Create or update the textures...\n\n\n      uploadTexture('uTextureY', formatUpdate, buffer.y.stride, format.height, buffer.y.bytes);\n      uploadTexture('uTextureCb', formatUpdate, buffer.u.stride, format.chromaHeight, buffer.u.bytes);\n      uploadTexture('uTextureCr', formatUpdate, buffer.v.stride, format.chromaHeight, buffer.v.bytes);\n\n      if (WebGLFrameSink.stripe) {\n        // Unpack the textures after upload to avoid blocking on GPU\n        unpackTexture('uTextureY', formatUpdate, buffer.y.stride, format.height);\n        unpackTexture('uTextureCb', formatUpdate, buffer.u.stride, format.chromaHeight);\n        unpackTexture('uTextureCr', formatUpdate, buffer.v.stride, format.chromaHeight);\n      } // Set up the rectangle and draw it\n\n\n      gl.useProgram(program);\n      gl.viewport(0, 0, canvas.width, canvas.height);\n      attachTexture('uTextureY', gl.TEXTURE0, 0);\n      attachTexture('uTextureCb', gl.TEXTURE1, 1);\n      attachTexture('uTextureCr', gl.TEXTURE2, 2); // Set up geometry\n\n      gl.bindBuffer(gl.ARRAY_BUFFER, buf);\n      gl.enableVertexAttribArray(positionLocation);\n      gl.vertexAttribPointer(positionLocation, 2, gl.FLOAT, false, 0, 0); // Set up the texture geometry...\n\n      gl.bindBuffer(gl.ARRAY_BUFFER, lumaPositionBuffer);\n      gl.enableVertexAttribArray(lumaPositionLocation);\n      gl.vertexAttribPointer(lumaPositionLocation, 2, gl.FLOAT, false, 0, 0);\n      gl.bindBuffer(gl.ARRAY_BUFFER, chromaPositionBuffer);\n      gl.enableVertexAttribArray(chromaPositionLocation);\n      gl.vertexAttribPointer(chromaPositionLocation, 2, gl.FLOAT, false, 0, 0); // Aaaaand draw stuff.\n\n      gl.drawArrays(gl.TRIANGLES, 0, rectangle.length / 2);\n    };\n\n    self.clear = function () {\n      gl.viewport(0, 0, canvas.width, canvas.height);\n      gl.clearColor(0.0, 0.0, 0.0, 0.0);\n      gl.clear(gl.COLOR_BUFFER_BIT);\n    };\n\n    self.clear();\n    return self;\n  } // For Windows; luminance and alpha textures are ssllooww to upload,\n  // so we pack into RGBA and unpack in the shaders.\n  //\n  // This seems to affect all browsers on Windows, probably due to fun\n  // mismatches between GL and D3D.\n\n\n  WebGLFrameSink.stripe = function () {\n    if (navigator.userAgent.indexOf('Windows') !== -1) {\n      return true;\n    }\n\n    return false;\n  }();\n\n  WebGLFrameSink.contextForCanvas = function (canvas) {\n    var options = {\n      // Don't trigger discrete GPU in multi-GPU systems\n      preferLowPowerToHighPerformance: true,\n      powerPreference: 'low-power',\n      // Don't try to use software GL rendering!\n      failIfMajorPerformanceCaveat: true,\n      // In case we need to capture the resulting output.\n      preserveDrawingBuffer: true\n    };\n    return canvas.getContext('webgl', options) || canvas.getContext('experimental-webgl', options);\n  };\n  /**\n   * Static function to check if WebGL will be available with appropriate features.\n   *\n   * @returns {boolean} - true if available\n   */\n\n\n  WebGLFrameSink.isAvailable = function () {\n    var canvas = document.createElement('canvas'),\n        gl;\n    canvas.width = 1;\n    canvas.height = 1;\n\n    try {\n      gl = WebGLFrameSink.contextForCanvas(canvas);\n    } catch (e) {\n      return false;\n    }\n\n    if (gl) {\n      var register = gl.TEXTURE0,\n          width = 4,\n          height = 4,\n          texture = gl.createTexture(),\n          data = new Uint8Array(width * height),\n          texWidth = WebGLFrameSink.stripe ? width / 4 : width,\n          format = WebGLFrameSink.stripe ? gl.RGBA : gl.LUMINANCE,\n          filter = WebGLFrameSink.stripe ? gl.NEAREST : gl.LINEAR;\n      gl.activeTexture(register);\n      gl.bindTexture(gl.TEXTURE_2D, texture);\n      gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_S, gl.CLAMP_TO_EDGE);\n      gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_T, gl.CLAMP_TO_EDGE);\n      gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, filter);\n      gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MAG_FILTER, filter);\n      gl.texImage2D(gl.TEXTURE_2D, 0, // mip level\n      format, // internal format\n      texWidth, height, 0, // border\n      format, // format\n      gl.UNSIGNED_BYTE, //type\n      data // data!\n      );\n      var err = gl.getError();\n\n      if (err) {\n        // Doesn't support luminance textures?\n        return false;\n      } else {\n        return true;\n      }\n    } else {\n      return false;\n    }\n  };\n\n  WebGLFrameSink.prototype = Object.create(FrameSink.prototype);\n  module.exports = WebGLFrameSink;\n})();\n\n//# sourceURL=webpack:///./node_modules/@byted/vertc-electron-sdk/node_modules/yuv-canvas/src/WebGLFrameSink.js?");

/***/ }),

/***/ "./node_modules/@byted/vertc-electron-sdk/node_modules/yuv-canvas/src/YCbCr.js":
/*!*************************************************************************************!*\
  !*** ./node_modules/@byted/vertc-electron-sdk/node_modules/yuv-canvas/src/YCbCr.js ***!
  \*************************************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

eval("/*\nCopyright (c) 2014-2019 Brion Vibber <brion@pobox.com>\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of\nthis software and associated documentation files (the \"Software\"), to deal in\nthe Software without restriction, including without limitation the rights to\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\nthe Software, and to permit persons to whom the Software is furnished to do so,\nsubject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\nFOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\nCOPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\nIN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\nCONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n*/\n(function () {\n  \"use strict\";\n\n  var depower = __webpack_require__(/*! ./depower.js */ \"./node_modules/@byted/vertc-electron-sdk/node_modules/yuv-canvas/src/depower.js\");\n  /**\n   * Basic YCbCr->RGB conversion\n   *\n   * @author Brion Vibber <brion@pobox.com>\n   * @copyright 2014-2019\n   * @license MIT-style\n   *\n   * @param {YUVFrame} buffer - input frame buffer\n   * @param {Uint8ClampedArray} output - array to draw RGBA into\n   * Assumes that the output array already has alpha channel set to opaque.\n   */\n\n\n  function convertYCbCr(buffer, output) {\n    var width = buffer.format.width | 0,\n        height = buffer.format.height | 0,\n        hdec = depower(buffer.format.width / buffer.format.chromaWidth) | 0,\n        vdec = depower(buffer.format.height / buffer.format.chromaHeight) | 0,\n        bytesY = buffer.y.bytes,\n        bytesCb = buffer.u.bytes,\n        bytesCr = buffer.v.bytes,\n        strideY = buffer.y.stride | 0,\n        strideCb = buffer.u.stride | 0,\n        strideCr = buffer.v.stride | 0,\n        outStride = width << 2,\n        YPtr = 0,\n        Y0Ptr = 0,\n        Y1Ptr = 0,\n        CbPtr = 0,\n        CrPtr = 0,\n        outPtr = 0,\n        outPtr0 = 0,\n        outPtr1 = 0,\n        colorCb = 0,\n        colorCr = 0,\n        multY = 0,\n        multCrR = 0,\n        multCbCrG = 0,\n        multCbB = 0,\n        x = 0,\n        y = 0,\n        xdec = 0,\n        ydec = 0;\n\n    if (hdec == 1 && vdec == 1) {\n      // Optimize for 4:2:0, which is most common\n      outPtr0 = 0;\n      outPtr1 = outStride;\n      ydec = 0;\n\n      for (y = 0; y < height; y += 2) {\n        Y0Ptr = y * strideY | 0;\n        Y1Ptr = Y0Ptr + strideY | 0;\n        CbPtr = ydec * strideCb | 0;\n        CrPtr = ydec * strideCr | 0;\n\n        for (x = 0; x < width; x += 2) {\n          colorCb = bytesCb[CbPtr++] | 0;\n          colorCr = bytesCr[CrPtr++] | 0; // Quickie YUV conversion\n          // https://en.wikipedia.org/wiki/YCbCr#ITU-R_BT.2020_conversion\n          // multiplied by 256 for integer-friendliness\n\n          multCrR = (409 * colorCr | 0) - 57088 | 0;\n          multCbCrG = (100 * colorCb | 0) + (208 * colorCr | 0) - 34816 | 0;\n          multCbB = (516 * colorCb | 0) - 70912 | 0;\n          multY = 298 * bytesY[Y0Ptr++] | 0;\n          output[outPtr0] = multY + multCrR >> 8;\n          output[outPtr0 + 1] = multY - multCbCrG >> 8;\n          output[outPtr0 + 2] = multY + multCbB >> 8;\n          outPtr0 += 4;\n          multY = 298 * bytesY[Y0Ptr++] | 0;\n          output[outPtr0] = multY + multCrR >> 8;\n          output[outPtr0 + 1] = multY - multCbCrG >> 8;\n          output[outPtr0 + 2] = multY + multCbB >> 8;\n          outPtr0 += 4;\n          multY = 298 * bytesY[Y1Ptr++] | 0;\n          output[outPtr1] = multY + multCrR >> 8;\n          output[outPtr1 + 1] = multY - multCbCrG >> 8;\n          output[outPtr1 + 2] = multY + multCbB >> 8;\n          outPtr1 += 4;\n          multY = 298 * bytesY[Y1Ptr++] | 0;\n          output[outPtr1] = multY + multCrR >> 8;\n          output[outPtr1 + 1] = multY - multCbCrG >> 8;\n          output[outPtr1 + 2] = multY + multCbB >> 8;\n          outPtr1 += 4;\n        }\n\n        outPtr0 += outStride;\n        outPtr1 += outStride;\n        ydec++;\n      }\n    } else {\n      outPtr = 0;\n\n      for (y = 0; y < height; y++) {\n        xdec = 0;\n        ydec = y >> vdec;\n        YPtr = y * strideY | 0;\n        CbPtr = ydec * strideCb | 0;\n        CrPtr = ydec * strideCr | 0;\n\n        for (x = 0; x < width; x++) {\n          xdec = x >> hdec;\n          colorCb = bytesCb[CbPtr + xdec] | 0;\n          colorCr = bytesCr[CrPtr + xdec] | 0; // Quickie YUV conversion\n          // https://en.wikipedia.org/wiki/YCbCr#ITU-R_BT.2020_conversion\n          // multiplied by 256 for integer-friendliness\n\n          multCrR = (409 * colorCr | 0) - 57088 | 0;\n          multCbCrG = (100 * colorCb | 0) + (208 * colorCr | 0) - 34816 | 0;\n          multCbB = (516 * colorCb | 0) - 70912 | 0;\n          multY = 298 * bytesY[YPtr++] | 0;\n          output[outPtr] = multY + multCrR >> 8;\n          output[outPtr + 1] = multY - multCbCrG >> 8;\n          output[outPtr + 2] = multY + multCbB >> 8;\n          outPtr += 4;\n        }\n      }\n    }\n  }\n\n  module.exports = {\n    convertYCbCr: convertYCbCr\n  };\n})();\n\n//# sourceURL=webpack:///./node_modules/@byted/vertc-electron-sdk/node_modules/yuv-canvas/src/YCbCr.js?");

/***/ }),

/***/ "./node_modules/@byted/vertc-electron-sdk/node_modules/yuv-canvas/src/depower.js":
/*!***************************************************************************************!*\
  !*** ./node_modules/@byted/vertc-electron-sdk/node_modules/yuv-canvas/src/depower.js ***!
  \***************************************************************************************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("/*\nCopyright (c) 2014-2016 Brion Vibber <brion@pobox.com>\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of\nthis software and associated documentation files (the \"Software\"), to deal in\nthe Software without restriction, including without limitation the rights to\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\nthe Software, and to permit persons to whom the Software is furnished to do so,\nsubject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\nFOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\nCOPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\nIN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\nCONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n*/\n(function () {\n  \"use strict\";\n  /**\n   * Convert a ratio into a bit-shift count; for instance a ratio of 2\n   * becomes a bit-shift of 1, while a ratio of 1 is a bit-shift of 0.\n   *\n   * @author Brion Vibber <brion@pobox.com>\n   * @copyright 2016\n   * @license MIT-style\n   *\n   * @param {number} ratio - the integer ratio to convert.\n   * @returns {number} - number of bits to shift to multiply/divide by the ratio.\n   * @throws exception if given a non-power-of-two\n   */\n\n  function depower(ratio) {\n    var shiftCount = 0,\n        n = ratio >> 1;\n\n    while (n != 0) {\n      n = n >> 1;\n      shiftCount++;\n    }\n\n    if (ratio !== 1 << shiftCount) {\n      throw 'chroma plane dimensions must be power of 2 ratio to luma plane dimensions; got ' + ratio;\n    }\n\n    return shiftCount;\n  }\n\n  module.exports = depower;\n})();\n\n//# sourceURL=webpack:///./node_modules/@byted/vertc-electron-sdk/node_modules/yuv-canvas/src/depower.js?");

/***/ }),

/***/ "./node_modules/@byted/vertc-electron-sdk/node_modules/yuv-canvas/src/yuv-canvas.js":
/*!******************************************************************************************!*\
  !*** ./node_modules/@byted/vertc-electron-sdk/node_modules/yuv-canvas/src/yuv-canvas.js ***!
  \******************************************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

eval("/*\nCopyright (c) 2014-2016 Brion Vibber <brion@pobox.com>\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of\nthis software and associated documentation files (the \"Software\"), to deal in\nthe Software without restriction, including without limitation the rights to\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\nthe Software, and to permit persons to whom the Software is furnished to do so,\nsubject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\nFOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\nCOPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\nIN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\nCONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n*/\n(function () {\n  \"use strict\";\n\n  var FrameSink = __webpack_require__(/*! ./FrameSink.js */ \"./node_modules/@byted/vertc-electron-sdk/node_modules/yuv-canvas/src/FrameSink.js\"),\n      SoftwareFrameSink = __webpack_require__(/*! ./SoftwareFrameSink.js */ \"./node_modules/@byted/vertc-electron-sdk/node_modules/yuv-canvas/src/SoftwareFrameSink.js\"),\n      WebGLFrameSink = __webpack_require__(/*! ./WebGLFrameSink.js */ \"./node_modules/@byted/vertc-electron-sdk/node_modules/yuv-canvas/src/WebGLFrameSink.js\");\n  /**\n   * @typedef {Object} YUVCanvasOptions\n   * @property {boolean} webGL - Whether to use WebGL to draw to the canvas and accelerate color space conversion. If left out, defaults to auto-detect.\n   */\n\n\n  var YUVCanvas = {\n    FrameSink: FrameSink,\n    SoftwareFrameSink: SoftwareFrameSink,\n    WebGLFrameSink: WebGLFrameSink,\n\n    /**\n     * Attach a suitable FrameSink instance to an HTML5 canvas element.\n     *\n     * This will take over the drawing context of the canvas and may turn\n     * it into a WebGL 3d canvas if possible. Do not attempt to use the\n     * drawing context directly after this.\n     *\n     * @param {HTMLCanvasElement} canvas - HTML canvas element to attach to\n     * @param {YUVCanvasOptions} options - map of options\n     * @returns {FrameSink} - instance of suitable subclass.\n     */\n    attach: function (canvas, options) {\n      options = options || {};\n      var webGL = 'webGL' in options ? options.webGL : WebGLFrameSink.isAvailable();\n\n      if (webGL) {\n        return new WebGLFrameSink(canvas, options);\n      } else {\n        return new SoftwareFrameSink(canvas, options);\n      }\n    }\n  };\n  module.exports = YUVCanvas;\n})();\n\n//# sourceURL=webpack:///./node_modules/@byted/vertc-electron-sdk/node_modules/yuv-canvas/src/yuv-canvas.js?");

/***/ }),

/***/ "./node_modules/webpack/buildin/module.js":
/*!***********************************!*\
  !*** (webpack)/buildin/module.js ***!
  \***********************************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("module.exports = function (module) {\n  if (!module.webpackPolyfill) {\n    module.deprecate = function () {};\n\n    module.paths = []; // module.parent = undefined by default\n\n    if (!module.children) module.children = [];\n    Object.defineProperty(module, \"loaded\", {\n      enumerable: true,\n      get: function () {\n        return module.l;\n      }\n    });\n    Object.defineProperty(module, \"id\", {\n      enumerable: true,\n      get: function () {\n        return module.i;\n      }\n    });\n    module.webpackPolyfill = 1;\n  }\n\n  return module;\n};\n\n//# sourceURL=webpack:///(webpack)/buildin/module.js?");

/***/ }),

/***/ "./src/preload/index.js":
/*!******************************!*\
  !*** ./src/preload/index.js ***!
  \******************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

eval("const remote = __webpack_require__(/*! electron */ \"electron\").remote;\n\nconst app = remote.app;\n\nconst SDK = __webpack_require__(/*! @byted/vertc-electron-sdk */ \"./node_modules/@byted/vertc-electron-sdk/js/index.js\");\n\nconst fs = __webpack_require__(/*! fs */ \"fs\");\n\nconst path = __webpack_require__(/*! path */ \"path\"); ////////////////////////////////////////////////////////////////\n// tools\n\n\nconst mkdirsSync = dirname => {\n  if (fs.existsSync(dirname)) {\n    return true;\n  } else {\n    if (mkdirsSync(path.dirname(dirname))) {\n      fs.mkdirSync(dirname);\n      return true;\n    }\n  }\n}; ////////////////////////////////////////////////////////////////\n\n\nconsole.log('SDK: ', SDK);\nwindow.veRTCEngine = SDK.veRTCEngine;\nwindow.veTools = {\n  getLogPath: () => {\n    let USER_DATA_PATH = app.getPath(\"userData\");\n    let logFilePath = path.join(USER_DATA_PATH, \"logs\", \"vertc\");\n    mkdirsSync(logFilePath);\n    return logFilePath;\n  },\n  getPlatform: () => {\n    return process.platform;\n  }\n};\n\n//# sourceURL=webpack:///./src/preload/index.js?");

/***/ }),

/***/ "electron":
/*!***************************!*\
  !*** external "electron" ***!
  \***************************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("module.exports = require(\"electron\");\n\n//# sourceURL=webpack:///external_%22electron%22?");

/***/ }),

/***/ "events":
/*!*************************!*\
  !*** external "events" ***!
  \*************************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("module.exports = require(\"events\");\n\n//# sourceURL=webpack:///external_%22events%22?");

/***/ }),

/***/ "fs":
/*!*********************!*\
  !*** external "fs" ***!
  \*********************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("module.exports = require(\"fs\");\n\n//# sourceURL=webpack:///external_%22fs%22?");

/***/ }),

/***/ "path":
/*!***********************!*\
  !*** external "path" ***!
  \***********************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("module.exports = require(\"path\");\n\n//# sourceURL=webpack:///external_%22path%22?");

/***/ })

/******/ });